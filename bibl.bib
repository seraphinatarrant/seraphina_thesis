@incollection{suresh2021framework,
  title={A framework for understanding sources of harm throughout the machine learning life cycle},
  author={Suresh, Harini and Guttag, John},
  booktitle={Equity and access in algorithms, mechanisms, and optimization},
  pages={1--9},
  year={2021}
}

@book{oneil2016weapons,
  title={Weapons of math destruction: How big data increases inequality and threatens democracy},
  author={O'neil, Cathy},
  year={2016},
  publisher={Broadway books}
}

@article{hardt2016equality,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@online{crawford_keynote,
author = {Crawford, Kate},
title = {The Trouble with Bias. (Keynote at NeurIPS)},
url = {https://www.youtube.com/watch?v=fMym_BKWQzk},
publisher = {NeurIPS},
year = {2017}
}

@inproceedings{Dixon2018MeasuringAM,
  title={Measuring and Mitigating Unintended Bias in Text Classification},
  author={Lucas Dixon and John Li and Jeffrey Scott Sorensen and Nithum Thain and Lucy Vasserman},
  booktitle={AIES '18},
  year={2018}
}


@article{McCurdy2017GrammaticalGA,
  title={Grammatical gender associations outweigh topical gender bias in crosslinguistic word embeddings},
  author={K. McCurdy and Oguz Serbetci},
  journal={WiNLP},
  year={2017},
}

@inproceedings{vig_causal,
 author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12388--12401},
 publisher = {Curran Associates, Inc.},
 title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
 url = {https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{pearl_direct_indirect,
author = {Pearl, Judea},
title = {Direct and Indirect Effects},
year = {2001},
isbn = {1558608001},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The direct effect of one event on another can be defined and measured by holding constant
all intermediate variables between the two. Indirect effects present conceptual and
practical difficulties (in nonlinear models), because they cannot be isolated by holding
certain variables constant. This paper presents a new way of defining the effect transmitted
through a restricted set of paths, without controlling variables on the remaining
paths. This permits the assessment of a more natural type of direct and indirect effects,
one that is applicable in both linear and nonlinear models and that has broader policy-related
interpretations. The paper establishes conditions under which such assessments can
be estimated consistently from experimental and nonexperimental data, and thus extends
path-analytic techniques to nonlinear and nonparametric models.},
booktitle = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
pages = {411–420},
numpages = {10},
location = {Seattle, Washington},
series = {UAI'01}
}

@article{Caliskan2017SemanticsDA,
  title={Semantics derived automatically from language corpora contain human-like biases},
  author={Aylin Caliskan and Joanna J Bryson and Arvind Narayanan},
  journal={Science},
  year={2017},
  volume={356},
  pages={183-186}
}

@inproceedings{hutchinson_mitchell_2019,
  title={50 years of test (un) fairness: Lessons for machine learning},
  author={Hutchinson, Ben and Mitchell, Margaret},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={49--58},
  year={2019}
}

@book{barocas-hardt-narayanan,
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  note = {\url{http://www.fairmlbook.org}},
  year = {2019}
}

@article{meng2022locating,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2022}
}

@inproceedings{
korbak2022on,
title={On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting},
author={Tomasz Korbak and Hady Elsahar and Germ{\'a}n Kruszewski and Marc Dymetman},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=XvI6h-s4un}
}


@inproceedings{ruder2019transfer,
  title={Transfer Learning in Natural Language Processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  pages={15--18},
  year={2019}
}

@inproceedings{jacobsandwallach,
author = {Jacobs, Abigail Z. and Wallach, Hanna},
title = {Measurement and Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445901},
doi = {10.1145/3442188.3445901},
abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {375–385},
numpages = {11},
keywords = {measurement, fairness, construct reliability, construct validity},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{tatman17_interspeech,
  author={Rachael Tatman and Conner Kasten},
  title={{Effects of Talker Dialect, Gender & Race on Accuracy of Bing Speech and YouTube Automatic Captions}},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={934--938},
  doi={10.21437/Interspeech.2017-1746}
}

@inproceedings{LeBras2020AdversarialFO,
  title={Adversarial Filters of Dataset Biases},
  author={Ronan Le Bras and Swabha Swayamdipta and Chandra Bhagavatula and Rowan Zellers and Matthew E. Peters and Ashish Sabharwal and Yejin Choi},
  booktitle={International Conference on Machine Learning},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:211076210}
}



@inproceedings{buolamwini18a,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}


@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group UK London}
}


@article{ng2016nuts,
  title={Nuts and bolts of building AI applications using Deep Learning},
  author={Ng, Andrew},
  journal={NIPS Keynote Talk},
  year={2016}
}
@inproceedings{suresh2021framework,
author = {Suresh, Harini and Guttag, John},
title = {A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle},
year = {2021},
isbn = {9781450385534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465416.3483305},
doi = {10.1145/3465416.3483305},
abstract = {As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.},
booktitle = {Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {17},
numpages = {9},
keywords = {societal implications of machine learning, representational harm, fairness in machine learning, allocative harm, algorithmic bias, AI ethics},
location = {--, NY, USA},
series = {EAAMO '21}
}



@book{oneil2016weapons,
  title={Weapons of math destruction: How big data increases inequality and threatens democracy},
  author={O'Neil, Cathy},
  year={2016},
  publisher={Broadway books}
}

@article{hardt2016equality,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@misc{crawford_keynote,
author = {Crawford, Kate},
title = {The Trouble with Bias. (Keynote at NeurIPS)},
url = {https://www.youtube.com/watch?v=fMym_BKWQzk},
publisher = {NeurIPS},
year = {2017}
}

@inproceedings{Dixon2018MeasuringAM,
  title={Measuring and Mitigating Unintended Bias in Text Classification},
  author={Lucas Dixon and John Li and Jeffrey Scott Sorensen and Nithum Thain and Lucy Vasserman},
  booktitle={AIES '18},
  year={2018}
}


@article{McCurdy2017GrammaticalGA,
  title={Grammatical gender associations outweigh topical gender bias in crosslinguistic word embeddings},
  author={K. McCurdy and Oguz Serbetci},
  journal={WiNLP},
  year={2017},
}

@article{mehrabi_survey,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {115},
numpages = {35},
keywords = {representation learning, natural language processing, machine learning, deep learning, Fairness and bias in artificial intelligence}
}


@book{kelvin1891popular,
  title={Popular lectures and addresses},
  author={Kelvin, William Thomson Baron},
  volume={3},
  year={1891},
  publisher={Macmillan and Company}
}

@article{kennedy2023public,
  title={Public awareness of artificial intelligence in everyday activities},
  author={Kennedy, Brian and Tyson, Alec and Saks, Emily},
  year={2023},
  publisher={Pew Research Center}
}

@inproceedings{vig_causal,
 author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12388--12401},
 publisher = {Curran Associates, Inc.},
 title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
 url = {https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{vaswani,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}


@article{forbes_hiring,
 author  = {Kelly, Jack},
 date    = {2023-11-04},
 title   = {How Companies Are Hiring And Reportedly Firing With AI},
 journal = {Forbes},
 url     = {https://www.forbes.com/sites/jackkelly/2023/11/04/how-companies-are-hiring-and-firing-with-ai/},
 urldate = {2024-02-18},
year={2023}
}

@TECHREPORT{ofcom_content_mod,
author={Winchcomb, Tim},
title={Use of AI in online content moderation},
institution={Ofcom},
year={2019}
} category; required fields are author, title, institution, year

@book{corbett_non_canonical,
  title={Non-canonical gender systems},
  author={Fedden, Sebastian and Audring, Jenny and Corbett, Greville G},
  year={2018},
  publisher={Oxford University Press}
}

@book{corbett_1991, place={Cambridge}, series={Cambridge Textbooks in Linguistics}, title={Gender}, publisher={Cambridge University Press}, author={Corbett, Greville G.}, year={1991}, collection={Cambridge Textbooks in Linguistics}} <div></div>

@article{BM25,
  title={The Probabilistic Relevance Framework: BM25 and Beyond},
  author={Stephen E. Robertson and Hugo Zaragoza},
  journal={Found. Trends Inf. Retr.},
  year={2009},
  volume={3},
  pages={333-389}
}

@article{atlas_jmlr,
  author  = {Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
  title   = {Atlas: Few-shot Learning with Retrieval Augmented Language Models},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {251},
  pages   = {1--43},
  url     = {http://jmlr.org/papers/v24/23-0037.html}
}

@inproceedings{pearl_direct_indirect,
author = {Pearl, Judea},
title = {Direct and Indirect Effects},
year = {2001},
isbn = {1558608001},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The direct effect of one event on another can be defined and measured by holding constant
all intermediate variables between the two. Indirect effects present conceptual and
practical difficulties (in nonlinear models), because they cannot be isolated by holding
certain variables constant. This paper presents a new way of defining the effect transmitted
through a restricted set of paths, without controlling variables on the remaining
paths. This permits the assessment of a more natural type of direct and indirect effects,
one that is applicable in both linear and nonlinear models and that has broader policy-related
interpretations. The paper establishes conditions under which such assessments can
be estimated consistently from experimental and nonexperimental data, and thus extends
path-analytic techniques to nonlinear and nonparametric models.},
booktitle = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
pages = {411–420},
numpages = {10},
location = {Seattle, Washington},
series = {UAI'01}
}

@article{Caliskan2017SemanticsDA,
  title={Semantics derived automatically from language corpora contain human-like biases},
  author={Aylin Caliskan and Joanna J Bryson and Arvind Narayanan},
  journal={Science},
  year={2017},
  volume={356},
  pages={183-186}
}

@inproceedings{hutchinson_mitchell_2019,
  title={50 years of test (un) fairness: Lessons for machine learning},
  author={Hutchinson, Ben and Mitchell, Margaret},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={49--58},
  year={2019}
}

@book{barocas-hardt-narayanan,
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  note = {\url{http://www.fairmlbook.org}},
  year = {2019}
}

@article{meng2022locating,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2022}
}

@inproceedings{
korbak2022on,
title={On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting},
author={Tomasz Korbak and Hady Elsahar and Germ{\'a}n Kruszewski and Marc Dymetman},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=XvI6h-s4un}
}


@inproceedings{ruder2019transfer,
  title={Transfer Learning in Natural Language Processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  pages={15--18},
  year={2019}
}

@inproceedings{jacobsandwallach,
author = {Jacobs, Abigail Z. and Wallach, Hanna},
title = {Measurement and Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445901},
doi = {10.1145/3442188.3445901},
abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {375–385},
numpages = {11},
keywords = {measurement, fairness, construct reliability, construct validity},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{tatman17_interspeech,
  author={Rachael Tatman and Conner Kasten},
  title={{Effects of Talker Dialect, Gender and Race on Accuracy of Bing Speech and YouTube Automatic Captions}},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={934--938},
  doi={10.21437/Interspeech.2017-1746}
}

@inproceedings{LeBras2020AdversarialFO,
  title={Adversarial Filters of Dataset Biases},
  author={Ronan Le Bras and Swabha Swayamdipta and Chandra Bhagavatula and Rowan Zellers and Matthew E. Peters and Ashish Sabharwal and Yejin Choi},
  booktitle={International Conference on Machine Learning},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:211076210}
}



@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{buolamwini18a,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}


@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{BOLD,
author = {Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
title = {BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445924},
doi = {10.1145/3442188.3445924},
abstract = {Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {862–872},
numpages = {11},
keywords = {natural language generation, Fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}



@article{ng2016nuts,
  title={Nuts and bolts of building AI applications using Deep Learning},
  author={Ng, Andrew},
  journal={NIPS Keynote Talk},
  year={2016}
}


@article{luccioni_bloom_carbon,
  author  = {Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure Ligozat},
  title   = {Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {253},
  pages   = {1--15},
  url     = {http://jmlr.org/papers/v24/23-0069.html}
}


@misc{qi2023finetuning,
      title={Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!}, 
      author={Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson},
      year={2023},
      eprint={2310.03693},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{
zhou2023lima,
title={{LIMA}: Less Is More for Alignment},
author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and LILI YU and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=KBMOKmX2he}
}


@misc{lin2023unlocking,
      title={The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning}, 
      author={Bill Yuchen Lin and Abhilasha Ravichander and Ximing Lu and Nouha Dziri and Melanie Sclar and Khyathi Chandu and Chandra Bhagavatula and Yejin Choi},
      year={2023},
      eprint={2312.01552},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{bolukbasi,
  title={Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
  author={Tolga Bolukbasi and Kai-Wei Chang and James Y. Zou and Venkatesh Saligrama and Adam Tauman Kalai},
  booktitle={NIPS},
  year={2016}
}

@proceedings{multiberts,
title	= {The MultiBERTs: BERT Reproductions for Robustness Analysis},
editor	= {Thibault Sellam and Steve Yadlowsky and Ian Tenney and Jason Wei and Naomi Saphra and Alexander Nicholas D'Amour and Tal Linzen and Jasmijn Bastings and Iulia Raluca Turc and Jacob Eisenstein and Dipanjan Das and Ellie Pavlick},
year	= {2022},
URL	= {https://arxiv.org/abs/2106.16163},
booktitle	= {ICLR 2022}
}


%this is in the anthology but the antho file is out of date
@inproceedings{karamolegkou-etal-2023-copyright,
    title = "Copyright Violations and Large Language Models",
    author = "Karamolegkou, Antonia  and
      Li, Jiaang  and
      Zhou, Li  and
      S{\o}gaard, Anders",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.458",
    doi = "10.18653/v1/2023.emnlp-main.458",
    pages = "7403--7412",
}


@article{wordsim,
  author       = {Lev Finkelstein and
                  Evgeniy Gabrilovich and
                  Yossi Matias and
                  Ehud Rivlin and
                  Zach Solan and
                  Gadi Wolfman and
                  Eytan Ruppin},
  title        = {Placing search in context: the concept revisited},
  journal      = {{ACM} Trans. Inf. Syst.},
  volume       = {20},
  number       = {1},
  pages        = {116--131},
  year         = {2002},
  url          = {https://doi.org/10.1145/503104.503110},
  doi          = {10.1145/503104.503110},
  timestamp    = {Mon, 26 Oct 2020 08:20:56 +0100},
  biburl       = {https://dblp.org/rec/journals/tois/FinkelsteinGMRSWR02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{black_rep_bias,
author = {Robert M. Entman},
title ={Blacks in the News: Television, Modern Racism and Cultural Change},

journal = {Journalism Quarterly},
volume = {69},
number = {2},
pages = {341-361},
year = {1992},
doi = {10.1177/107769909206900209},

URL = { 
    
        https://doi.org/10.1177/107769909206900209

},
eprint = { 
    
        https://doi.org/10.1177/107769909206900209

}
}

@book{dixon2017dangerous,
  title={A dangerous distortion of our families: Representations of families, by race, in news and opinion media: A study},
  author={Dixon, Travis L},
  year={2017},
  publisher={Color of Change}
}


@inproceedings{Dhamala_2021, series={FAccT ’21},
   title={BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation},
   url={http://dx.doi.org/10.1145/3442188.3445924},
   DOI={10.1145/3442188.3445924},
   booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
   year={2021},
   month=mar, collection={FAccT ’21} }

@inproceedings{bianchi-2023,
author = {Bianchi, Federico and Kalluri, Pratyusha and Durmus, Esin and Ladhak, Faisal and Cheng, Myra and Nozza, Debora and Hashimoto, Tatsunori and Jurafsky, Dan and Zou, James and Caliskan, Aylin},
title = {Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594095},
doi = {10.1145/3593013.3594095},
abstract = {Machine learning models that convert user-written text descriptions into images are now widely available online and used by millions of users to generate millions of images a day. We investigate the potential for these models to amplify dangerous and complex stereotypes. We find a broad range of ordinary prompts produce stereotypes, including prompts simply mentioning traits, descriptors, occupations, or objects. For example, we find cases of prompting for basic traits or social roles resulting in images reinforcing whiteness as ideal, prompting for occupations resulting in amplification of racial and gender disparities, and prompting for objects resulting in reification of American norms. Stereotypes are present regardless of whether prompts explicitly mention identity and demographic language or avoid such language. Moreover, stereotypes persist despite mitigation strategies; neither user attempts to counter stereotypes by requesting images with specific counter-stereotypes nor institutional attempts to add system “guardrails” have prevented the perpetuation of stereotypes. Our analysis justifies concerns regarding the impacts of today’s models, presenting striking exemplars, and connecting these findings with deep insights into harms drawn from social scientific and humanist disciplines. This work contributes to the effort to shed light on the uniquely complex biases in language-vision models and demonstrates the ways that the mass deployment of text-to-image generation models results in mass dissemination of stereotypes and resulting harms.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1493–1504},
numpages = {12},
location = {Chicago, IL, USA},
series = {FAccT '23}
}


@article{llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      journal={arXiv preprint arXiv:2307.09288},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{chawla2002smote,
  title={SMOTE: synthetic minority over-sampling technique},
  author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal={Journal of artificial intelligence research},
  volume={16},
  pages={321--357},
  year={2002}
}


@inproceedings{ma-etal-2023-intersectional,
    title = "Intersectional Stereotypes in Large Language Models: Dataset and Analysis",
    author = "Ma, Weicheng  and
      Chiang, Brian  and
      Wu, Tong  and
      Wang, Lili  and
      Vosoughi, Soroush",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.575",
    doi = "10.18653/v1/2023.findings-emnlp.575",
    pages = "8589--8597",
}
@article{bucinca_2021,
author = {Bu\c{c}inca, Zana and Malaya, Maja Barbara and Gajos, Krzysztof Z.},
title = {To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449287},
doi = {10.1145/3449287},
month = {apr},
articleno = {188},
numpages = {21},
keywords = {artificial intelligence, cognition, explanations, trust}
}

@misc{smith2023identifying,
      title={Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey}, 
      author={Victoria Smith and Ali Shahin Shamsabadi and Carolyn Ashurst and Adrian Weller},
      year={2023},
      eprint={2310.01424},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{mikolov_word2vec,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
 volume = {26},
 year = {2013}
}


@article{hartmann2023sok,
  title={SoK: Memorization in General-Purpose Large Language Models},
  author={Hartmann, Valentin and Suri, Anshuman and Bindschaedler, Vincent and Evans, David and Tople, Shruti and West, Robert},
  journal={arXiv preprint arXiv:2310.18362},
  year={2023}
}


@article{hupkes2023taxonomy,
  title={A taxonomy and review of generalization research in NLP},
  author={Hupkes, Dieuwke and Giulianelli, Mario and Dankers, Verna and Artetxe, Mikel and Elazar, Yanai and Pimentel, Tiago and Christodoulopoulos, Christos and Lasri, Karim and Saphra, Naomi and Sinclair, Arabella and others},
  journal={Nature Machine Intelligence},
  volume={5},
  number={10},
  pages={1161--1174},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@INPROCEEDINGS{wang2019balanced,
  author={Wang, Tianlu and Zhao, Jieyu and Yatskar, Mark and Chang, Kai-Wei and Ordonez, Vicente},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations}, 
  year={2019},
  volume={},
  number={},
  pages={5309-5318},
  keywords={Predictive models;Task analysis;Data models;Computational modeling;Computer vision;Visualization;Neural networks},
  doi={10.1109/ICCV.2019.00541}}


@InProceedings{pmlr-v80-kearns18a,
  title = 	 {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  author =       {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2564--2572},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kearns18a/kearns18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kearns18a.html},
}


@inproceedings{guo_and_caliskan,
author = {Guo, Wei and Caliskan, Aylin},
title = {Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462536},
doi = {10.1145/3461702.3462536},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {122–133},
numpages = {12},
keywords = {AI ethics, bias, intersectionality, language models, social psychology, word embeddings},
location = {Virtual Event, USA},
series = {AIES '21}
}


@article{jiang2024mixtral,
  title={Mixtral of Experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    journal={Not Even on Arxiv},
  year={2019}
}

@article{Zheng2016ImprovingTR,
  title={Improving the Robustness of Deep Neural Networks via Stability Training},
  author={Stephan Zheng and Yang Song and Thomas Leung and Ian J. Goodfellow},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={4480-4488},
  url={https://api.semanticscholar.org/CorpusID:2102547}
}

@article{bertrand2004emily,
  title={Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination},
  author={Bertrand, Marianne and Mullainathan, Sendhil},
  journal={American economic review},
  volume={94},
  number={4},
  pages={991--1013},
  year={2004},
  publisher={American Economic Association}
}

@inproceedings{
    rafailov2023direct,
    title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
    author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D Manning and Stefano Ermon and Chelsea Finn},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://arxiv.org/abs/2305.18290}
}
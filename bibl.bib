@incollection{suresh2021framework,
  title={A framework for understanding sources of harm throughout the machine learning life cycle},
  author={Suresh, Harini and Guttag, John},
  booktitle={Equity and access in algorithms, mechanisms, and optimization},
  pages={1--9},
  year={2021}
}

@book{oneil2016weapons,
  title={Weapons of math destruction: How big data increases inequality and threatens democracy},
  author={O'neil, Cathy},
  year={2016},
  publisher={Broadway books}
}

@article{hardt2016equality,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@online{crawford_keynote,
author = {Crawford, Kate},
title = {The Trouble with Bias. (Keynote at NeurIPS)},
url = {https://www.youtube.com/watch?v=fMym_BKWQzk},
publisher = {NeurIPS},
year = {2017}
}

@inproceedings{Dixon2018MeasuringAM,
  title={Measuring and Mitigating Unintended Bias in Text Classification},
  author={Lucas Dixon and John Li and Jeffrey Scott Sorensen and Nithum Thain and Lucy Vasserman},
  booktitle={AIES '18},
  year={2018}
}


@article{McCurdy2017GrammaticalGA,
  title={Grammatical gender associations outweigh topical gender bias in crosslinguistic word embeddings},
  author={K. McCurdy and Oguz Serbetci},
  journal={WiNLP},
  year={2017},
}

@inproceedings{vig_causal,
 author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12388--12401},
 publisher = {Curran Associates, Inc.},
 title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
 url = {https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{pearl_direct_indirect,
author = {Pearl, Judea},
title = {Direct and Indirect Effects},
year = {2001},
isbn = {1558608001},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The direct effect of one event on another can be defined and measured by holding constant
all intermediate variables between the two. Indirect effects present conceptual and
practical difficulties (in nonlinear models), because they cannot be isolated by holding
certain variables constant. This paper presents a new way of defining the effect transmitted
through a restricted set of paths, without controlling variables on the remaining
paths. This permits the assessment of a more natural type of direct and indirect effects,
one that is applicable in both linear and nonlinear models and that has broader policy-related
interpretations. The paper establishes conditions under which such assessments can
be estimated consistently from experimental and nonexperimental data, and thus extends
path-analytic techniques to nonlinear and nonparametric models.},
booktitle = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
pages = {411–420},
numpages = {10},
location = {Seattle, Washington},
series = {UAI'01}
}

@article{Caliskan2017SemanticsDA,
  title={Semantics derived automatically from language corpora contain human-like biases},
  author={Aylin Caliskan and Joanna J Bryson and Arvind Narayanan},
  journal={Science},
  year={2017},
  volume={356},
  pages={183-186}
}

@inproceedings{hutchinson_mitchell_2019,
  title={50 years of test (un) fairness: Lessons for machine learning},
  author={Hutchinson, Ben and Mitchell, Margaret},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={49--58},
  year={2019}
}

@book{barocas-hardt-narayanan,
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  note = {\url{http://www.fairmlbook.org}},
  year = {2019}
}

@article{meng2022locating,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2022}
}

@inproceedings{
korbak2022on,
title={On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting},
author={Tomasz Korbak and Hady Elsahar and Germ{\'a}n Kruszewski and Marc Dymetman},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=XvI6h-s4un}
}


@inproceedings{ruder2019transfer,
  title={Transfer Learning in Natural Language Processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  pages={15--18},
  year={2019}
}

@inproceedings{jacobsandwallach,
author = {Jacobs, Abigail Z. and Wallach, Hanna},
title = {Measurement and Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445901},
doi = {10.1145/3442188.3445901},
abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {375–385},
numpages = {11},
keywords = {measurement, fairness, construct reliability, construct validity},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{tatman17_interspeech,
  author={Rachael Tatman and Conner Kasten},
  title={{Effects of Talker Dialect, Gender and Race on Accuracy of Bing Speech and YouTube Automatic Captions}},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={934--938},
  doi={10.21437/Interspeech.2017-1746}
}

@inproceedings{LeBras2020AdversarialFO,
  title={Adversarial Filters of Dataset Biases},
  author={Ronan Le Bras and Swabha Swayamdipta and Chandra Bhagavatula and Rowan Zellers and Matthew E. Peters and Ashish Sabharwal and Yejin Choi},
  booktitle={International Conference on Machine Learning},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:211076210}
}



@inproceedings{buolamwini18a,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}


@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{BOLD,
author = {Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
title = {BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445924},
doi = {10.1145/3442188.3445924},
abstract = {Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {862–872},
numpages = {11},
keywords = {natural language generation, Fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}



@article{ng2016nuts,
  title={Nuts and bolts of building AI applications using Deep Learning},
  author={Ng, Andrew},
  journal={NIPS Keynote Talk},
  year={2016}
}


@article{luccioni_bloom_carbon,
  author  = {Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure Ligozat},
  title   = {Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {253},
  pages   = {1--15},
  url     = {http://jmlr.org/papers/v24/23-0069.html}
}


@misc{qi2023finetuning,
      title={Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!}, 
      author={Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson},
      year={2023},
      eprint={2310.03693},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{
zhou2023lima,
title={{LIMA}: Less Is More for Alignment},
author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and LILI YU and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=KBMOKmX2he}
}


@misc{lin2023unlocking,
      title={The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning}, 
      author={Bill Yuchen Lin and Abhilasha Ravichander and Ximing Lu and Nouha Dziri and Melanie Sclar and Khyathi Chandu and Chandra Bhagavatula and Yejin Choi},
      year={2023},
      eprint={2312.01552},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{bolukbasi,
  title={Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
  author={Tolga Bolukbasi and Kai-Wei Chang and James Y. Zou and Venkatesh Saligrama and Adam Tauman Kalai},
  booktitle={NIPS},
  year={2016}
}

@proceedings{multiberts,
title	= {The MultiBERTs: BERT Reproductions for Robustness Analysis},
editor	= {Thibault Sellam and Steve Yadlowsky and Ian Tenney and Jason Wei and Naomi Saphra and Alexander Nicholas D'Amour and Tal Linzen and Jasmijn Bastings and Iulia Raluca Turc and Jacob Eisenstein and Dipanjan Das and Ellie Pavlick},
year	= {2022},
URL	= {https://arxiv.org/abs/2106.16163},
booktitle	= {ICLR 2022}
}


%this is in the anthology but the antho file is out of date
@inproceedings{karamolegkou-etal-2023-copyright,
    title = "Copyright Violations and Large Language Models",
    author = "Karamolegkou, Antonia  and
      Li, Jiaang  and
      Zhou, Li  and
      S{\o}gaard, Anders",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.458",
    doi = "10.18653/v1/2023.emnlp-main.458",
    pages = "7403--7412",
}

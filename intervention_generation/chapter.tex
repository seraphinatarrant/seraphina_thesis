\part{Fairness in Fact-Based Retrieval and Generation}
\label{part:generation}



%Maybe Finish  transition by consider adding label balance addendum and also random seed addendum and maybe also training dynamics addendum

Until now, all research in this thesis has been on classification systems, where pretrained embedding or a language model are then adapted into a downstream classification system. 
While this was the dominant transfer learning paradigm for the duration of this research, it notably neglects two entirely different type of systems that grew in prominence: generative systems, and retrieval systems (which are increasingly often combined).

A neural retrieval system is often called \textbf{dense retrieval} to contrast with sparse vectors from lexical word-counting based approaches. These systems start with a pretrained language model as an initialisation -- just as in all the previous sections -- but then instead of fine-tuning that model with a classification layer, it is fine-tuned on data to optimise the quality of the representation for determining \textit{relevance} between a query and a document. And then if used in a generation system, the top retrieved document representations are context that can be used to generate an answer to a query, out of the composition of both model parameters and document representations. 

This is thus a very similar but also very different setup, and adds an interesting element on top of the previous factors that could affect fairness: the retrieval corpus itself. 

It's worth noting that, though it was not a motivation for this, in the course of doing the following research (which began in August 2022), retrieval systems have become nearly as ubiquitous as transfer learning became over the course of this thesis, as a way to mitigate the lossy compression effects ("hallucinations") that became a focus since the rise of generative NLP after the release of ChatGPT. 


In the following work, we analyse gender bias and the properties of dense retrieval systems. We build upon both the methods and the questions that we accumulated in previous work in this thesis, and apply them in a very different setup. We use information theoretic probing for gender information, as a predictor of gender bias that we discovered in \ref{chapter:gender_bias_probing}. We analyse the impact of the new retrieval training objective on gender information and show that it is a predictor of allocational bias, even in this new setup. We also do an extensive investigation into the impact of random seeds, based on the findings from both works in \ref{part:crosslingual} on the surprisingly large impact of random initialisation on fairness, as well as \citet{multiberts}, which came out in the interim and found the same effect.

In this work, we answer a few separate questions:
\begin{enumerate}
    \item What impact does retriever training have on the demographic gender encoded in the representaion, and how does this differ from a standard language model (which we analysed in \ref{chapter:gender_bias_probing})?
    \item What impact does random seed have on our results?
    \item What is the cause of any observed gender bias?
\end{enumerate}

Many of these questions had very surprising results: the random seed experiments showed quite strange behaviour, and we dedicated more experiments and analysis to that than expected. We also found that, for the dataset we looked at, the gender bias was not attributable to the representations, such that in this case gender biases cannot easily be corrected by representations alone. 

So we leave this as an interesting final piece of work. In \ref{chapter:gender_bias_probing}, removing gender from language model representations \textit{did} correlate with downstream fairness, but in this work, we find a system where it does not. This expands our view, and shows the true complexity of the fairness space, and the reinforces the need to focus on a whole system not just on any single aspect of modelling. The first work in this thesis showed that a language model can't be measured in isolation from a downstream application. This work shows that in many now commonly used systems, a model even trained on an application cannot be considered without the data it is retrieving, and that factors such as random seeds, which were not previously thought to matter at all for fairness, have a large effect. 





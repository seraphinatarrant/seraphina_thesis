%thesis.tex 
%Model LaTeX file for Ph.D. thesis at the 
%School of Mathematics, University of Edinburgh

\title{Fairness in Transfer Learning for Natural Language Processing}
\author{Seraphina Goldfarb-Tarrant}
\date{2023}

\documentclass[phd,ilcc,oneside,leftchapter,parskip]{infthesis}

\usepackage[T2A,T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}

\usepackage{natbib}

% \usepackage{biblatex}
% \addbibresource{biblatex_bib.bib}

\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{times}
\usepackage{dirtytalk}



\usepackage[dvipsnames]{xcolor}

\usepackage{subcaption}
\usepackage{mwe}
\usepackage{booktabs} % Top and bottom rules for tables
\usepackage{svrsymbols}

\usepackage{tikz-dependency}
\usepackage{tikz-qtree}
\usepackage{paralist}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
\usetikzlibrary{matrix}
\usepackage{etoolbox}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{verbatim}
% \usepackage{todonotes}
% \usepackage[utf8]{inputenc}
\usepackage{CJKutf8}

\usepackage{epigraph}
\setlength{\epigraphwidth}{0.7\textwidth} 

\usepackage[final]{pdfpages}

\usepackage{url}
\usepackage{hyperref}
\usepackage{cleveref}
\urlstyle{rm}
\definecolor{revcolor}{rgb}{0.81, 0.09, 0.13}
\definecolor{darkblue}{rgb}{0.2, 0.2, 0.6}


\usepackage[left, mathlines]{lineno}
\renewcommand\linenumberfont{\normalfont\bfseries\small\color{lightgray}}

\hypersetup{
    colorlinks,
    citecolor=darkblue,
    filecolor=black,
    linkcolor=darkblue,
    urlcolor=darkblue
}

% \usepackage{lsubfiles}

\newcommand{\sgtcomment}[1]{\textcolor{blue}{[ST: #1]}}

\newcommand{\E}{\mathbb{E}}


\abstract{Natural Language Processing (NLP) systems have come to permeate so many areas of daily life that it is difficult to live a day without having one or many experiences mediated by an NLP system. These systems bring with them many promises: more accessible information in more languages, real-time content moderation, more data-driven decision making, intuitive access to information via Question Answering and chat interfaces.
%and scale and efficiency at low costs. 
But there is a dark side to these promises, for the past decade of research has shown that NLP systems can contain social biases and deploying them can incur serious social costs. Each of these promises has been found to have unintended consequences: racially charged errors and rampant gender stereotyping in language translation, censorship of minority voices and dialects, Human Resource systems that discriminate based on demographic data, a proliferation of toxic generated text and misinformation, and many subtler issues. 

Yet despite these consequences, and the proliferation of bias research attempting to correct them, NLP systems have not improved very much. There are a few reasons for this. First, measuring bias is difficult; there are not standardised methods of measurement, and much research relies on one-off methods that are often insufficiently careful and thoroughly tested. Thus many works have contradictory results that cannot be reconciled, because of minor differences or assumptions in their metrics. Without thorough testing, these metrics can even mislead and give the illusion of progress. Second, much research adopts an overly simplistic view of the causes and mediators of bias in a system. NLP systems have multiple components and stages of training, and many works test fairness at only one stage. They do not study how different parts of the system interact, and how fairness changes during this process. So it is unclear whether these isolated results will hold in the full complex system. Here, we address both of these shortcomings. We conduct a detailed analysis of fairness metrics applied to upstream language models (models that will be used in a downstream task in transfer learning). We find that a) the most commonly used upstream fairness metric is not predictive of downstream fairness, such that it should not be used but that b) information theoretic probing is a good alternative to these existing fairness metrics, as we find it is both predictive of downstream bias and robust to different modelling choices.   %illuminating the inconsistencies in a number of common metrics, and making recommendations to improve their utility. 
We then use our findings to track how unfairness, having entered a system, persists and travels throughout it. We track how fairness issues travel between tasks (from language modelling to classification) in monolingual transfer learning, and between languages, in multilingual transfer learning. We find that multilingual transfer learning often exacerbates fairness problems and should be used with care, whereas monolingual transfer learning generally improves fairness. Finally, we track how fairness travels between source documents and retrieved answers to questions, in fact-based generative systems. Here we find that, though retrieval systems strongly represent demographic data such as gender, bias in retrieval question answering benchmarks does not come from the model representations, but from the queries or the corpora. We reach all of our findings only by looking at the entire transfer learning system as a whole, and we hope that this encourages other researchers to do the same. We hope that our results can guide future fairness research to be more consistent between works, better predictive of real world fairness outcomes, and better able to prevent unfairness from propagating between different parts of a system. 
}

\begin{document}

%% First, the preliminary pages
\begin{preliminary}

%% This creates the title page
\maketitle

\begin{laysummary}
\textbf{Natural Language Processing} describes any AI system that deals with human language. Today NLP systems are part of every person's daily life; visibly as phone voice assistants and automatic YouTube captioning, and invisibly, when that person applies for a job or when all the data they put online is analysed for content moderation, marketing, opinion polling, and more. \textbf{Transfer Learning} describes systems that are multi-part, which almost all systems are today, because they are built from one big language model, like ChatGPT, Llama, Mistral, or Cohere, which is later \textit{fine-tuned} to a particular task, such as customer service or resume processing. Fine-tuning just means that some data in that domain, usually expensive, often proprietary, is used after the model is trained in order to make the language model less general purpose and better at that type of data. This system described so far is Transfer Learning. In many of today's systems, the language model is then connected to a RAG (Retrieval Augmented Generation) component, where the model can search a database of documents: Wikipedia, confidential medical records, all the PDFs that a PhD student has downloaded over the course of their degree\footnote{Roughly 1227 papers, in my case.}. The premise of the research in this thesis is that you have to know how the parts of these systems interact, in order to judge whether a system is \textbf{fair}. This means the interaction between all of: the original big language model, the data you fine-tune on, anything else you make at the fine-tuning stage, the retrieval system, the corpus it retrieves from. A system that is \textbf{fair} is one that doesn't propagate social biases and stereotypes, and doesn't screw over minority groups in society. To be precise to the definition of fairness, it shouldn't screw over \textit{any} group, even straight white men, but the minority ones tend to be the ones we worry about since they tend to be most screwed over, with some exceptions. 

In this work, we discover a couple of things about how the parts of an NLP system interrelate, with regard to fairness. First, you cannot measure fairness of just the language model in isolation and know whether your model will be fair or unfair when it's deployed in an application later. You can get an indication as to the model's \textit{potential} for unfairness, but that is all. This idea of \textit{potential} can be understood by analogy to genetics---we can measure whether a person is more or less likely to develop cancer, but whether they do or not over the course of their life depends on whether they work in copper mines and what they eat, how much they exercise and what pollutants are in the air where they live. Some people with a high propensity may never develop it, and some people with no propensity at all still will if they live next to Chernobyl. With language models, we can measure how much potential there is for unfairness, but we can't know what this really means without the environmental factors: the fine-tuning data, the RAG system, and even the cultural context of deployment, which determines what demographic groups are considered minorities. We can make predictions about how likely the model will be to become unfair, if it is fine-tuned on skewed data that doesn't have positive examples of good resumes for people who aren't white, or doesn't have high quality RAG data for people who are not male (this is, incidentally, true of Wikipedia \citep{sun-peng-2021-men}). So we can improve, or mitigate this \textit{potential} in a language model. But in real world applications, where we need \textit{know} that people are not screwed over with some degree of certainty, we need to test the final system.

Most language models today are also \textbf{multilingual}. If you type something into ChatGPT or Cohere's model in English, it can retrieve information from documents in Turkish and summarise them for you in English. If you type something in Korean, it will respond in Korean. In this work, we discovered that data in one language can influence fairness behaviour in \textit{another} language. An NLP system that handles Japanese can become more sexist and racist when you add data from English, even though the data you're adding is not in Japanese, and even though the sexist and racist stereotypes in English data are not the same as those in native Japanese. 

Overall, this work shows that you cannot assume that the addition of new data---from fine-tuning, for other languages, from RAG---does not change the fairness properties of an original model. So fairness cannot just be the domain of the tech giants and gold-plated start-ups, of Google, Meta, Anthropic, Cohere, or whatever companies are producing the new hot model next year. Is has to be a collaboration between the people training the big models, the people deploying systems in the real world, and, ideally, the users. 

\end{laysummary}

% Acknowledgements
\begin{acknowledgements}
People say that PhDs are the most solitary time of your life. In some ways, I've found that to be true. But I very much did not do this alone. 

The inkling in my mind that I wanted to leave my career and go study NLP began roughly ten years ago, and I would not be here today without the herculean efforts of some, and the small graces of many. I want to thank first Somusa Ratanarak, who encouraged me to leave Google and pursue my passions when all my colleagues thought leaving a stable job was utterly insane. She's been with me every step of the way, with a lot of cleverness and care, and a little bit of \begin{CJK*}{UTF8}{ipxm} 愛の鞭 \end{CJK*}, 
and I am a much better person because of her. I want to thank my parents, all of them, Mom\#1, Mom\#2, Dad, Tyger, and Rachel, who also didn't think I was crazy. Thank you for seeing and celebrating my accomplishments, with flowers and lemons and jam, even when I didn't stop to appreciate them because I was busy running for the next goalpost. Thank you for being proud of me.

There were still others whose help I needed even to reach the start of my PhD. I want to thank Ozan Mindek, for giving me my first chance at NLP on a 20\% project. Then Emily Bender, for walking her talk and creating an MSc program that was \textit{truly} diverse in practice. She took me with my undergraduate in Ancient Greek, and my now friends Brian from the Navy and Lonny working on Yupiq preservation, along with the usual CS suspects. I aspire to be that conscientious in organisations that I lead, and I can see already how challenging it will be to live up to that. I also need to thank that same diverse cohort of Brian, Lonny, Catharine, Genevieve, Amandalynne, Chris, and Ben for spending Saturday mornings helping me practice interview for my PhD.
And I aspire also to be like two more women who helped me: Fei Xia, who can make absolutely any question you ask her interesting, no matter how silly it was, and who showed me the beauty and elegance of statistical machine learning. And Nanyun Peng, for taking on a green MSc student and teaching me to write papers and staying up late with me, and showing me that I absolutely loved doing research. 

Now we have reached the start of my PhD. Here I want to thank both my advisors. Björn, for taking me on partway through and bringing his energy and enthusiasm to keep me going, at a point in the pandemic when my own energy was flagging; our Meadows walks kept me going. Adam, for accepting me, trusting whatever direction I ran off in, and immediately being for me a mentor who cares deeply about both good science and about people. Adam has changed the way I do science and the way I see the world; I am proud to be in his intellectual lineage. 

So many other people made my PhD possible, by keeping me afloat along the way, in a period in which very many things went wrong in my life and in the world. I owe to them, and to those moments, both a piece of this research and of who I am. 

Late nights walking the City of London with Sameer Bansal, whose dark humour and enormous generosity lightens all loads. Hikes, bothies, wine and cheese nights with Kate McCurdy, who would always make me laugh and teach me something marvelous about humans or language, and who became a better friend and colleague than I could've ever wished for. Always-inadvisably-late nights by the fire with Henry Conklin, with whom I get my most interesting NLP inspiration and also with whom I can be my whole self, unfiltered. All my adventures in the beautiful wilds with Ida Szubert, who holds so much of my soul and my delight in life, and with whom it is so easy to share space, and somehow never a chore to work late. Pints at Sandy Bells with Ramon Sanabria, who sees the cheerful beauty in even quite shitty situations and shows me how to do this too. Dinners and walks with Yevgen Matusevych, helped me when I needed it and scolded me for not asking. Coffees with Tom Hosking, who has a knack for re-energising me about the whole field of NLP over coffee, just when I've been banging my head against a problem so long it's become tedious. Writing accountability meetings at the Burn with Tom Sherborne, who was instrumental in getting my nightmare multilingual work onto a page, and has been a companion since. Brewing the worst beer I have ever made with Sander Bijl de Vroe and drinking it anyway. Walks down the Innocent Railway with Siddarth Narayanaswamy, reminding me that I wished I'd learnt first principles better, but also that this wish is why I became a scientist in the first place. Whisky nights with Jasmijn Bastings, as well as our long chat on my 15-hour solo drive south from NAACL after I caught Covid-19 there. She was always so easygoing, and foolish/caring enough to come to all my panels and talks, without me asking. 

All of my direct collaborators also kept me going through a PhD that was much, much lonelier than I could have ever anticipated. Meetings with my collaborators Hadas Orgad and Yonatan Belinkov, which I would look forward to from the start of every week and would keep me energised until the end of it.  Adam and Sharon's Agora research group in ILCC provided feedback, structure, and scholarly companionship. Diego Marcheggiani and Roi Blanco helped me manage my first enormous multilingual project. Patrick Lewis and Pedro Rodriguez exemplified a combination of rigor and curiosity in their science, as well as conscience, that I admire and hope to live up to. 

I am thankful for the small graces (and some large graces) far beyond the research sphere also. To James and Deena Owers-Bardsley for my intro to cycling in Scotland and for the cocktail kits we left on each others' doorsteps when we couldn't see other people. To David Halliwell and Narma Gebruk, for celebrating my intended submission with me last May, and still sticking with me for the next nine months. To the entire Beltane family --- especially my performance groups Veles, Goblin Fire Arch, Goblin Bower, The Summer King, and Obsidian --- who gave me a haven away from academia which at times I desperately needed. To Alison Stewart, for taking me in practically as a family member after that one coffee in George Square. To Sam Roots and Ruari Cathmoir, for being my chosen family. To Ellen Mears, who has been my companion in doing the things that I want to do for \textit{me}, rather than spending all my time on my sometimes heavy responsibilities. To Guru Khalsa, for every one of those calls from the road. To Ivan Ivanov, who was accidentally stuck with me during the pandemic, and who I hope to get stuck with many more times in my life. To James Hartley, who is one of the few people in the world I feel I can lean on, and who has always provided me with so much love it keeps me warm from another hemisphere. To Ezra Baydur, for drunk-chat-chess and seeing the best in me. To Craig Innes, who has walked beside me and introduced me to a version of myself that I didn't know was there before, and that I am so glad to have gotten to know.  Who reliably has something uplifting to say about my skills, to prop me up, before I give a scary public appearance. To Tasuku Koda, Narumi Ota, and Makoto Takashima, for showing me \begin{CJK*}{UTF8}{ipxm} 本音 \end{CJK*} and keeping a place on the other side of the world that feels like home. To Alice in Slumberland, my Burning Man family, who I saw only once during my PhD but who taught me that I could express myself any way I wanted, and that I could even redefine the world in which I lived. I'm trying to do a little bit of that redefinition in here, with this work. 

\end{acknowledgements}


%% Next we need to have the declaration.
%\standarddeclaration
\begin{declaration}
   I declare that this thesis was composed by myself,
   that the work contained herein is my own 
   except where explicitly stated otherwise in the text,
   and that this work has not been submitted for any other degree or
   professional qualification except as specified.

   My specific contributions are as follows for each chapter:
   
   Chapter~\ref{chapter:intrinsic_bias_metrics}: I designed the research agenda: I envisioned the research question and wrote a research plan document with methods, goals, metrics, and a literature review. I recruited and supervised three MSc students who implemented pipelines for three different systems and did initial investigations into the correlation between intrinsic and extrinsic metrics for their MSc theses. I gathered these pipelines together, extended them, ran experiments, and wrote and presented the paper, with the help of Adam. 

   Chapter~\ref{chapter:gender_bias_probing}: I was second author on this paper, assisting the first author Hadas Orgad who proposed this extension of the work in Chapter~\ref{chapter:intrinsic_bias_metrics}. I implemented some of the metrics on intrinsic analysis of language model representations, implemented the additional extrinsic fairness metrics (which are now open-sourced), and co-wrote the paper with Hadas Orgad and Yonatan Belinkov.

   Chapters~\ref{chapter:multilingual_sentiment_analysis} and \ref{chapter:multilingual_sentiment_analysis_pt2}: I did this project almost entirely on my own save for the writing, which my supervisors Adam and Björn assisted with greatly. I designed the research question and outlined the project, developed and programmed the experiment framework, found training data, created evaluation data (with the help of native speakers of each language) and wrote up the results (with the help of my supervisors). I received regular weekly consultation from Diego Marcheggiani, Roi Blanco, and Lluis Marquez at Amazon Barcelona for the first of the two projects.

   Chapter~\ref{chapter:contrievers}: I designed the research question in collaboration with Patrick Lewis. I made the research project plan, chose datasets and interpretability methods, wrote all pipeline code and ran all experiments, and finally wrote up the findings into a paper, with help from Pedro Rodriguez.

   \par
   \vspace{1in}\raggedleft({\em Seraphina Goldfarb-Tarrant})
   \end{declaration}

% \dedication{stuff}

%% Finally, a dedication (this is optional -- uncomment the following line if
%% you want one).
% \dedication{To my mum
%% Create the table of contents
\tableofcontents

%% If you want a list of figures or tables, uncomment the appropriate line(s)
% \listoffigures
% \listoftables

\end{preliminary}
%\linenumbers

\include{introduction/chapter}

\include{background/chapter}

\include{measurement/chapter}

% Background for this probably talk about predictive vs. descriptive measures
\include{measurement/intrinsic_bias_metrics/chapter}
\includepdf[pages=-]{pdfs/2021.acl-long.150.pdf}

\include{measurement/gender_bias_probing/chapter}
\includepdf[pages=-]{pdfs/2022.naacl-main.188.pdf}


\include{crosslingual/chapter}
\include{crosslingual/multi_sentiment_analysis/chapter}
\includepdf[pages=-]{pdfs/bias_beyond_english_final.pdf}
\include{crosslingual/multi_sentiment_analysis_part2/chapter}
\includepdf[pages=-]{pdfs/cross_lingual_transfer_final.pdf}

%\include{crosslingual/swahili_gender_representation/chapter}

\include{intervention_generation/chapter}
\include{intervention_generation/retrieval/chapter}
\includepdf[pages=-]{pdfs/probing_multi_contrievers_ARR_deanon.pdf}

\include{final/chapter}
\include{conclusion/chapter}

\bibliographystyle{apalike}
\bibliography{anthology,anthology_p2,bibl}

\appendix

\end{document}

The answers to these questions are by no means clear, yet they are essential to understand the potential harms of present day NLP systems. Transfer learning has grown exponentially since 2015 \cite{andrew ng talk} to near ubiquity today \cite{}, and it has the potential to improve fairness outcomes, and the potential to worsen them. Transfer learning can make a system better able to generalise, and generalisation failures are generally fairness failures also. It also enables the expansion of NLP models to tasks and languages that do not have the resources (financial, computational, engineering) to train models from scratch \citep{}. Within a language, this can make stronger and more robust models, which can improve fairness by handling long-tails better, and between languages, it can narrow the gap between English and other languages, increasing access to technology and information and decreasing marginalisation.

For instance, two of the tasks that exist in the most languages are toxicity detection \citep{} and sentiment analysis \citep{}. Without transfer learning (monolingual or cross-lingual), these systems may not exist of will be weaker in languages other than English. And these systems convey real benefits (enumerate?). However, with transfer learning, there is a risk of bringing harmful behaviour learnt in the original language or task that is unexpected in the new downstream task.
%However, it creates serious challenges for ensuring that models are fair; that they have similar performance for different demographics (e.g. false positive in hate speech example TODO) and that they do not propagate harmful stereotypes of use them in decision making. It makes the first stage of data intensive development modular, and divorces it from downstream applications in which it will be used. How does an engineer training a model on language modelling for English CommonCrawl\footnote{define what this is, also talk about the examples of the famous problems in it as an introduction to some bad stuff about data in transfer learning} know whether the changes and improvements they have made to their task performance will make a system more or less fair when their model is applied to a task downstream? (should I cite my paper here?) 

These relationships and outcomes are difficult to predict, but it is important to avoid the paradoxical situation of extending access to lower resourced settings and then ending up further marginalising them by unforseen and unfair model behaviour.

In Part \ref{part:measurement} we examine how fairness in the first language modelling task, used for transfer, relates to fairness in downstream tasks. We examine whether interventions and fairness measurements at the language modelling stage correspond to better fairness downstream. We find that common debiasing techniques and lower bias measurements at the language modelling stage do not correspond to lower bias measurements downstream (Chapter \ref{chapter:intrinsic_bias_metrics}), except in cases where the debiasing has distributed or removed \textit{information} about the target demographic - rather than just removing negative associations. That is, removing the stereotype that women aren't in STEM does not correspond to a better ability to classify biographies of female scientists accurately. Removing gender markers that encode the information that text is about a female does (Chapter \ref{chapter:gender_bias_probing}).

Subsec: Intrinsic Bias Metrics Do Not Correlate with Application Bias

Subsec: How Gender Debiasing Affects Internal Model Representations, and Why It Matters

In Part \ref{part:crosslingual}, we examine how transfer learning across affects fairness outcomes for five different languages, both monolingually and when transferring information across languages. We find that the increased power of transfer learning improves fairness outcomes within one language, but that transferring information across languages can be dangerous and often can introduce new fairness problems that weren't present in the original target language. We additionally find that the presence of stronger or weaker gender markers in a given language is related to how strongly gender bias will manifest in cross-lingual transfer, which adds support to our previous finding that it is the presence of demographic information in a language model that is most predictive of bias.

Subsec: Cross-lingual Transfer Can Worsen Bias in Low-Resource Sentiment Analysis
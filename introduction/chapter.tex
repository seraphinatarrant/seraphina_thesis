%!TEX root = ../thesis.tex
\chapter{Introduction} \label{chapter:introduction}
In the past decade, Natural Language Processing (NLP) systems have come to saturate everyday life. NLP has expanded from being used to translate webpages and recommend new videos to having inescapable reach; it is now also used to moderate social media content$^{\heartsuit}$ \citep{}, to create content to answer user questions$^{\spadesuit}$ \citep{}, to track public opinion about products or politicians$^{\diamondsuit}$ \citep{} \textbf{c}, to sort and filter resumes for potential new workers$^{\clubsuit}$ \cite{}, and myriad other applications.
%, and to track likelihood of committing a crime and recidivism rates$^{\star}$ \citep{}.  
This increase in scope and usage of NLP systems comes with many promises of efficiency, cost reduction, and even social good from scale--promises of (in order of the above examples) reducing hatespeech and aggression online$^{\heartsuit}$, of greater and easier access to information$^{\spadesuit}$, of direct and inexpensive democratic feedback for companies or policies$^{\diamondsuit}$, and of more efficient hiring practices$^{\clubsuit}$.
%, and of better allocation of police resources or more objective, data-driven sentencing$^{\star}$. 

But this territorial expansion introduces many new harms that diminish and sully these promises.  
The often referenced promise of mathematical objectivity---freeing us from human subjectivity, inconsistency, and biases---has proven to be mythical \citep{oneil2016weapons}. A large growing body of work analysing NLP systems has shown that they are not \textit{fair}; they do not behave similarly and work equally well for different genders, races, nationalities, and other demographic groups. So given that NLP systems, and the data, models, and optimisation and evaluation metrics they are composed of are \textit{not} inherently fair, they must be carefully designed to be so. When they are not, companies and organisations using them are often worse off than before there were automated systems, since they are now scaling a flawed system. An individual Human Resources person may have flaws and biases, but they work for only one or a few companies and have time to read only so many resumes in a day. Some resumes will be sent to a different person, who may have different biases, preventing the inequities introduced by the first person's views from being complete and impenetrable over a wide swath of potential jobs. When a flawed and biased system is scaled, it does not get tired and can process as many thousands of resumes as time and compute allows, and the same system is often used by many companies. The very variability of human behaviour, and the inconsistencies in human decisionmaking that are often considered undesirable, limit the possible scope of each individual's (or even each company or organisation's) biases. An NLP system, in contrast, replicates the same biases to an unlimited extent, and whatever unfortunate minorities it is biased against will experience more widespread discrimination.

Recognition of fairness problems in NLP has grown exponentially, beginning in 2016, such that major conferences now have a dedicated track for fairness research\footnote{\url{https://aclrollingreview.org/cfp}}, encourage papers to self-declare potential hazards\footnote{Section A2 in \url{https://aclrollingreview.org/responsibleNLPresearch/}, and section 1c in \url{https://neurips.cc/Conferences/2021/PaperInformation/PaperChecklist}}, and have a committee appointed to review them\footnote{\url{https://www.aclweb.org/adminwiki/index.php?title=Formation_of_the_ACL_Ethics_Committee}}. Yet despite all the attention and effort expended on fairness issues in NLP, we as a community have made only such a small dent in known problems as to be aware of the magnitude of unaddressed fairness problems. This is because both discovering and addressing fairness problems in an NLP system is extremely challenging. 

There are multiple ways that a system can be unfair. NLP systems are often not \textbf{allocationally fair}, and have different accuracies and rates of false positives and false negatives for different demographics. A example such situation is when a toxicity detection system has much higher rates of false positives for text that is actually neutral or positive but contains terms about race, religion, or sexual orientation. In such as case, a sentence like \textit{ I am a gay man} can be flagged and removed, as was the case with Google's toxicity detection system in 2018 \citep{Dixon2018MeasuringAM}. NLP systems are also often not \textbf{representationally fair}; they reproduce and propagate negative stereotypes for minoritised demographics \citep{crawford_keynote}. For example, prominent generation systems will disproportionately describe women as taking carer roles, and portray racial minorities as criminals \citep{sheng-etal-2019-woman}. There is not even a consensus on how best to measure each type of unfairness. Most metrics used to measure them are ad-hoc and have not been standardised or analysed for \textbf{predictive validity}---their ability to predict actual fairness problems--or \textbf{concurrent validity}---their agreement with other metrics in use. We make some progress towards assessing the predictive and concurrent validity in Part~\ref{part:measurement}.  

Another challenge is that fairness issues can appear at almost any stage of building an NLP system \citep{suresh2021framework}. NLP papers commonly claim that `model biases reflect biases in data they were trained on' but this is both a such a gross oversimplification as to be unhelpful (how did the biases get into the data? Do imbalances in labels over different sensitive groups count, or do only stereotypes count?) as well as incorrect. All choices in the process of training an NLP model have been shown to affect the resulting bias. A resume filtering system can be trained on data in which humans made racist or sexist decisions (\textbf{historical} bias) and that bias will persist, can even be amplified, and scale, with the authority of an objective AI system behind it. This happened with Amazon's attempt at an AI for Human Resources.\footnote{ \url{https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G}.} A content moderation and toxicity detection system can be unfamiliar with non-prestige dialects and censor them incorrectly, as happened when Twitter incorrectly flagged African-American Vernacular English (AAVE) as toxic speech \citep{sap-etal-2019-risk}. Even though AAVE is common in the world, it was not well-represented in data the model has seen, resulting in \textbf{sampling}, aka \textbf{representation}, bias). An NLP system is often uses labelled data (for supervised learning) where the labels are a proxy for the task that is to be learnt, and that label can be a better or worse proxy for the desired task -- e.g. in the resume filtering example \textit{was previously hired based on this resume} as a proxy for \textit{was suitable for the job}. This is \textbf{measurement} bias. These are only a few of the myriad ways that unfairness can enter a system, which we selected as examples as they are the types that we spend the most time examining below. There are many more subtle ways that make mitigation even more challenging, which we discuss in full in Chapter~\ref{chapter:background}.


% Remining things are aggregation (example from hardt) and evaluation (maybe example from my reality check paper) and also learning (the metric being optimised). 

%Fairness issues can arise also in fitting the same function to multiple groups that require different functions (aggregation bias) and in the metric used to evaluate (evaluation bias) \citep{hardt2016equality}.


%it has become a vital responsibilty for us as researchers to examine our models, algorithms, and data as to their behaviour for different demographic groups. Do they behave similarly and work equally well for different genders, different races, different nationalities? We need to know this information to ensure that the systems we build innovate, and improve society, rather than accelerating marginalisation and societal divisions and isolation. 

%These are the ways that unfairness can enter a system. 

%\sgtcomment{PICK UP HERE -- connect to the issues of *scaling* and of *not knowing how something gets into the system. Might need to connect transfer learning to it more smoothly -- since transfer learning is responsible for scale, and basically introduces a new source of all of these types of biases that is unexplored (a second set of data and of sampling and of measurement blah blah). It is also now the dominant paradigm (scale) that has been underexplored. And we want to know if these systems are better or worse than systems before using them, and we also want to pinpoint the source to enable us to mitigate it -- if we performed mitigation at the wrong stage it might not work}

An NLP system can contain one, many, or all, of these sources of bias, and this bias can enter in via the data collection, dataset splits, learning objective, model architecture, model deployment choices (such as decoding hyperparameters or classifier thresholds). And worse still is that most of these choices are now made \textit{twice}.  Current scale in NLP is driven by \textbf{transfer learning}, where a model is trained on high resource task(s) or language(s) (e.g. unstructured web crawl text) and then ported to a lower resourced one (e.g. any supervised task requiring labels, like sentiment analysis) -- not necessarily objectively \textit{low resource}, but relatively lower resourced, i.e. with less data than the dominant task or language being used for transfer. 
%Within a language, this is usually done with a language model pre-trained on large scale web text and then applied to a supervised learning task requiring labelled data. Between languages, this is usually done between English and another language with less or no labelled data. 
It was already difficult to pinpoint where biases enter a system, and now most systems are composed of multiple sets of training data, multiple objectives, multiple measurements. This is now the dominant paradigm in NLP, but previous to our work, fairness research considered only one of the two stages \sgtcomment{and usually only one type of bias}. If a language model that will later be used in our example resume filtering system (which we refer to as an \textbf{upstream model} has been debiased with regard to gender, will the classifier on top of it (which we refer to as the \textbf{downstream model} also be debiased, or not? If instead the classifier is debiased, is the language model also safe to use, or will bias then surface if the language model is used in another task, or directly without the classifier? We cannot answer these questions without studying the entire system and learning the relationship between upstream and downstream models. And without these answerss,
%further complicating this already multiplex ecosystem of threats to fairness. It is now even harder to track and mitigate biases in NLP systems because they are multi-stage. 
bias mitigation methods or measurements can be at best ineffective, and at worst misleading. With these answers we can apply effective bias mitigation strategies at the correct stage of the system, and we will understand the contribution of transfer learning to fairness in NLP systems and be informed as to whether systems are becoming better or worse as they scale. \sgtcomment{This understanding is a pre-requisite to effective work in NLP bias, and yet we have little knowledge of it. }

So here, in the below, we explore an as yet unstudied area of NLP fairness; how unfairness, having entered a system, persists and travels throughout it. 

We first focus, in Part~\ref{part:measurement}, on measurement at different stages of transfer learning. No real research can be done without good measures of bias, and an understanding of how measures of bias relate at different stages of transfer learning. In Chapter~\ref{chapter:intrinsic_bias_metrics}, we study whether the most common \textbf{intrinsic} bias measurements--at the language model pretraining stage--are predictive of later downstream bias in two classification tasks in two languages. We find that they are not predictive, and that the widespread use of these measures has indeed been leading to a false sense of progress in debiasing research, since we cannot tell whether debiasing efforts are propagating downstream or not. Our results show that more effort needs to be spent on measuring bias on the downstream task itself. We then study the transfer learning measurement relationship in the \textit{reverse} direction, in Chapter~\ref{chapter:gender_bias_probing}. Here we ask instead how a pretrained language model changes when different debiasing methods are applied downstream. We find that a new metric, based on information theoretic probing (also known as minimum description length (MDL) probing) \citep{} \textit{can}, when applied to the pretrained language model, differentiate between different downstream bias methods. We find that this measure is predictive of how robust debiasing of the pretrained language model is, and whether the debiasing will remain if that model is then used in another task. These two results together imply that the \textbf{geometry} (cosine or other distance measures) of concepts in language model representation space does not reliably predict downstream bias, but the \textbf{extractability} of concepts (as measured by information theoretic codelengths) is better predictor. In that work, we also explore a wide suite of fairness metrics, as they are not all suitable for the same situations, and they are mutually unsatisfiable.

% Prior to the work in this dissertation, it has been unknown how fairness (and lack of) in an initial model relates to fairness in a system in which it is used. 

We then use our findings on measurement to conduct experiments addressing a broad question about how the use of transfer learning affects the fairness of a system. There is no previous work on this, but previous work on aspects of transfer learning leads to two competing possibilities of how transfer learning could impact fairness. Does transfer learning \textit{improve} fairness, because the additional data sources lead to overall better models that are better at modelling long tail phenomena \cite{} (and data on minorities is often long tail)? Or does the additional complexity bring in new or magnified undesirable biases, via one of the many mechanisms discussed? 
%We study this for transfer learning between tasks within one language, and also for transfer learning between different languages. Within one language, will a language model that has been trained on raw text data that underrepresents women in prestige careers in STEM also fail to appropriately classify women's biographies into STEM roles \citep{biosbias} and incorrectly filter women's resumes for STEM positions? 

In Part~\ref{part:crosslingual} we pick a task---sentiment analysis, which we selected since this task enables us to test in a number of languages---and study this effect for transfer learning between tasks/objectives (the current dominant NLP paradigm, which we will sometimes refer to as monolingual transfer learning to distinguish it) and transfer learning between languages, or multilingual transfer learning. Prior to our first investigation, previous work had shown that language models trained on unstructured text have gender and racial biases \cite{bolukbasi, caliskan, zhao-etal-2019-gender, https://aclanthology.org/2020.acl-main.260, sheng-etal-2019-woman}. So we asked, will this carry through in monolingual transfer learning and cause gender and racial biases to appear or increase in a downstream sentiment model, beyond what can be attributed to the downstream training data? For instance, let's say that an upstream language model has learnt to associate conventionally negative attributes with certain minorities, such as to represent gay men as doing drugs, and black men as pimps (examples from \citet{sheng-etal-2019-woman}). Will a sentiment classifier built on this upstream language model also associate negative sentiment with gay and black men, \textit{even if} there is little or no data about gay and black men in the sentiment training data? 
Or will that bias be overridden or lost, either because the role of the classifier is strong enough to disregard that, or because the now larger and more expressive system can generalise better to other positive association involving black and gay men, such as stars in politics and arts, or affirmational personal stories, such as those in \cite{Dixon2018MeasuringAM}? We find that, overall, the additional stability from transfer learning is helpful in a resource constrained setting (i.e. one in which you cannot gather more annotated sentiment data), and this is effect is enough to overall reduce gender and racial biases (even if perhaps new negative associations have been introduced).

We also study this effect for transfer learning between languages, or \textbf{cross-lingual transfer learning}. In this setting, not only can an upstream model learn biases from multiple data sources, but also from multiple languages. Exactly how much information cross-lingual transfer learning shares across languages is not well understood and there are some contradictory empirical studies \cite{}. So we ask whether, in cross-lingual transfer learning, a language model that has learnt harmful stereotypes can carry those negative associations across languages? In the above example where a model has learnt negative associations \textit{in English} about black and gay men, will a classifier in Japanese have these same associations, if they do not occur in Japanese? Can the collision between competing stereotypes in different languages weaken them, and in effect fight bias with bias? \citep{winomt}. Can anything be done in the initial task before transfer, to ensure better outcomes in the second task? We find that, contrary to what we found in monolingual transfer learning, cross-lingual transfer learning tends to (with exceptions) exacerbate biases, though this effect can be mitigated with distilled/compressed models with little loss in performance. 

In Part \ref{part:generation}, we look at a third type of system: retrieval augmented generation, which presents an inversion of the standard transfer learning setup. In the standard setup,  a language model feeds into a classifier, and in retrieval augmented generation, the classifier selects source documents to answer a query, and this feeds into a language model, which conditions on those documents to generate an answer. This inverted system allows us to also ask the reverse question: if a language model has learnt problematic associations and stereotypes, can these be counteracted by conditioning on source documents? For instance, if a language model generates results about women predominantly in low-prestige roles, will it change this if it is conditioned on source documents about female CEOs and doctors? Or is it more likely to ignore the source information in this case then in the case of male CEOs and doctors? Or, as a third option, the retriever itself is biased, and doesn't select documents about female CEOs, so we never even get to that point?
However, prior to our work, not only was there no research examining how fairness flows between retrieval models and generative language models, there was little research analysing dense retrievers at all. So we began by asking the sub-question: a retriever representation is necessarily a compression of a document, so what information is actually in this representation, such that 
 a language model can condition on it? (Recall \ref{chapter:gender_bias_probing} where information in a representation as measured by information theoretic probing is most predictive of bias). Is information about demographics--gender, race, etc--in a retriever representation predictive of allocational bias in retrieved results? That is, does a retriever with stronger information about gender pick documents about gender more unequally? We do a case study in allocational gender bias and find that, though retrievers quite strongly encode gender in their representations, allocational bias is not attributable to the representations themselves. This bias persists even when we remove gender from the representation, meaning that it comes from either the composition of the corpus or the queries themselves. We leave completing the high level question via studying of what happens when a language model uses these representations to future work. 

 We conclude with a summary of our contributions, and with a set of recommendations in light of our findings.

\section{Contributions}
We make contributions to three broad categories: more meaningful and reliable \textbf{measurement} of fairness in language models, analysis of how \textbf{transfer learning} affects fairness, and analysis of fairness in \textbf{retrieval-augmented generation}.

\subsection{Measurement} 
We did the first study evaluating whether the most commonly used fairness metric for upstream language models---which comprise one third of fairness research--correlated with downstream fairness. We thoroughly examined the relationship between upstream and downstream metrics across a broad set of experimental conditions: two types of bias (gender, racial), two different tasks (coreference resolution and hatespeech detection), two different languages (English and Spanish), two common embedding algorithms (fastText and word2vec), two common methods of debiasing (preprocessing training data, and post-processing on representations), two downstream fairness metrics (difference in precision and difference in recall). We found that the common upstream metric was not predictive of downstream bias, which changes the focus of the fairness field to utilising metrics that are more predictive. Our work has inspired follow up studies examining the predictive validity of fairness metrics \citep{cao-etal-2022-intrinsic, others?}, which further extend and corroborate our findings. \textbf{My contribution:} I designed the research agenda: I envisioned the research question and wrote a research plan document with methods, goals, and metrics. I recruited and subsequently supervised three MSc students who implemented pipelines for three different systems and did initial investigations into the correlation between intrinsic and extrinsic metrics for their theses. I gathered these pipelines together, modified and extended them, ran experiments, and wrote and presented the paper. 

We also did the first study investigating how debiasing \textit{downstream} (rather than upstream) affects language model (upstream) representations. We focused on gender bias in English and considered two different common transformer models, two different tasks (coreference resolution, and biography classification), three debiasing methods, two different intrinsic metrics (CEAT and a new one, MDL compression, that we proposed), and ten different downstream fairness metrics. We found that our new metric was predictive of whether the upstream model had been successfully debiased, and correlated well with most downstream metrics. We also found that not all fairness metrics correlated, and highlighted the trade-offs between them and the importance of not relying overly much on one metric. \textbf{My contribution:} I was second author on this paper, assisting the first author Hadas Orgad who envisioned this extension of my previous work. I implemented some code on analysis of language model representations, implemented the additional distributional fairness metrics, and co-wrote the paper.

\subsection{Transfer Learning}
We did the first research on the effect of both standard (monolingual) transfer learning and cross-lingual transfer learning on gender and racial biases in sentiment analysis. We examined whether, for five languages (Japanese, Chinese, Spanish, German, English) monolingual transfer learning (via pretrained models) and multilingual transfer learning (via multilingual models and via cross-lingual labelled data) changed the biases in sentiment analysis systems. We found that, though the story is reasonably complex, overall cross-lingual transfer learning can increase bias even in unexpected cases such as culturally specific racial biases, but monolingual transfer learning usually reduces it, even though the training data used for transfer contains new biases. \textbf{My contribution:} I did this project almost entirely on my own save for the writing, which my supervisors assisted with. I designed the research question and outlines the project, developed and programmed the experiment framework, found training data, created evaluation data (with the help of native speakers of each language) and wrote up the results (with the help of my supervisors). 

\subsection{Retrieval Augmented Generation}
We did the first analysis of the properties of \textbf{Dense Passage Retrievers} (as contrasted with sparse TF-IDF based approaches), which are the basic component of retrieval-augmented generation systems. Knowing what information is in a retrieved representation is a pre-requisite to analysing how the retriever influences a downstream generative language model, but there was previously no work applying analysis or interpretability methods to retrievers. Dense passage retrievers are initialised from a pretrained language model, and so we analysed how the information captured in a representation differs for a retriever vs. its language model. We used information theoretic probing to analyse how extractable two important features were from a representation (topic of a passage and gender of a subject). We analysed how these correlated to raw performance and to allocational gender bias. We found that gender extractability did correlate to performance on gender related questions and allocational gender bias, but that allocational gender bias persisted even when gender information was erased, meaning it was not attributable to the representation itself. \textbf{My contribution:} I designed the research question, in collaboration with my internship supervisor Patrick Lewis. I made the research project plan, chose datasets and interpretability methods, and wrote all pipeline code and ran all experiments, and finally wrote up the findings into a paper.

\section{Recommendations}
In light of this body of research, we make the following recommendations. On \textbf{measurement}, we recommend not to use geometric intrinsic measurements of bias (based on cosine-similarity and the like), as they are not predictive of downstream behaviour. This is true for geometric intrinsic measurements, like WEAT \citep{} and CEAT \citep{}, regardless of whether they are applied to a non-contextual embedding like word2vec, or to a language model. These metrics are good for revealing human social biases reflected in the data that trained the model, as was done in the original work of \citet{Caliskan2017SemanticsDA} that inspired the usage of this type of metric. But they are not good for predicting \textit{model} behaviour. We can tentatively recommend instead using information theoretic probing as an alternative and reliably predictive intrinsic metric. However, note that we studied it only for \textit{allocational gender bias in English}. For other types of bias, no intrinsic metric has yet been validated and downstream metrics should still be used until such a time when one is tested and available. Research on other options for intrinsic measurements is nascent, and we recommend always measuring fairness on a downstream task rather than in a language model when possible.
\sgtcomment{should I say why I might expect gender bias to be an exception? A: it is much more strongly encoded in langauge than other demographic signals, save that given by dialect, and so this might affect the extractability to bias relationship}
We recommend that downstream metrics be selected with reference to the desired system behaviour. This may seem simple, but few works in the NLP literature give this consideration, despite that the suite of all downstream fairness metrics is \textit{provably not mutually satisfiable.} Different downstream metrics mean different things \sgtcomment{list like two and give an intuition} and debiasing efforts often will only make sense for some of them. In our work in Chapter~\ref{chapter:gender_bias_probing} we additionally found that information theoretic probing was not predictive of all of them, only of some \sgtcomment{give details on which ones}.


On \textbf{transfer learning}, we recommend to use monolingual transfer learning (also called pretraining) when working with less data, at least for classification. We tested sentiment classification in three language families, so we expect our findings to hold for all similar tasks, but cannot claim to generalise to generative tasks.
However, we recommend to take more care when using cross-lingual transfer learning, as it risks introducing new biases into the target language from other language data. When cross-lingual transfer learning is necessary, we recommend using distilled cross-lingual models, as we found distilled models to have nearly equivalent performance and much lower bias overall than their full-size counterparts.


\sgtcomment{What regarding Retrieval Augmented Generation?}

%Adam reference: https://matt.might.net/articles/advice-for-phd-thesis-proposals/

% NOTES TO SELF:
% TODO need to define what I mean by fairness more? Maybe later (representational, allocational, systematic but arbitrary vs. learned negative associatiosn)

% Walk through the structure of the sections and chapters in a narrative progression
% First part is methodological -- this is how we measure the process, and then we apply this to other things.

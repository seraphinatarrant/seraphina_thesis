%!TEX root = ../thesis.tex
\chapter{Introduction} \label{chapter:introduction}
In the past decade, Natural Language Processing (NLP) systems have come to saturate everyday life. NLP has expanded from being used to translate webpages and recommend new videos to having inescapable reach; it is now also used to moderate social media content$^{\heartsuit}$ \citep{}, to generate answers to any user questions$^{\spadesuit}$ \citep{}, to track public opinion about products or politicians$^{\diamondsuit}$ \citep{}, to sort and filter resumes for potential new workers$^{\clubsuit}$ \citep{}, and myriad other applications.
%, and to track likelihood of committing a crime and recidivism rates$^{\star}$ \citep{}.  
This increase in scope and usage of NLP systems comes with many promises of efficiency, cost reduction, and even social good from scale--promises of (in order of the above examples) reducing hatespeech and aggression online$^{\heartsuit}$, of greater and easier access to information$^{\spadesuit}$, of direct and inexpensive democratic feedback for companies or policies$^{\diamondsuit}$, and of more efficient hiring practices$^{\clubsuit}$ that are less dependent on `who you know' and idiosyncratic instincts of a person.
%, and of better allocation of police resources or more objective, data-driven sentencing$^{\star}$. 

But this territorial expansion introduces many new harms that diminish these promises.  
The often referenced promise of mathematical objectivity---freeing us from human subjectivity, inconsistency, and biases---has proven to be mythical. 
At best, NLP systems learn and propagate these same biases, but with a veneer of objectivity that fosters over-reliance \citep{oneil2016weapons} and reduces accountability and recourse when data is incorrect and decisions go wrong. 
A large growing body of work analysing NLP systems has shown that they do not behave similarly and work equally well for different genders, races, nationalities, and other demographic groups. This is the standard definition of \textbf{fairness}, which we use throughout this thesis: these systems are not \textbf{fair}. 
So given that NLP systems, and the data, models, and optimisation and evaluation metrics they are composed of are \textit{not} inherently fair, we must analyse the ways they are not, so we know what to expect and can mitigate where possible. When NLP systems are not fair, companies and organisations using them (and people subjected to their outputs) are worse off than before automation systems, since this flawed system has now been scaled. An individual Human Resources manager may have flaws and biases, but they work for only one or a few companies and have time to read only so many resumes in a day. Some resumes will be sent to a different person, who may have different biases, preventing the inequities of the first person's views from being complete and consistent over a wide swath of potential jobs. When a flawed and biased NLP HR system is scaled, it does not sleep, get tired, or clock-off and can process as many thousands of resumes as time and compute allows. The same system is used by many companies. The very variability of human behaviour, and the inconsistencies in human decisionmaking that are often considered undesirable, limit the possible scope of each individual's (or even each company or organisation's) biases. This lack of scaling of humans is an accidental safeguard. An NLP system, in contrast, replicates the same biases to an unlimited extent, and whatever unfortunate minorities it is biased against will experience more widespread discrimination. This is the situation of the present day, and sets the scene for this research.

The research world noticed this, eventually. Recognition of fairness problems in NLP began in 2016 and has grown exponentially. By the time of writing, major conferences now have a dedicated track for fairness research\footnote{\url{https://aclrollingreview.org/cfp}}, encourage papers to self-declare potential hazards\footnote{Section A2 in \url{https://aclrollingreview.org/responsibleNLPresearch/}, and section 1c in \url{https://neurips.cc/Conferences/2021/PaperInformation/PaperChecklist}}, and have an ethics committee appointed to review potential fairness problems in any work\footnote{\url{https://www.aclweb.org/adminwiki/index.php?title=Formation_of_the_ACL_Ethics_Committee}}. Yet despite all the attention and effort expended on fairness issues in NLP, we as a community have made only such a small dent in known problems as to be aware of the magnitude of unaddressed fairness problems. Both discovering and addressing fairness problems in an NLP system remains extremely challenging. 

There are multiple ways that a system can be unfair. NLP systems are often not \textbf{allocationally fair}; they and have different accuracies and rates of false positives and false negatives for different demographics. A example such situation is when a toxicity detection system has much higher rates of false positives for text that is actually neutral or positive but contains terms about race, religion, or sexual orientation. In such as case, a sentence like \textit{ I am a gay man} can be flagged as toxic and censored, as was the case with Google's toxicity detection system in 2018 \citep{Dixon2018MeasuringAM}. NLP systems are also often not \textbf{representationally fair}; they reproduce and propagate negative stereotypes for minoritised demographics \citep{crawford_keynote}. For example, prominent generation systems will disproportionately describe women as taking carer roles, and portray racial minorities as criminals \citep{sheng-etal-2019-woman}. There is not even a consensus on how best to measure each type of unfairness. Most metrics used to measure fairness are ad-hoc and have not been standardised or analysed for \textbf{predictive validity}---their ability to predict actual fairness problems that will occur--or \textbf{concurrent validity}---their agreement with other metrics in use. We make some progress towards assessing predictive and concurrent validity of fairness metrics in Part~\ref{part:measurement}.  

Another challenge is that fairness issues can appear at almost any stage of building an NLP system \citep{suresh2021framework}. NLP papers commonly claim that `model biases reflect biases in data they were trained on' but this is both a such a gross oversimplification as to be unhelpful (how did the biases get into the data? Do imbalances in labels over different sensitive groups count, or do only stereotypes count?) as well as incorrect\footnote{I expect the prevalence of this statement is a way of shirking responsibility. It is the data's fault, and society's fault for creating the data, not the fault of the engineer or company}. All choices in the process of training an NLP model have been shown to affect the resulting bias. A resume filtering system can be trained on data in which humans made racist or sexist decisions (\textbf{historical} bias) and that bias will persist, be amplified, and scale, with the authority of an objective AI system behind it. This happened with Amazon's attempt at an AI for Human Resources.\footnote{ \url{https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G}.} A content moderation and toxicity detection system can be unfamiliar with non-prestige dialects and censor them incorrectly, as happened when tweets in African-American Vernacular English (AAVE) were incorrectly flagged as toxic speech \citep{sap-etal-2019-risk}. Even though AAVE is common in the world, it was not well-represented in data the model has seen, resulting in \textbf{sampling}, aka \textbf{representation}, bias). An NLP system often uses labelled data (for supervised learning) where the labels are a proxy for the task that is to be learnt -- e.g. in the resume filtering example \textit{was previously hired based on this resume} as a proxy for \textit{was suitable for the job}. That label can be a better or worse proxy for the desired task. This is \textbf{measurement} bias. These are only a few of the myriad ways that unfairness can enter a system, selected as examples as they are the types that I spend the most time examining below. There are more subtle ways that can make mitigation even more challenging, which we discuss in Part~\ref{chapter:background}.


% Remining things are aggregation (example from hardt) and evaluation (maybe example from my reality check paper) and also learning (the metric being optimised). 

%Fairness issues can arise also in fitting the same function to multiple groups that require different functions (aggregation bias) and in the metric used to evaluate (evaluation bias) \citep{hardt2016equality}.


%it has become a vital responsibilty for us as researchers to examine our models, algorithms, and data as to their behaviour for different demographic groups. Do they behave similarly and work equally well for different genders, different races, different nationalities? We need to know this information to ensure that the systems we build innovate, and improve society, rather than accelerating marginalisation and societal divisions and isolation. 

%These are the ways that unfairness can enter a system. 

%\sgtcomment{PICK UP HERE -- connect to the issues of *scaling* and of *not knowing how something gets into the system. Might need to connect transfer learning to it more smoothly -- since transfer learning is responsible for scale, and basically introduces a new source of all of these types of biases that is unexplored (a second set of data and of sampling and of measurement blah blah). It is also now the dominant paradigm (scale) that has been underexplored. And we want to know if these systems are better or worse than systems before using them, and we also want to pinpoint the source to enable us to mitigate it -- if we performed mitigation at the wrong stage it might not work}

An NLP system can contain one, many, or all of these sources of bias, and this bias can enter in via the data collection, dataset splits, learning objective, model architecture, model deployment choices (such as decoding hyperparameters or classifier thresholds). And worse still is that most of these choices are now made \textit{twice}.  Current scale in NLP is driven by \textbf{transfer learning}, where a model is trained on high resource task(s) or language(s) (e.g. unstructured web crawl text) and then ported to a lower resourced one (e.g. any supervised task requiring labels, like sentiment analysis) -- not necessarily objectively \textit{low resource}, but relatively lower resourced, i.e. with less data than the dominant task or language being used for transfer. 
%Within a language, this is usually done with a language model pre-trained on large scale web text and then applied to a supervised learning task requiring labelled data. Between languages, this is usually done between English and another language with less or no labelled data. 
It was already difficult to pinpoint where biases enter a system, and with transfer learning most systems are composed of multiple sets of training data, multiple objectives, multiple measurements. Transfer learning is now the dominant paradigm in NLP, but previous to the work in this thesis, fairness research considered only one of the two stages: the pre-training or the fine-tuning stage. If a language model that will later be used in our example resume filtering system (which we refer to as an \textbf{upstream model}) has been debiased with regard to gender, will the classifier on top of it (which we refer to as the \textbf{downstream model}) also be debiased, or not? If instead the classifier is debiased, is the language model also safe to use, or will bias then surface if the language model is used in another task, or directly without the classifier? We cannot answer these questions without studying the entire system and learning the relationship between upstream and downstream models. And without these answers,
%further complicating this already multiplex ecosystem of threats to fairness. It is now even harder to track and mitigate biases in NLP systems because they are multi-stage. 
bias mitigation methods or measurements are at best ineffective, and at worst misleading. With these answers we can apply effective bias mitigation strategies at the correct stage of the system, and we will understand the contribution of transfer learning to fairness in NLP systems and be informed as to whether systems are becoming better or worse as they scale. This understanding is a pre-requisite to effective work in NLP bias, and yet before the work in this thesis, the field had little knowledge of it.

So here, in the below, we explore a previously yet unstudied area of NLP fairness; how unfairness, having entered a system, persists and travels throughout it. 

We first focus, in Part~\ref{part:measurement}, on fairness measurement at different stages of transfer learning. No real research can be done without good measures, and we need an understanding of how measures of bias relate at different stages of transfer learning, since interventions are customarily applied at one stage. In Chapter~\ref{chapter:intrinsic_bias_metrics}, we study whether the most common \textbf{intrinsic} bias measurements--at the language model pre-training stage--are predictive of later downstream, or \textbf{extrinsic}, bias in two classification tasks in two languages. We find that they are not predictive, and that the widespread use of these measures has been leading to a false sense of progress in debiasing research. Most work was at the time done on only upstream models, and our work shows that we cannot tell whether debiasing efforts are propagating downstream. Our results show that more effort needs to be spent on measuring bias on the downstream task itself. Following this, in Chapter~\ref{chapter:gender_bias_probing}, we study the relationship of transfer learning measurements in the \textit{reverse} direction. Here we ask how a pre-trained upstream language model changes when different debiasing methods are applied downstream. We find that a new metric, based on information theoretic probing (also known as minimum description length (MDL) probing) \citep{} can, when applied to the pre-trained language model, differentiate between different downstream bias levels, and different downstream debiasing techniques, and show which are more effective. We find that this measure is predictive of how robust debiasing of the pre-trained language model is, and whether the debiasing will remain if that model is then used in another task. These two results together imply that the \textbf{geometry} (cosine or other distance measures, previously used as upstream metrics) of concepts in language model representation space does not reliably predict downstream bias, but the \textbf{extractability} of concepts (as measured by information theoretic codelengths) is better predictor. In that work, we also are the first to use a wide suite of ten downstream fairness metrics that refer to slightly different notions of fairness. We find that though they tend to track together, if we had naively used a subset of them, based on what was most popular for certain datasets, we might have come to a different conclusion. Different metrics are suitable for different applications and scenarios, and they do not always tell the same story. 

% Prior to the work in this dissertation, it has been unknown how fairness (and lack of) in an initial model relates to fairness in a system in which it is used. 

We then use our findings on measurement to conduct experiments addressing a broad question about how the use of transfer learning affects the fairness of a system. There is no previous work on this, but previous work on aspects of transfer learning leads to two competing possibilities of how transfer learning could impact fairness. Does transfer learning \textit{improve} fairness, because the additional data sources lead to overall better models that are better at modelling long tail phenomena \cite{} (and data on minorities is often long tail)? Or does the additional complexity bring in new or magnified undesirable biases, via one of the many mechanisms introduced above? 
%We study this for transfer learning between tasks within one language, and also for transfer learning between different languages. Within one language, will a language model that has been trained on raw text data that underrepresents women in prestige careers in STEM also fail to appropriately classify women's biographies into STEM roles \citep{biosbias} and incorrectly filter women's resumes for STEM positions? 

In Part~\ref{part:crosslingual} we pick a task---sentiment analysis, which we selected since this task enables us to test in a number of languages---and study this effect for transfer learning between \textit{tasks/objectives} (the current dominant NLP paradigm, which we will sometimes refer to as monolingual transfer learning to distinguish it) and transfer learning between \textit{languages}, called multilingual or crosslingual transfer learning (used interchangeably but the field and by us). Prior to our first investigation, previous work had shown that language models trained on unstructured text have gender and racial biases \citep{bolukbasi, Caliskan2017SemanticsDA, zhao-etal-2019-gender, zhao-etal-2020-gender, sheng-etal-2019-woman}. So we asked, will this carry through in monolingual transfer learning and cause gender and racial biases to appear or increase in a downstream sentiment model, beyond what can be attributed to the downstream training data? For instance, let's say that an upstream language model has learnt to associate conventionally negative attributes with certain minorities, such as to represent gay men as doing drugs, and black men as pimps (examples from \citet{sheng-etal-2019-woman}). Will a sentiment classifier built on this upstream language model also associate negative sentiment with gay and black men, \textit{even if} there is little or no data about gay and black men in the sentiment training data? 
Or will that bias be overridden or lost, either because the role of the classifier is strong enough to disregard that, or because the now larger and more expressive system can generalise better to other positive association involving black and gay men, such as stars in politics and arts, or affirmational personal stories, such as those in \cite{Dixon2018MeasuringAM}? We find that, overall, the additional stability from transfer learning is helpful in a resource constrained setting (i.e. one in which you cannot gather more annotated sentiment data), and this effect is enough to reduce overall gender and racial biases (despite new negative associations having been introduced).

We also study this effect for transfer learning between languages, or \textbf{cross-lingual transfer learning}. In this setting, not only can an upstream model learn biases from multiple data sources, but also from multiple languages. Exactly how much information cross-lingual transfer learning shares across languages is not well understood and there are some contradictory empirical studies \citep{}. So we ask, in cross-lingual transfer learning, if a language model has learnt harmful stereotypes in one language, can those negative associations carry across languages? In the above example where a model has learnt negative associations \textit{in English} about black and gay men, will a classifier in Japanese have these same associations, if they do not occur in Japanese? Can the collision between competing stereotypes in different languages weaken them, and in effect fight bias with bias? \citep{stanovsky-etal-2019-evaluating}. Can anything be done in the initial task before transfer, to ensure better outcomes in the second task? We find that, contrary to what we found in monolingual transfer learning, cross-lingual transfer learning tends to (with exceptions) exacerbate biases, though this effect can be mitigated with distilled/compressed models with little loss in performance. 

In Part \ref{part:generation}, we look at a third type of system: retrieval augmented generation, which presents an inversion of the standard transfer learning setup. In the standard setup,  a language model feeds into a classifier, and in retrieval augmented generation, the classifier selects source documents to answer a query, and this feeds into a language model, which conditions on those documents to generate an answer. This inverted system allows us to also ask the reverse question: if a language model has learnt problematic associations and stereotypes, can these be counteracted by conditioning on source documents? For instance, if a language model generates results about women predominantly in low-prestige roles, will it change this if it is conditioned on source documents about female CEOs and doctors? Or is it more likely to ignore the source information in this case then in the case of male CEOs and doctors? Or, as a third option, the retriever itself is biased, and doesn't select documents about female CEOs, so we never even get to that point?
However, prior to our work, not only was there no research examining how fairness flows between retrieval models and generative language models, there was little research analysing neural retrievers at all. So we began by asking the sub-question, inspired by all our work in Parts~\ref{part:measurement} and \ref{part:crosslingual}: a retriever representation is necessarily a compression of a document, so what information is actually in this representation, such that 
 a language model can condition on it? (Recall Chapter~\ref{chapter:gender_bias_probing} where information in a representation as measured by information theoretic probing is most predictive of bias). Is information about demographics--gender, race, etc--in a retriever representation predictive of allocational bias in retrieved results? That is, does a retriever with stronger information about gender pick documents about gender more unequally? We do a case study in allocational gender bias and find that, though retrievers quite strongly encode gender in their representations, allocational bias is not attributable to the representations themselves. This bias persists even when we remove gender from the representation, meaning that it comes from either the composition of the corpus or the queries themselves. We leave completing the high level question of studying what happens when a language model uses these representations to future work. 

 We conclude with a summary of our contributions, and with a set of recommendations in light of our findings.

\section{Contributions}
We make contributions to three broad categories: 
\begin{enumerate}
    \item More meaningful and reliable \textbf{measurement} of fairness in language models
    \item Analysis of how \textbf{transfer learning} affects fairness
    \item Analysis of fairness in \textbf{retrieval-augmented generation}    
\end{enumerate}

\subsection{Measurement} 
\textbf{Chapter~\ref{chapter:intrinsic_bias_metrics}}

We did the first study evaluating whether the most commonly used fairness metric for upstream language models correlated with downstream fairness. At the time, upstream only studies comprised one third of fairness research, making this a vital question. We examined the relationship between upstream and downstream metrics across a broad set of experimental conditions: two types of bias (gender, racial), two different tasks (coreference resolution and hatespeech detection), two different languages (English and Spanish), two common embedding algorithms (fastText and word2vec), two common methods of debiasing (preprocessing training data, and post-processing on representations), and two downstream fairness metrics (difference in precision and difference in recall). This is a much broader scope than most fairness research at the time this was done. We found that the common upstream metric, based on cosine similarity, was not predictive of downstream bias. This changed the focus of the fairness field toward evaluating bias downstream and finding upstream metrics that are more predictive. Our work has inspired follow up studies examining the predictive validity of fairness metrics \citep{cao-etal-2022-intrinsic, others?}, which further extend and corroborate our findings. 

\textbf{My contribution:} I designed the research agenda: I envisioned the research question and wrote a research plan document with methods, goals, metrics, and a literature review. I recruited and subsequently supervised three MSc students who implemented pipelines for three different systems and did initial investigations into the correlation between intrinsic and extrinsic metrics for their MSc theses. I gathered these pipelines together, modified and extended them, ran experiments, and wrote and presented the paper, with the help of Adam. 

\textbf{Chapter~\ref{chapter:gender_bias_probing}}

We also did the first study investigating how debiasing \textit{downstream} (rather than upstream) affects language model (upstream) representations. We focused on gender bias in English and considered two common transformer models, two tasks (coreference resolution, biography classification), three debiasing methods, two different intrinsic metrics (a contextual extension of the cosine similarity metric from the previous work and a new one, MDL compression, that we proposed adapted from \citet{voita-titov-2020-information}), and ten downstream fairness metrics. We found that our new metric was predictive of whether the upstream model had been successfully debiased, and correlated well with most downstream metrics. We also found that not all downstream fairness metrics correlated to each other, highlighting the importance of not relying overly much on one metric. 

\textbf{My contribution:} I was second author on this paper, assisting the first author Hadas Orgad who proposed this extension of the work in Chapter~\ref{chapter:intrinsic_bias_metrics}. I implemented some of the metrics on intrinsic analysis of language model representations, implemented the additional extrinsic fairness metrics (which are now open-sourced), and co-wrote the paper with Hadas Orgad and Yonatan Belinkov.

\subsection{Transfer Learning}
\textbf{Chapters~\ref{chapter:multilingual_sentiment_analysis} and \ref{chapter:multilingual_sentiment_analysis_pt2}}

We did the first research on the effect of both standard (monolingual) transfer learning and cross-lingual transfer learning on gender and racial biases in sentiment analysis. We first examined whether, for five languages (Japanese, Chinese, Spanish, German, English) monolingual transfer learning (via pre-trained models) changed the biases in sentiment analysis systems. We then ran similar experiments for the much more complex setup of multilingual transfer learning (via multilingual models and via cross-lingual labelled data). We found that, though the story is reasonably complex, cross-lingual transfer learning can increase bias even in unexpected cases such as culturally specific racial biases. Monolingual transfer learning usually reduces biases, even though the training data used for transfer contains new biases. It stabilises the model and that effect outweighs bad content learnt in pre-training. 

\textbf{My contribution:} I did this project almost entirely on my own save for the writing, which my supervisors Adam and Björn assisted with greatly. I designed the research question and outlined the project, developed and programmed the experiment framework, found training data, created evaluation data (with the help of native speakers of each language) and wrote up the results (with the help of my supervisors). I received regular weekly consultation from a team at Amazon Barcelona for the first of the two projects.

\subsection{Retrievers}
\textbf{Chapter~\ref{chapter:contrievers}}

We did the first analysis of the properties of \textbf{Dense Retrievers} (as contrasted with sparse TF-IDF based approaches), which are the basic component of retrieval-augmented generation systems. Knowing what information is in a retrieved representation is a pre-requisite to analysing how the retriever influences a downstream generative language model, but there was previously no work applying analysis or interpretability methods to retrievers. Dense retrievers are initialised from a pre-trained language model, and then further trained and adapter to excel at determining the relevance of a document representation to a given query, such as returning all the articles about Finnish prime ministers given the query, `Who is the prime minister of Finland?'. 
We analysed how the information captured in a representation differs for a retriever vs. the language model it was initialised from. We used information theoretic probing (based on the results in Chapter~\ref{chapter:gender_bias_probing} that is was predictive of bias) to analyse how extractable two features were from a representation: topic of a passage and gender of a subject. We analysed how these correlated to raw performance and to allocational gender bias. We found that gender extractability did correlate to performance on gender related questions and allocational gender bias, but that allocational gender bias persisted even when gender information was erased, meaning it was not attributable to the representation itself. We thus show another case when an entire system has to be considered in debiasing an NLP system.

\textbf{My contribution:} I designed the research question, in collaboration with Patrick Lewis. I made the research project plan, chose datasets and interpretability methods, and wrote all pipeline code and ran all experiments, and finally wrote up the findings into a paper, with very minor proofreading from three colleagues.

\section{Recommendations}
In light of this body of research, we make the following recommendations. 

On \textbf{Measurement}, we recommend not to use geometric intrinsic measurements of bias (based on cosine-similarity like WEAT \citep{} and CEAT \citep{}), as they are not predictive of downstream behaviour. This is true regardless of whether they are applied to a non-contextual embedding like word2vec \citep{}, or to a language model like BERT \citep{} or RoBERTa \citep{} and company. These metrics \textit{are} good for studying human social biases via what is reflected in the data that trained the model, as was done in the original work of \citet{Caliskan2017SemanticsDA} that inspired the usage of this type of metric.\footnote{Though for this type of use case we note that RIPA \citep{} is likely better, or at the very least word frequencies need to be normalised for results to be valid.} But they are not good for predicting \textit{model} behaviour. We can tentatively recommend instead using information theoretic probing as an alternative and reliably predictive intrinsic metric. However, this recommendation comes with two limitations: we studied information theoretic probing only for \textit{allocational gender bias in English}. First, gender encoding differs greatly in different languages (more than other demographics) due to gender agreement systems, so these findings should be validated in more languages before being trusted beyond English. Second, even including English, other biases may not be stored the same way (for the same reason of the grammaticality of gender). So for other types of bias, no intrinsic metric has yet been validated and downstream metrics should still be used until more research has been done. Research on other options for intrinsic measurements is nascent, and we recommend always measuring fairness on a downstream task rather than in a language model when possible.
%\sgtcomment{should I say why I might expect gender bias to be an exception? A: it is much more strongly encoded in langauge than other demographic signals, save that given by dialect, and so this might affect the extractability to bias relationship}
We also recommend that downstream metrics be selected with reference to the desired system behaviour. This may seem simple, but few works in the NLP literature acknowledge this, despite that the suite of all downstream fairness metrics is provably not mutually satisfiable, so you do actually have to pick one as a constraint. Different downstream metrics mean different things, and debiasing efforts often will only make sense for some metrics. Equalised false positive rates make more sense in the context of content moderation or toxicity, where the risk is censorship, equalised false negative rates make more sense for resume screening where the risk is excluding people from the potential to interview. In NLP, we often try to avoid making normative decisions about the world that our models will be embedded in; it is a messy and complex world, even more so than our data. Part of the brittleness and unreliability of bias evaluations and bias metrics--poor predictive and concurrent validity--is that researchers don't think these through and make them explicit. Each debiasing method only make sense for some type of bias, and our better intrinsic metric from Chapter~\ref{chapter:gender_bias_probing} still only correlates with most extrinsic measures, there is a family of measures that it does not work for. Fairness researcher do need to engage with the world they are imagining and how it should work. All fairness work contains an assertion like this, and if left implicit, it can be scientifically messy. So we recommend that researchers make explicit, reasoned choices about why they use the metrics that they do. 

On \textbf{Transfer learning}, we recommend to use monolingual transfer learning (also called pre-training) when working with less data, at least for classification. We tested sentiment classification in three language families, so we expect our findings to hold for all similar tasks, but cannot claim to generalise to generative tasks.
However, we recommend to take more care when using cross-lingual transfer learning, as it risks introducing new biases into the target language from other language data. When cross-lingual transfer learning is used, we recommend using distilled cross-lingual models, as we found distilled models to have nearly equivalent performance and much lower bias overall than their full-size counterparts. We recommend also the use of two of our analytical methods: causal or counterfactual evaluations, combined with a granular heatmap based analysis of the results.

On \textbf{Retrievers}, we recommend to analyse the entire system: corpus, queries, and model representations, as our work shows that a model constrained to have perfectly fair representation may still create an unfair system because of the other components. From the extensive experiments on random seed initialisations in this section, and the smaller scale experiments in the previous, we also recommend to test models based on a large number of random initialisations. We found this to have a disproportionate effect on model fairness and model performance both. In cases where trustworthy evaluations are available, ones which are faithful to a use case and which generalise, they can be used to select a seed with better generalisation properties for fairness, and this difference can exceed the difference from any common debiasing approaches or interventions. In cases where this is not possible, we recommend using majority voting across three to five random seeds, to minimise by seed variance. 

%Adam reference: https://matt.might.net/articles/advice-for-phd-thesis-proposals/

% NOTES TO SELF:

% First part is methodological -- this is how we measure the process, and then we apply this to other things.

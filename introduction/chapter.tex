%!TEX root = ../thesis.tex
\chapter{Introduction} \label{chapter:introduction}
As Natural Language Processing systems quickly saturate everyday life, it has become a vital responsibilty for us as researchers to examine our models, algorithms, and data as to their behaviour for different demographic groups. Do they behave similarly and work equally well for different genders, different races, different nationalities? We need to know this information to ensure that the systems we build innovate, and improve society, rather than accelerating marginalisation and societal divisions and isolation. 

The exponential growth of transfer learning -- from nothing in 2015 to near ubiquity today -- has a complex relationship to fairness in NLP. Transfer learning, where a model is trained on high resource task(s) or language(s) and then ported to a lower resourced one\footnote{Not necessarily objectively \textit{low resource}, but relatively lower resourced, i.e. with less data than the dominant task or language being used for transfer.}, enables the expansion of NLP models to tasks and languages that do not have the resources (financial, computational, engineering) to train models from scratch. This increases access for these communities %TODO Add Examples
to technology and information, and decreases marginalisation. However, it creates serious challenges for ensuring that models are fair; that they have similar performance for different demographics (e.g. false positive in hate speech example TODO) and that they do not propagate harmful stereotypes of use them in decision making. It makes the first stage of data intensive development modular, and divorces it from downstream applications in which it will be used. How does an engineer training a model on language modelling for English CommonCrawl\footnote{define what this is, also talk about the examples of the famous problems in it as an introduction to some bad stuff about data in transfer learning} know whether the changes and improvements they have made to their task performance will make a system more or less fair when their model is applied to a task downstream? (should I cite my paper here?)  Will a new model with #Add Improvement# be even less likely to predict that women can be doctors\footnote{One of the most famous problems in NLP fairness literature, TODO add some citations and explain the spiel how this is actually not reflecting world bias it is reflecting selection bias (maybe explain that later, but this sets it up...TBD)} or will it be better able to generalise to less frequent and unseen cases? (which will improve its fairness outcomes). Will an English model that has learnt to discriminate against women (TODO cite, maybe amazon study) carry that discrimination through when it is applied to another language? Can anything be done in the initial task before transfer, to ensure better outcomes in the second task?
These relationships and outcomes are difficult to predict, but it is important to avoid the paradoxical situation of extending access to lower resourced settings (TODO examples or concreteness) and then ending up further marginalising them by unforseen and unfair model behaviour. 

In Section \ref{TBD} we examine how fairness in the first language modelling task relates to fairness in downstream tasks, and when the relationship between the two is and is not reliable and predictable (TODO should I put my findings here?).
Subsec: Intrinsic Bias Metrics Do Not Correlate with Application Bias
Subsec: How Gender Debiasing Affects Internal Model Representations, and Why It Matters

In Section \ref{TBD} we examine how transfer learning across languages affects fairness outcomes, and how this relates to linguistic properties (wc?) of these languages.
Subsec: Amazon paper
Subsec: Swahili representations

Finally, we do interventions and generations (either I focus on what happens if you try to control it, OR on representation bias, which is generation, or both.)

We conclude with a summary of our contributions and what recommendations we would make to the ethically aware NLP researcher or engineer. We suggest future work in light of our findings that can continue the journey (gross, wc) to expanding NLP to a wide and diverse range of settings and languages, in an aware (wc) way that is equitable for diverse (sp) users and scenarios. 

NOTES:
TODO need to define what I mean by fairness more? Maybe later (representational, allocational, systematic but arbitrary vs. learned negative associatiosn)

Walk through the structure of the sections and chapters in a narrative progression





Fairness research has grown exponentially over the
past five years, in step with the concurrent growth
of multilingual Natural Language Processing
and cross-lingual transfer learning.1 But the
two are rarely combined, which leaves a large
gap in research and understanding. This gap is
dangerous, as it risks further marginalising already
marginalised communities (which are the targets of
transfer learning). 
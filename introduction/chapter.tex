%!TEX root = ../thesis.tex
\chapter{Introduction}


% \epigraph{``Always remember the importance of words.''

% ``And numbers,'' added the Mathemagician forcefully.

% ``Surely you don't think numbers are as important as words?'' he heard Azaz shout from the distance.}{Norton Juster, \textit{The Phantom Tollbooth}}

\epigraph{``Maybe if I buy some I can learn how to use them,'' said Milo eagerly as he began to pick through the words in the stall. Finally he chose three which looked particularly good to him---``quagmire,'' ``flabbergast,'' and ``upholstery.'' He had no idea what they meant, but they looked very grand and elegant.
}{Norton Juster, \textit{The Phantom Tollbooth}}

% \epigraph{
% % ``Is this the right road for Dictionopolis?'' asked Milo, a little bowled over by the effusive greeting.
% % 
% % ``Well now, well now, well now,'' he began again, 
% I don't know of any wrong road to Dictionopolis, so if this road goes to Dictionopolis at all it must be the right road, and if it doesn't it must be the right road to somewhere else, because there are no wrong roads to anywhere.}{Norton Juster, \textit{The Phantom Tollbooth}}
% \hspace{10mm}


\textbf{Language models} (LMs), a crucial element of applications that require text generation, have seen many implementations in their decades-long history. The current the era of artificial neural networks has seen a surge in exuberant claims of underlying linguistic competence. However, we still see a lack of accompanying serious investigation into how modern models like LSTMs~\citep{hochreiter_long_1997} actually learn.

For years, the LM world was dominated by n-gram models~\citep{shannon_mathematical_1948,rosenfeld_two_2000}. These models would directly store sequences of words and record their probabilities, holding to a Markov assumption that history was only relevant a predefined distance back. In an n-gram model, we would record a table directly mapping ``sentence'', ``example sentence'', and ``silly example sentence'' to the number of times they were observed in a training corpus. Confronted with n-gram models, it would have been unusual to claim that such a table fundamentally represented syntax.\footnote{This is not to say such claims were not made. A Markov model based on shifting windows (easily implemented with an n-gram lookup table) seems to be the basis of the Sausage Machine, which \citet{frazier_sausage_1978} provided as a model of human language processing. Later, \citet{wanner_atn_1980} would declare ``the Sausage Machineâ€™s putative explanation of \ldots behavior \ldots is simply incorrect'', criticizing the over-interpretation of a mechanical model, in a tradition that continues to this day in NLP.} But modern language models, from \textbf{Transformers}~\citep{vaswani_attention_2017} and other \textbf{fully attentional models} \citep{brown_language_2020,liu2019roberta,Devlin2019BERTPO,yang_xlnet_2020} trained on petabytes of internet data, to more lightweight \textbf{Long Short Term Memory networks} ~\citep[LSTMs; ][]{hochreiter_long_1997}, 
are frequently subject to such claims~\citep{linzen_assessing_2016,lakretz_emergence_2019,kuncoro_lstms_2018,lu_influence_2020,vig_analyzing_2019,htut_attention_2019,vashishth_attention_2019,wiegreffe_attention_2019,clark_what_2019,voita_analyzing_2019,marecek_balustrades_2019}. 
\citet{gulordava_colorless_2018}, for example, suggested that ``the ability to capture structural generalizations is an important aspect of what makes the best RNN architectures so good at language modeling''---or even further, ``Recurrent neural networks empirically generate natural language with high syntactic fidelity.''~\citep{hewitt_rnns_2020}. For example, LSTMs prefer ``\textit{complains}'' over ``the philosopher the Greeks like \textit{complain}'', modeling relative clauses syntactically rather than just using the most recent noun's (``Greeks'') inflection~\citep{gulordava_colorless_2018}.
 
The design of these opaque modern models does not explicitly encode human linguistic intuitions such as underlying syntactic structure---and yet, their success at language modeling and generation seems to indicate that the trained models encode these intuitions, enough that researchers ask questions about \textit{how}. A variety of methods have therefore been employed to test these models for syntactic structure. Many methods analyze the intermediate representations and internal behavior of language models. Some probe the representations with small classifiers, looking for properties like part of speech implicitly encoded in the vectors that represent particular words~\citep{belinkov_what_2017,voita_information-theoretic_2020}. Others consider whether the geometry of these representational spaces reveals the connections between words in a sentence~\citep{hewitt_structural_2019}. Still other tests consider what words a model pays attention to at each step in the sequence~\citep{voita_analyzing_2019,clark_what_2019}. These methods share a property: they examine the fully trained model, considering only how it encodes language after it has been fully trained. This dissertation moves beyond such a limited analysis, developing an understanding of the model by investigating the entirety of the training process.

\section{The Lens of Inductive Bias}

Often, questions reach beyond a particular set of parameters and behavior, asking questions about the \textbf{inductive bias} associated with a model. This term is often nebulous in use, but broadly it refers to the assumptions made by a learning algorithm. This set of assumptions form a resulting model's biases---in particular, those that don't come from the training data itself. No set of biases are universal; for example, biases that support language modeling on English might damage the same model's performance on Chinese. Such trade-offs are a proven limit of learning algorithms, demonstrating a rule known as the No Free Lunch Theorem \citep{Wolpert1997NoFL}.

% Inductive bias is distinct from the \textbf{capacity} of the model architecture. The capacity here means whether \textit{any} parameter setting of our model could solve the problem we targeted. In our world of highly overparameterized nonlinear neural networks with simplistic target tasks, architecture capacity is rarely the limitation under discussion. Instead, we consider whether the \textit{training process} is biased to lead to representations that exhibit some useful property of the data, such as a tendency towards hierarchy or a focus on a highly frequent words. 

% The role of the training process, including the optimizer chosen and other regularizing elements, is clearly significant. \citet{levy_neural_2014} illustrated that the word2vec~\citep{mikolov_distributed_2013} objective was equivalent to a shifted Positive Pointwise Mutual Information (PPMI) score, but could not achieve the same performance with a representation learned as a matrix decomposition with the same PPMI objective. This gap is likely because the training process is essential for implicitly regularizing the skipgram objective.

\citet{geirhos_shortcut_2020} describe inductive bias as consisting of four components:

\begin{enumerate}
    \item \textbf{Structure:} The functions chosen to combine parameters and inputs. In this thesis, architectures are centered around an LSTM module, though many modern language models tend to be fully attentional and use Transformers\footnote{Throughout the thesis, we note how our methods and results can generalize to attentional models.}. 
    \item \textbf{Experience:} The training data used to train the model.
    \item \textbf{Goal:} The objective optimized in training. Most neural classifiers use a cross-entropy loss function, but \citet{geirhos_shortcut_2020} point out that regularizers are often used to prevent overfitting by memorizing training data or using other undesirable heuristics.
    \item \textbf{Learning:} The optimization process itself. This includes decisions such as whether to use gradient clipping or some implementation of momentum.
\end{enumerate}

In practice, however, we cannot disentangle these factors entirely\footnote{We can still try, with dataset manipulation (Section \ref{sec:interrogation_data_sets}), ablation tests, or causal analysis.}. An optimizer runs on a loss surface defined by the relationship between goal and structure, as guided by experience. These different factors are selected specifically \textit{expecting} the others, e.g., while we use Adam~\citep{Kingma2015AdamAM} to help optimize an LSTM for language data, it might not help the same architecture to memorize random noise. As \citet{wang_curious_2020} put it, the simultaneous consideration that has gone into choosing a goal, engineering an architecture, and designing an optimizer to match means that every modern neural network is ``a consequence of data-driven optimization, engendering the inductive bias---the free lunch is paid for by all the unfit that failed to survive natural selection''. Because the aspects of inductive bias have all been optimized jointly, we have presented empirical results that treat the learning process as a whole, rather than force our expectations onto a particular architecture or corpus.

\subsection{The Role of Inductive Bias in NLP}

Inductive bias is crucial to the performance of language models, or any model handling natural language---indeed, any predictive model at all. Although massive Transformers are the high-performance darlings of the current NLP landscape, recurrent operations like those in the LSTM are still valuable. The smaller the training data, the more crucial inductive bias becomes to prevent the model from memorizing  or adopting overly complex heuristics. 

\citet{tran_importance_2018} found that tasks which required a model to represent latent hierarchical structure, such as subject-verb agreement \citep{linzen_assessing_2016} and  the artificial logic language of \citet{bowman_tree-structured_2015}, saw higher performance from RNNs than fully attentional models. They therefore  demonstrated the role of the architecture itself in  imposing inductive bias.

The architecture is not the only element in inductive bias, however. \citet{abnar_transferring_2020}  connected inductive bias back to the goal during learning by demonstrating how fully attentional models benefited by learning from the representations produced by recurrent models. \citet{levy_neural_2014} illustrated that the word2vec \citep{mikolov_distributed_2013} objective and regularizer were identical to a combined objective that could be optimized by matrix factorization, but they could not achieve the same performance with a representation using this equivalent matrix decomposition. This performance gap is likely because the optimizer is essential for implicitly regularizing the word2vec objective, in addition to the goal and architecture.

If any ML domain were oriented towards an investigation into the biases of training processes, it is language. For decades, linguists have asked whether humans have some inherent inductive bias that points us towards linguistic structure~\citep{chomsky_language_1988}, or whether we use a generic neural architecture that gradually learns language the same as it would any other set of rules: through overwhelming input and feedback. This question is not answered by studying how a person processes a sentence after a lifetime of language exposure. Instead, linguists apply crucial tools from the field of developmental linguistics, which studies how human language is acquired over the course of a child's development. Our goal in this thesis is likewise to present empirical work on the acquisition of language in artificial neural networks.

\textbf{We claim that in order to understand the biases that lead to high performing language models---and all models that handle language---we must analyze the training process.} Studies of how a model optimization regime works in practice, i.e., of \textit{training dynamics}, will reveal how structural patterns and linguistic properties such as syntax are gradually built up hierarchically, revealing \textit{why} neural language models learn so effectively and how they ``see'' the data.

If we wish to explore questions about how LSTMs learn language, we might be tempted to appropriate methods directly from developmental linguistics. However, in this work we avoid framing artificial neural networks as cognitive models. We instead use specific and mathematical tools for investigating language model training dynamics, such as synthetic datasets and similarity-based model comparisons. These tools can take advantage of crucial differences between child development and model training: we have access to activations, weights, and gradients in a learning model, and can manipulate learning behavior directly or by perturbing inputs. The tools we use often come from computer vision research, as research on training dynamics tends to focus solely on vision tasks, but language has well-documented and intuitive latent hierarchical structures (e.g., syntax and semantics) which make it an ideal domain for exploring the effect of training dynamics on the representation of such structure.

\section{Structure of Thesis}

\textbf{We claim that training dynamics provide an essential view for understanding language structure in LMs.} We support this claim with examples from several case studies, introducing new tools for inspecting neural networks during training. Along with background and commentary, we illustrate these tools, and investigate what they tell us about the development and bias of language models. The novel work in this thesis appears in order from the most conventionally language-motivated (``NLP'') to the most conventionally domain-abstracted (``machine learning''). This leads to a chapter structure as follows:

\paragraph*{Chapter~\ref{chapter:linguistic}: Background: Language Structure in Models} This thesis lies at the intersection of two existing fields. The first is NLP interpretability, which focuses on understanding why text models work by investigating how their behavior expresses fundamental linguistic properties, in particular syntax and semantics. The interpretation methods we focus on show how these structures result (or fail to result) in the biases of model architectures and in the behavior of trained models.

\paragraph*{Chapter~\ref{chapter:dynamics}: Background: Training Dynamics} The second field we survey is training dynamics, which aims to describe how a model changes as it trains, often investigating the biases that allow a model to learn effectively. We discuss how a number of works imply that, at varying scales, training happens in phases. We then discuss how the movement of weights during training responds to the underlying structure of input data. Finally, we consider a variety of methods commonly used to investigate learning dynamics and the conclusions about training that they have reached. Some of these methods (particularly SVCCA; Section~\ref{sec:svcca}) are used directly in this thesis, while others have led to conclusions about the training process that inspire further work here.

% \paragraph*{Chapter~\ref{chapter:data}: Data Scheduling} The implications of the training process  in imposing biases towards linguistic structure has not been entirely neglected by the NLP community. However, interest until recently has taken the form of questions around the effect of data scheduling, that is, considerations of when particular examples of language would be introduced to a model. These experiments are often inspired by intuitions about how complex language structures emerge over the course of training. In this chapter, we survey the static and dynamic data scheduling methods in use, and present results as to the practical effects of basic scheduling heuristics on a simple LSTM language model.

\paragraph*{Chapter~\ref{chapter:svcca}: Beyond Diagnostic Classifiers: Probing Language Model Development} How does an LSTM LM's acquisition of language information vary, when we compare local syntactic properties with source document information? We discover that conventional diagnostic classifiers, a popular method for assessing the linguistic properties of language models, are not sensitive enough to capture changes during training, so we develop an alternate method. We apply SVCCA, a simple and flexible similarity measurement, to compare the development of LSTM language models predicting the next \textit{word} with LSTM taggers predicting \textit{general categories} (part of speech, semantic tag, and source; which vary in the type of data used for accurate tagging). We find that part-of-speech is learned earlier than topic, indicating that local structure is learned well before long-distance information. This discovery inspired work on hierarchical construction of meaning in Chapter~\ref{chapter:interdependence}. We also find that an LM's recurrent layers become more task-independent over the course of training, while an embedding layer becomes more task-specific later in training. We point out that the tendency to lose shared input structure later in training resembles the predictions of a controversial theory about phase-based learning, the Information Bottleneck Hypothesis.

\paragraph*{Chapter~\ref{chapter:interdependence}: Beyond Probing: The Development of Word Interdependence} Why do hierarchical structures like syntax tend to emerge in LSTMs? We move from a view of the output representation space as a whole to focus on local interactions between word pairs, asking how an LSTM moves from shorter relations like those required for POS prediction to longer relations like those required for topic tagging in Chapter~\ref{chapter:svcca}. We measure interactions between word pairs by extending a recent method called Contextual Decomposition to a measure we call Decompositional Interdependence. Applying this measure in a set of experiments on synthetic languages supports a specific hypothesis about how hierarchical structures are discovered over the course of training: that LSTMs rely on smaller constituents as scaffolding for larger trees, rather than learning longer-range relations (e.g., ``either''/``or'') independently of their intervening constituents. 

\paragraph*{Chapter~\ref{chapter:sparsity}: Beyond Words: The Development of Feature Sparsity} How does the distribution of vector unit magnitude and neuron importance change over the course of training? Here, we  investigate a tendency in LSTMs to change in the sparsity of their gradients and activations over time. We find that frequent input words are associated with sparse activations, while frequent target words are associated with dispersed \textit{activations} but sparse \textit{gradients} (which relate to neuron salience). We find that gradients associated with function words are more sparse than the gradients of content words, even controlling for word frequency---could this sparsity signify a stronger role in defining linguistic structure? These properties change dramatically over time, with some layers beginning dense and growing sparser while others remain stable. We consider whether the gradient sparsification which we observe may be an expression of a compression phase from the Information Bottleneck Hypothesis, similar to the link from Chapter~\ref{chapter:svcca}.

\paragraph*{Chapter~\ref{chapter:conclusion}: Conclusion} We discuss recent developments building on the work in this thesis. Based on these developments in addition to our own work, we explore the implications of this thesis and possible future directions.

\section{Contributions}

\paragraph*{A Note on Contributions } All papers reprinted in this thesis are joint work with my supervisor, Adam Lopez. In all cases, I conceived and implemented the original ideas, and Dr. Lopez helped strengthen these ideas through discussion, especially by challenging them and demanding more specific claims and stronger evidence. The papers themselves were written primarily by me, with Dr. Lopez editing, ``killing my darlings''~\citep{quiller-crouch_art_1916}, and adding some of the visualizations.

\begin{itemize}
    \item Understanding Learning Dynamics of Language Models With SVCCA
    \begin{itemize}
        \item To our knowledge, this is the first in-depth study of learning dynamics of neural language models.
        \item We introduce a flexible new probing method based on model similarity, which enables us to compare learned representations across time and across models. We compare the representations produced by language models (word predictors) and tag predictors. This probing method does not require us to have annotated evaluation data, and is efficient to train because it is based on simple matrix factorizations.
        % \item Comparing a single model with itself, we find that closer layers are more similar and that they are less correlated than they are with the same layer of a differently initialized model.
        % \item We use comparisons to empirical upper bounds (same task, different initialization) and lower bounds (randomized tags) to aid interpretation of similarity information. 
        % \item These high lower bounds indicate that much of the similarity between models is caused by their memorization of arbitrary input associations, rather than representation of linguistic structure.
        % \item We compare language modeling to both next-tag prediction and input tagging.
        \item We find that coarse part of speech is learned first, and topic information is learned last, with fine-grained and semantic information learned in between. Early in training, models targeting different tasks with the same inputs tend to produce similar representations, and then specialize to their tasks.
        \item Different layers exhibit different behavior. Recurrent layer representations become more generic in late training, but embedding layers become more specialized to their task late in training. However, embedding layers remain very generic throughout training, explaining the effectiveness of pretrained embeddings to initialize representations for other tasks.
        % \item Nearly every task shows an initial decline in similarity after the first epoch. This indicates that the language model finds an initial representation with properties shared by all tasks, before learning a more task specific representation.
        % \item When looking at input tagging instead of next-tag prediction, we find that more granular tagging is more similar. This suggests that complex tag information is retained about input, even though only simple traits are ultimately used in prediction (i.e., language modeling).
    \end{itemize}
    \item LSTMs Compose---and Learn---Bottom-up
    \begin{itemize}
        \item We develop an extension of Contextual Decomposition called Decompositional Interdependence (DI). DI computes the level of nonlinear interactions there are between two words at a particular timestep, indicating the level to which they influence each other's ``meaning'' in context.
        \item We test DI on an English LSTM LM, finding that when average DI is stratified by the word pair's sequential proximity (which is highly correlated with  DI), higher DI indicates closer syntactic distance. This trend holds regardless of whether the words considered are closed or open POS classes.
        \item We introduce the idea of a subtree acting naturally as \textit{scaffolding} to build an interrelated meaning for a nearby item while predicting the next term in a sequence.
        In order to test the idea that known constituents act as scaffolding for longer-distance relations in LSTMs, we create a synthetic dataset with long distance bracketing expressions surrounding constituents of varying familiarity.
        % \item We find that an LSTM trained with familiar constituents cannot generalize across domains (with different constituents).
        % \item We use CD to find that long distance rules are required faster with familiar constituents.
        We find that poor generalization performance after training with familiar intervening spans can be attributed to the high interdependence between the constituent and the opening symbol of the bracketing expression.
        We conclude that LSTM learning is biased towards bottom up learning, using a known constituent as a scaffold to support new long distance rules.
        % \item We confirm that the gradient passed back by the LSTM further than the constituent is related to the familiarity of the constituent and, even more obviously, that more frequent rules and shorter-distance connections are learned faster.
        % \item Our results indicate that predictable patterns shape the representation of symbols around them by composing their meaning in a way that is intertwined with these predictable patterns themselves.
    \end{itemize}
    \item Sparsity Emerges Naturally in Neural Language Models
    \begin{itemize}
        
        \item This work represents the first application in NLP of the Taxi-Euclidean norm for measuring soft sparsity.
        % \item We find that the sparsity of a word's gradient at the final layer increases with exposure: this applies to both training time and word frequency.
        % \item We investigated content (open class) and function words (closed class) vector representations, but unlike previous work we control for word frequency by reserving only words of similar frequency in both sets.
        % \item We also factor in word frequency by looking at correlations with frequency rather than raw magnitudes.
        \item We find that gradient concentration at the final RNN layer depends on a target word's POS class (open or closed), even after controlling for word frequency. Frequent target words from closed classes start out with highly concentrated gradients and soon stabilize, while frequent words from open classes continue to become more concentrated throughout training. We posit that this represents a transition from learning basic syntactic structure to learning general content words.
        % \item We confirm that these effects are not a result of concentrated activations, because activation sparsity is not correlated with target word frequency.
        % \item We investigate the correlation between sparsity and word frequency in both activations and gradients. Where this correlation is noteworthy, it is strongest at the most frequent words.
        \item We find that the recurrent layers quickly learn to correlate sparsity with word frequency in their activations, but gradually the embedding layer surpasses the recurrent layers in this respect as the network converges. 
    \end{itemize}
\end{itemize}
\chapter{Conclusion}\label{chapter:conclusion}

%“For every subtle and complicated question, there is a perfectly simple and straightforward answer, which is wrong. —H.L. Mencken”
In this section I will condense the takeaways from my work, then expand them into the questions they pose. I'll discuss implications for future work, and do a bit more theorising than in each individual conclusion, made possible by the aggregation of all the works. I will also relate my work to the present day mania for Large Language models (LLMs), generative AI, and scaling.

This body of work has focused on \textbf{measurement of fairness}, and then, with respect to those measurements, on \textbf{analysis of monolingual and cross-lingual transfer}, and analysis of \textbf{dense retrievers}. We contribute to enriching the understanding of fairness within a multi-part system, which was previously very poorly understood. This poverty was much more marked at the commencement of this work than today, but does still persist. Fairness and interpretability research have grown, but so has the field as a whole.\footnote{Fairness work has grown exponentially, but so have ACL and NeurIPS, both with about 40-50\% growth in submissions year over year, \url{https://aclweb.org/aclwiki/Conference_acceptance_rates} and \url{https://medium.com/criteo-engineering/neurips-2020-comprehensive-analysis-of-authors-organizations-and-countries-a1b55a08132e})} The scale and speed of productionisation still far outstrips understanding. 

In Part~\ref{part:measurement} I first examined the dominant method of bias evaluation in language models, based on embedding geometry and cosine similarity, found it to have poor predictive validity. I advocated for measuring bias downstream. 
I next found an alternative upstream measure in information theoretic probing of demographics. I caveat that it shows though only \textit{potential} for social bias downstream, a potential that may or may not be realised depending on classifier training. The full system is still needed to see the full picture.

In Part~\ref{part:crosslingual}, I examined monolingual and cross-lingual transfer in the setting of sentiment classification, in five languages, and found that both settings changed fairness outcomes. Monolingual transfer generally improved fairness despite the introduction of new data scraped from the depths of Common Crawl. I attributed this to increased stability in model decisions from the additional data: where stability is defined as not just less errors under the counterfactual, but smaller magnitude ones. Multilingual transfer, however, often worsened fairness outcomes, despite using more data than monolingual transfer. This may not contradict the previous result, as there are less parameters per language, such that even with regard to performance multilingual transfer models are not more stable than monolingual transfer models. The reasons for this difference are left to future work. 

In Part~\ref{part:generation}, I examined dense retrieval models, using the tools and research questions accumulated in Parts~\ref{part:measurement} and \ref{part:crosslingual}. I found that random initialisation and random data shuffle play a much larger role than previously thought. This challenges the standard practice of using only one random seed for research, which was then common and is now ubiquitous in the age of LLMs, where training one model takes over 400,000 kWh \citep{luccioni_bloom_carbon}, or the same amount of energy to make 14 million cups of tea, approximately the same amount that is drunk in Scotland daily.\footnote{It takes about 336000 joules to raise 1 liter (4 cups of tea) of water to a boil, which is equivalent to 0.116 kWh assuming an electric kettle at 80\% efficiency. This makes an LLM equivalent to approximately 13,714,286 cups of tea. The UK drinks on average 3 cups of tea per day, and the population of Scotland is 5.4 million today.} In this work I also found a case where information theoretic probing was \textit{not} predictive of gender bias in retrievers, because the bias was caused by other factors beyond the model representation itself. 

The combined work in this thesis repeatedly shows how little it makes sense to make choices or come to conclusions about fairness without understanding and simulating the entire system. 

Each individual work raised as many questions as it answered, which have not yet been answered by other analytical work. These questions are significant in both size and importance and would be valuable extensions to the understanding of the field, even in the era of LLMs. For both works in Part~\ref{part:measurement} the question remains: what about generative models?  What is the relationship between upstream language model metrics and downstream bias in generation, as opposed to in classification? The objective in generation is more similar to that of pre-training, so the story may be different. Chapter~\ref{chapter:gender_bias_probing} showed that whether bias was realised was a property of both the language model and the classifier, so what if there's no classifier? 
But to answer that question, there has to be a way to measure bias in generation, which the field has by no means settled on, as we discuss in \S\ref{sec:measuring_fairness}. 

In Part~\ref{part:measurement} in classification, I used observational studies to determine allocational bias by measuring whether the error rates across populations that differ with respect to a demographic variable were equal (male group vs. female group, etc). In Part~\ref{part:crosslingual} I used interventional studies that measured whether change in a demographic variable changed predictions, when the predictions should be equal. Both of these measurements require a notion of equality -- which is very easy with a discrete label space or ordinal values. The same type of study could be set up for generative models: instead of classifying resumes, a model could write summaries of resumes with a recommendation to proceed or not, which is then read by a human.\footnote{This is what is happening in practice with generative AI now.} We can make the same assertion, that if we change the gender or race on the resume, the summary should not materially change. But what is a \textbf{material change}? In principle, it is a change that is large enough to cause the summary to be less accurate. Or to be equivalently accurate but to affect a human's opinion positively or negatively. Probably we want to know both for different reasons. How do we measure this? 

The little previous work on this raises just as many questions. \citet{vig_causal} consider social bias in generation to be the relative probability of different gendered pronouns along with stereotypical professions like \textit{doctor} and \textit{nurse}. But how would that be extended to different grammatical systems, like Turkish, which has no gendered pronouns? What about different demographic biases that aren't encoded the way gender is? There is far less research about those \citep{}, but they are no less important from the viewpoint of ethics or of law. 
%Even with those worked out, this measure is so zoomed in that, while it matters, it's hard to see what scope of bias problems it can and cannot represent. 
%BELOW IS REDUNDANT
%\citep{sheng-etal-2019} use \textit{regard} of the demographic that a generation is about, which is generalisable as a notion, but not in practice when operationalising: they train one classifier, which is available in English only, for three binaries (male/female, white/black, and gay/straight) and has never been updated. 
\citet{BOLD}, which is used in most LLM works today (e.g. \citep{llama2,jiang2024mixtral}) create a dataset where they use gender term frequency and regard, in combination with sentiment and toxicity, to try to overcome the issues of each. 
%Plenty of harmful stereotypes carry positive sentiment [example] and lots of social bias doesn't manifest via differences in toxicity [example. 
More essential measurement groundwork has to be done for generation before we can begin to determine whether these metrics are predictable from upstream metrics.

The second work in that section, Chapter~\ref{chapter:gender_bias_probing}, raises the question: what about beyond gender? Most work in this thesis by design looks at bias beyond gender (\ref{chapter:intrinsic_bias_metrics}, \ref{chapter:multilingual_sentiment_analysis}, and \ref{chapter:multilingual_sentiment_analysis_pt2} all include some notion of race or country of origin) but this one,  which proposes a new metric, looks only at gender (partly for lack of suitable datasets beyond it). But gender is encoded very differently in language than other demographic features, so it could reasonably have a different way it operates in model representations and social bias. In English, which weakly marks gender, and other languages with stronger gender agreement, gender is clearly important to represent for correct grammar and language reconstruction from a noising objective \citep{}. But race and country of origin are not as strong signals: it is not easy to determine these save from specific words like names (and even then the signal is not perfect at all, unlike with pronouns). How does this difference in encoded information affect the relationship between language models and downstream bias?

In Chapter~\ref{chapter:multilingual_sentiment_analysis} we found that there was less bias in aggregate in monolingual transfer, and more reasonable patterns of bias (less dramatic changes in sentiment score), but what about tracing individual examples through from pre-training? Could we track a specific negative stereotype in pre-training and see if it can affect decisions later? Extending to the work in Chapter~\ref{chapter:multilingual_sentiment_analysis_pt2}, could we extend tracing individual biases into multiple languages? Almost all bias research is done on aggregate information, and we extended our focus to be on patterns of bias, but we stopped short of doing fine-grained analysis. A more granular analysis would be valuable. We've spent this thesis tracing how fairness persists and travels through a system at a macro level, but we could extend this to a micro level. Such research would not even be bias specific; for there isn't concrete knowledge yet of how \textit{any} information travels between different training stages of models (of which there are increasingly many in the age of LLMs).

In Chapter~\ref{sec:fairness_as_other_fields}, I introduced the notion of fairness as a generalisation error vs. as a learnt dataset artifact (whether an artifact from spurious correlation or a historical bias). A deeper investigation into this could help enlighten why racial bias can increase with cross-lingual transfer. Is it really compounding biases (stereotypes) or could it be a generalisation error? One of these types of bias would have much less certainty than the other. Can an investigation into model uncertainty help illuminate which of these cases is involved in any measurements we observe?

Part~\ref{part:generation} shares the question of biases beyond gender from Chapter \ref{chapter:gender_bias_probing}, as it is also solely gender focused. It also raises many questions that have particular importance to fairness but are also general to our understanding of our systems as a whole. Why is random seed initialisation so important for bias and for generalisation? Why is it possible for a couple of seeds to \textit{just not work at all}, never mind fairness? Some of the anistropy of the representations from earlier training stages seems potentially predictive of later behaviour. Can we understand this well enough to utilise it? If so we could be able to actively encourage model training that is less prone to shortcutting. 

Now I want to bring this into current industry practice and zeitgeist. I do this partly because I've spent the better part of the last year working full-time on fairness at an LLM company. And partly because, in reviewing my PhD work, I don't want to ignore the sea change in NLP research that's taken place over the past year. I am not someone for whom `\textit{scaling is a way of life}'\footnote{This light shade given by Tatsunori Hashimoto when questioned about it at GenBench at EMNLP 2023.}, but it would be disingenuous, in a field intended to improve people's lives, to not speak about how my work relates to current research, current discourse, and current practice. This thesis was initially inspired by a sea change that I saw happening six years ago, after all. 

This work was all done on models three orders of magnitude smaller than the ones that I deal with in my work today. 

This does not matter at all. No conclusions in this thesis were model specific. If some architecture arises to replace neural embeddings, LSTMs, and Transformers which bears no genetic link to it, then they may no longer hold. But until then, the differences between the models I use at work today and the models in this thesis are: 1) scale 2) a veneer of preference tuning (RLHF, DPO, etc) 3) instruction tuning \citep{} and 4) more math, logic, and code in both pretraining and fine-tuning than is usually present in NLP tasks (though some non-trivial amount will of course be present in CommonCrawl, which does drive the models in this thesis). None of these differences affect my conclusions. 

In my first rebuttal to ACL reviewers when Chapter~\ref{chapter:intrinsic_bias_metrics} was under review, one of the reviewers asked the common reviewer question `But have you tried this on {\tt Newest Model Architecture}' (which in this case, was BERT). Adam gave me the advice to turn that into a the question: `Is there any reason to expect that {\tt Newest Model Architecture} would behave differently? Otherwise, they're just saying it is magic'. To answer this question broadly for LLMs: there is no reason to believe and of the four recent innovations change any of the discovered fairness behaviours here.

Here are some examples of this being proven. The replication and extension of my work in Chapter~\ref{chapter:intrinsic_bias_metrics} by \citet{cao-etal-2022-intrinsic} did use BERT, and 18 other transformer architectures of varying sizes, and came to the same conclusions. We've seen the same bias amplification affects in LLMs at scale \citep{article} that we saw in small models in \citet{zhao-etal-2017-men}. Current research shows that RLHF and the family of preference tuning algorithms predominantly affect style and structure of response, rather than content \citep{min-etal-2022-rethinking,  lin2023unlocking}. More research shows it can be trivially changed with a few dozen fine-tuning examples \citep{qi2023finetuning} and that it quickly `wears off' over conversation turns \citep{llama2} and doesn't affect true fairness behaviour. All information is learnt at earlier stages, predominantly pretraining \citep{zhou2023lima}. So from this we conclude that preference tuning will not affect our conclusions. There is no research I have seen that enables inferences on the effect of instruction tuning or the inclusion of more code and math in data, but there's no reason a priori to think they would change fairness behaviour.  

There is one salient change that will matter. Language models are trained to compress and then reconstruct the data they were trained on, and this lossy compression has become less lossy as an effect of scaling, that is: LLMs memorise more individual training samples \citep{karamolegkou-etal-2023-copyright}. This could change fairness outcomes, though will it help or will it harm? This depends somewhat on whether the source of the unfairness is a dataset artifact or a generalisation error (\S \ref{sec:fairness_as_other_fields}). Overall increased memorisation is likely to exacerbate the learning of artifacts. We don't yet understand how scaling affects generalisation, as it is too difficult to test in the current era of closed language models and unknown pretraining and fine-tuning data. Note that regardless of this, scaling won't affect the measurements or mechanisms of bias transfer, which are the subject of this thesis. But these potential interactions of scaling do lend weight to the need for more work on disentangling sources of bias and looking at the effects of increased memorisation from overparameterisation. To date almost all work on memorisation has been from the viewpoint of copyright \citep{}, security \citep{}, and privacy \citep{}, but it would be very relevant to look at it from the viewpoint of fairness.
 
When I started this thesis I focused on validating metrics, not because of a dedication to evaluation; I had grand plans for applying my ideas to cross-lingual bias mitigation. But I'd seen unvalidated assumptions in the standard metrics of the field, and it made me nervous about using those metrics in my own work. I didn't want to stake my PhD research and journey on a metric that I didn't believe in and find out 1.5 years in. But now that I work in a deployed product, I spend at least half my time on evaluation. Because good evaluation was \textit{always very hard} and the rise of generative AI has only made it harder. And I can only throw darts at a wall (a perhaps unfair caricature of LLM training) if I know when they've hit something useful, and \textit{that's} the hard part, not the dart throwing [the knowing not the throwing :p]. 

There is some irony in how Chapter~\ref{chapter:intrinsic_bias_metrics}, my first fairness work, was the seminal work showing that you cannot do upstream social bias mitigation, and then I took a job where I am supposed to do just that. Where in practice, I need to, since education about NLP systems is not good enough that many of the people deploying language models have the knowledge or resources to do bias mitigation themselves. So, I use the tools and the ways of thinking and discoveries that I made over the course of this thesis to evaluate and analyse and create wide bounds for what types of bias could occur in different reasonable settings, and then make this information public, so that deployers know and can work around it. 

But this is still not satisfying enough. I do not think we will ever get to a point in which we rely on one single large pretrained model for thousands of use cases and can predict bias effects downstream for anything but the most common ones. All of this research has progressively taught me that I need to consider the entire NLP system in my measurements for bias: the pretraining, the fine-tuning, the task, the inputs, the corpus that a model can query. The limit case of this it that I need to consider the user interface, the users themselves, the societal power structures within which the NLP system is embedded. And I do think, at some stage, these do need to be part of the experiment conditions and considerations. We cannot consider the harmful effects of QA systems providing false information in absence of how it is displayed in a UI, and how much that UI encourages trust or overreliance \citep{} and bias research cannot consider stereotypes in absence of the power structures that make them harmful \citep{blodgett-etal-2021-stereotyping}. No more can most NLP systems be considered without these things, which all together make it increasingly complex to predict all of these things at an upstream stage. 

But we can get to a point where we understand better the effect of the choices we've made in the lifecycle of an NLP system. Which ones tend to make things worse, which better, and why. With that, we can better predict potential bias in new systems, and then allocate evaluations and mitigation methods accordingly. So that we can take a set of facts about a model, and then generalise from it. %like humans do





% NOTES:
% The role of fairness research is the understanding that industry doesn't have time to do but that can still have hope of being applied in practice of having bearing on a real world situation -- this is a characteristic of the field since it is necessarily grounded in real humans and their lives.


%We are building out the picture of the full ecosystem of what can matter over the course of the thesis




% WHAT else can I bring in about science about goodharts law about things I've learnt about doing this all in practice?


%%% Outline what I want to cover in the discussion
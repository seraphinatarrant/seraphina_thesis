\chapter{Conclusions, Questions, and the Present Day}\label{chapter:conclusion}

\say{\textit{For every subtle and complicated question, there is a perfectly simple and straightforward answer, which is wrong.}}  â€”H.L. Mencken

In this section I will condense the takeaways from my work, then expand them into the new questions they pose to the field. I'll discuss implications for future work, do a bit more reflection than in each individual piece of research, made possible by the aggregation of all the my work. I will also include a less traditionally academic section, and relate my work to the present day mania for Large Language models (LLMs), generative AI, and scaling.

\section{Takeaways}
This body of work has focused on \textbf{measurement of fairness}, and then, with respect to those measurements, on \textbf{analysis of monolingual and cross-lingual transfer}, and finally analysis of \textbf{dense retrievers}. It contributes to enriching our understanding of fairness within a multi-part system, which was previously poorly understood. This poverty was much more marked at the commencement of this work than today, but does still persist. Fairness and interpretability research have grown exponentially, but so has the field as a whole, at an equal pace.\footnote{Fairness work has grown exponentially, but so have ACL and NeurIPS, both with about 40-50\% growth in submissions year over year, \url{https://aclweb.org/aclwiki/Conference_acceptance_rates} and \url{https://medium.com/criteo-engineering/neurips-2020-comprehensive-analysis-of-authors-organizations-and-countries-a1b55a08132e}).} The scale and speed of productionisation still far outstrips our understanding of NLP systems. 

In Part~\ref{part:measurement} I first examined the dominant method of bias evaluation in language models, based on embedding geometry and cosine similarity, and found it to have poor predictive validity. I advocated for measuring bias downstream. 
I next found an alternative upstream measure in information theoretic probing of demographics. This measure shows though only \textit{potential} for social bias downstream, a potential that may or may not be realised depending on classifier training. This both enables better understanding, but also reinforces that the full NLP system is still needed to accurately measure fairness.

In Part~\ref{part:crosslingual}, I examined monolingual and cross-lingual transfer in the setting of sentiment classification, in five languages, and found that both settings changed fairness outcomes. Monolingual transfer generally improved fairness despite that this introduces reams of foul pretraining data scraped from the depths of Common Crawl. I attributed this to increased stability in model decisions from the additional data: where stability is defined as not just fewer errors under the counterfactual, but smaller magnitude ones. 

Multilingual transfer, however, often worsened fairness outcomes, despite using more data than monolingual transfer. This may not contradict the previous result, as there are less parameters per language, such that even with regard to performance multilingual transfer models are not more stable than monolingual transfer models. The reasons for this difference are left to future work. 

In Part~\ref{part:generation}, I examined dense retrieval models, using the tools and research questions accumulated in Parts~\ref{part:measurement} and \ref{part:crosslingual}. I found that random initialisation and random data shuffle play a much larger role than previously thought, and that both performance and fairness were quite sensitive to them. This challenges the standard practice of using only one model with one initialisation and data shuffle for research. Using only one model was then common and is now ubiquitous in the age of LLMs, where training one model takes over 400,000 kWh \citep{luccioni_bloom_carbon}, or the same amount of energy to make 14 million cups of tea, approximately the same amount that is drunk in Scotland daily.\footnote{It takes about 336000 joules to raise 1 liter (4 cups of tea) of water to a boil, which is equivalent to 0.116 kWh assuming an electric kettle at 80\% efficiency. This makes an LLM equivalent to approximately 13,714,286 cups of tea. The UK drinks on average 3 cups of tea per day \citep{tea_stats}, and the population of Scotland is 5.4 million today.} 

In this work I also found a case where information theoretic probing was \textit{not} predictive of gender bias in retrievers, because the bias was caused by other factors beyond the model representation itself. 

\textbf{The combined work in this thesis repeatedly shows how little it makes sense to make choices or come to conclusions about fairness without understanding and simulating the entire system. }

\section{Questions}
Each individual work raised as many new questions as it answered.
%, which have not yet been answered by other analytical work. 
These questions are significant in both size and importance and would be valuable extensions to the understanding of the field, even in the era of LLMs. 

For both works in Part~\ref{part:measurement} the question remains: what about generative models?  What is the relationship between upstream language model metrics and downstream bias in generation, as opposed to in classification? The objective in generation is more similar to that of pre-training, so there is a chance that there is a more predictable correspondence between the two stages. Chapter~\ref{chapter:gender_bias_probing} additionally showed that whether bias was realised was a property of both the language model and the classifier, so what if there's no classifier? 
Future work could use some of the very recent progress in measuring biases in generation and measuring representational biases (discussed in \S\ref{sec:measuring_fairness}) to answer these questions. 
%But to answer that question, there has to be a way to measure bias in generation, which the field still needs more work on, 

In this section (Part~\ref{part:measurement}) for classification, I used \textbf{observational} studies to determine \textbf{allocational bias} by measuring whether the error rates were equal across different demographic populations (male group vs. female group, etc). In Part~\ref{part:crosslingual} I used \textbf{interventional studies} that measured whether a change in demographic variable changed predictions, when the predictions should be equal. Both of these measurements require a notion of \textbf{equality} --- which is very easy with a discrete label space or ordinal values. The same type of study could and should be done for generative models: instead of classifying resumes, a model could write summaries of resumes with a recommendation to proceed or not, which is then read by a human.\footnote{This is what is happening in practice with generative AI now, as I will discuss in the next section about my experience in Industry.} We can make the same invariance assertion as we made in classification: if we change the gender or race on the resume, the summary should not materially change. But what is a \textbf{material change}? In principle, it is a change that is large enough to cause the summary to be less accurate. Or to be equivalently accurate but to affect a human's opinion positively or negatively. Both would be good operationalisations for different contexts. How do we measure this? 

There is scarce previous work on counterfactuals in generative systems, but it raises just as many questions. \citet{vig_causal} consider social bias in generation to be the relative probability of professions like \textit{doctor} and \textit{nurse} under a counterfactual where male and female pronouns are swapped. How would that be extended to different grammatical systems, like Turkish, which has no gendered pronouns? What about different demographic biases that aren't encoded the way gender is? There is far less research about other demographic biases in this kind of setting. The comparatively thin coverage has been noted for race \citep{field-etal-2021-survey}, and there is vanishingly little on other demographic biases (with some exceptions of different coverage, such as \citet{hutchinson-etal-2020-social}, and of very broad coverage, such as \citet{esiobu-etal-2023-robbie}). But though they are less well represented in NLP, they are no less important from the viewpoint of ethics or of law. 
%Even with those worked out, this measure is so zoomed in that, while it matters, it's hard to see what scope of bias problems it can and cannot represent. 
%BELOW IS REDUNDANT
%\citep{sheng-etal-2019} use \textit{regard} of the demographic that a generation is about, which is generalisable as a notion, but not in practice when operationalising: they train one classifier, which is available in English only, for three binaries (male/female, white/black, and gay/straight) and has never been updated. 

% \citet{BOLD}, which is used as the standard of bias measurement in the LLM technical reports of today (e.g. Llama and Mixtral \citep{llama2,jiang2024mixtral}) create a dataset where they use gender term frequency and regard, in combination with sentiment and toxicity, to try to overcome the issues of each. 
% %Plenty of harmful stereotypes carry positive sentiment [example] and lots of social bias doesn't manifest via differences in toxicity [example. 
% More essential measurement groundwork has to be done for generation before we can begin to determine whether these metrics are predictable from upstream metrics.

The second work in Part~\ref{part:measurement}, Chapter~\ref{chapter:gender_bias_probing}, raises the question: what about beyond gender? Most work in this thesis by design looks at bias beyond gender (\ref{chapter:intrinsic_bias_metrics}, \ref{chapter:multilingual_sentiment_analysis}, and \ref{chapter:multilingual_sentiment_analysis_pt2} all include some notion of race or country of origin) but this one,  which proposes a new metric, looks only at gender (partly for lack of suitable datasets beyond it). But gender is encoded very differently in language than other demographic features, so it could reasonably have a different way it operates in model representations and social bias. In English, which weakly marks gender, and other languages with stronger gender agreement, gender information is necessary for correct grammar. A model will need to represent gender well for correct language reconstruction of any text from a noising objective, which is how Transformer models are currently trained  \citep{liu2019roberta, lewis-etal-2020-bart, vaswani}. But race and country of origin are not as strong signals. It is not easy to determine these save from specific words like names,and even then the signal is not as clear as with pronouns, as names do not \textit{just} encode race but also class, gender, time period, etc. How does this difference in encoded information affect the relationship between language models and downstream bias?

In Part~\ref{part:crosslingual}, Chapter~\ref{chapter:multilingual_sentiment_analysis}, we found that there was less bias in aggregate in monolingual transfer, and more reasonable patterns of bias, evidenced by less dramatic changes in sentiment score under the counterfactual. But what about tracing individual examples through from pre-training? Could we track a specific negative stereotype in pre-training and see if it affects decisions later? Extending to the work in Chapter~\ref{chapter:multilingual_sentiment_analysis_pt2}, could we extend tracing individual biases into multiple languages?\footnote{One work has recently come out that also shows `stereotype leakage' across languages \citep{cao2023multilingual}, which also helps form a foundation for this new question.} Almost all bias research is done on aggregate information, and we extended our focus to be on patterns of bias, but we stopped short of doing fine-grained analysis, which would be valuable. 

We've spent this thesis tracing how fairness persists and travels through a system at a macro level, but we could extend this to a micro level. Such research would not even be bias specific; for there isn't concrete knowledge yet of how \textit{any} information travels between different training stages of models (of which there are increasingly many in the age of LLMs).\footnote{There are some methods like this that are starting to do this, like influence functions \citep{grosse2023studying} and some methods that try to do this with Natural Language explanations, which is nonsense and does not work, as \citet{huang-etal-2023-rigorously} found `no evidence for causal efficacy' of them.}

In Background Section~\ref{sec:fairness_as_other_fields}, I introduced the notion of fairness as a generalisation error vs. as a learnt dataset artifact---whether an artifact from spurious correlation or a historical bias. Investigation into this difference in causes could help enlighten why racial bias can increase with cross-lingual transfer. Is it really compounding biases (stereotypes) or could it be a generalisation error? %One of these types of bias could have much less certainty than the other. 
Can an investigation into model uncertainty help illuminate which of these cases causes the effects we have observed?

Part~\ref{part:generation} shares the question of `biases beyond gender' from Chapter \ref{chapter:gender_bias_probing}, as it is also solely gender focused. It also raises questions are general to our understanding of NLP systems as a whole, but have particular importance to fairness. Why is random seed initialisation so important for bias and for generalisation? Why is it possible for a couple of seeds to \textit{just not work at all}, never mind fairness? Some of the anistropy of the representations from earlier training stages seems potentially predictive of later behaviour. Can we understand this well enough to utilise it? If so we could potentially be able to actively encourage model training that is less prone to shortcutting. 

\section{Present Day}
Now I want to bring this into current industry practice and zeitgeist. I do this partly because I've spent the better part of the last year working full-time on fairness at an LLM company (Cohere). And partly because, in reviewing my PhD work, I don't want to ignore the sea change in NLP research that's taken place over the past year and a hald. I am not someone for whom `\textit{scaling is a way of life}'\footnote{This light shade given by Tatsunori Hashimoto when questioned about it at GenBench at EMNLP 2023.}, but it would be disingenuous, in a field intended to improve people's lives, to not speak about how my work relates to current research, current discourse, and current industry practice. This thesis was initially inspired by a sea change that I saw happening six years ago, after all. 

This work was all done on models three orders of magnitude smaller than the ones that I deal with in my work today. 

This does not matter as much as it might seem. No conclusions in this thesis were model specific. If some architecture arises to replace neural embeddings, LSTMs, and Transformers which bears no genetic link to them, then they \textit{may} no longer hold. But until then, the differences between the models I use at work today and the models in this thesis are: 1) scale 2) a veneer of preference tuning (RLHF, DPO, etc) \citep{rafailov2024direct} 3) instruction tuning \citep{ouyang2022training} and 4) more training data that is explicitly in the domain of math, logic, and code than we used to include in general NLP models.\footnote{Though some amount of math and code will be present in CommonCrawl, which does drive the models in this thesis.} None of these differences affect my conclusions. 

In my first rebuttal to ACL reviewers when the work in Chapter~\ref{chapter:intrinsic_bias_metrics} was under review, one of the reviewers asked the common reviewer question `But have you tried this on {\tt Newest Model Architecture}' (which in this case, was BERT). Adam gave me the advice to turn that into a the question: `Is there any reason to expect that {\tt Newest Model Architecture} would behave differently? Otherwise, they're just saying it is magic'. To answer this question broadly for LLMs: there is no reason to believe any of the four recent innovations change the things we discovered about fairness in this thesis.

Here are some examples of this being proven. 

The replication and extension of my work in Chapter~\ref{chapter:intrinsic_bias_metrics} by \citet{cao-etal-2022-intrinsic} did use BERT, and 18 other transformer architectures of varying sizes, and came to the same conclusions. We've seen the same bias amplification affects in LLMs at scale \citep{bianchi-2023} that we saw in small models in \citet{zhao-etal-2017-men}. 

Current research shows that RLHF and the family of preference tuning algorithms predominantly affect \textit{style and structure} of generation, rather than content \citep{min-etal-2022-rethinking,  lin2023unlocking}. More research shows it can be trivially changed with a few dozen fine-tuning examples \citep{qi2023finetuning} and that it quickly `wears off' over conversation turns \citep{llama2}. All information is learnt at earlier stages, predominantly pretraining \citep{zhou2023lima}. So from this we conclude that preference tuning will not affect our conclusions. 

There is no research I have seen that enables inferences on the effect of instruction tuning or the inclusion of more code and math in data, but there's no reason a priori to think they would change fairness behaviour.  

There is one salient change that will matter. Language models are trained to compress and then reconstruct the data they were trained on, and this lossy compression has become less lossy as an effect of scaling. That is: LLMs memorise more individual training samples \citep{karamolegkou-etal-2023-copyright}. This could change fairness outcomes, though will it help or will it harm? This depends somewhat on whether the source of the unfairness is a dataset artifact or a generalisation error (\S \ref{sec:fairness_as_other_fields}). On the one hand, overall increased memorisation is likely to exacerbate the learning of artifacts. On the other hand, we don't yet understand how scaling affects generalisation, as it is too difficult to test in the current era of closed language models and unknown pretraining and fine-tuning data. 

Regardless of this, scaling won't affect the measurements or mechanisms of bias transfer. But these potential interactions of scaling do lend weight to the need for more work on disentangling sources of bias and looking at the effects of increased memorisation from overparameterisation. To date almost all work on memorisation has been from the viewpoint of copyright \citep{karamolegkou-etal-2023-copyright}, security and privacy \citep{smith2023identifying, hartmann2023sok}, or rarely, model quality (where memorisation is at odds with generalisation) \citep{tanzer-etal-2022-memorisation}. The NLP community should also look at it from the viewpoint of fairness.
 
When I started this thesis I focused on validating metrics, not because of a dedication to evaluation; I had grand plans for applying my ideas to cross-lingual bias mitigation. But I'd seen unvalidated assumptions in the standard metrics of the field, and it made me unwilling to use those metrics in my own work. I didn't want to stake my PhD research on a metric that I didn't trust, and find out 1.5 years in that my intuition not to trust it had been correct. But now that I work on a deployed product, I spend at least half my time on evaluation. Because good evaluation was \textit{always very hard} and the rise of generative AI has only made it harder. And I can only throw darts at a wall (a perhaps unfair caricature of LLM training) if I know when they've hit something useful, and \textit{that's} the hard part, not the dart throwing.

There is some irony in how Chapter~\ref{chapter:intrinsic_bias_metrics}, my first fairness work, was the seminal work showing that you cannot do upstream social bias mitigation, and then I took a job where I am supposed to do just that. In practice, I need to try, since education about NLP systems is not yet good enough, and the deployers of language models do not yet have the knowledge and resources to do bias mitigation themselves. So I use the tools and discoveries that I made over the course of this thesis to evaluate my models, and measure wide bounds for what types of bias \textit{could} occur in different reasonable settings, and then make this information public, so that deployers know, can work around it, and maybe do something about it.

But this is still not satisfying enough. I do not think we will ever get to a point in which we rely on one single large pretrained model for thousands of use cases and can predict bias effects downstream for anything but the most common ones. All of this research has progressively taught me that I need to consider the entire NLP system in my measurements for bias: the pretraining, the fine-tuning, the task, the inputs, the corpus that a model can query. The limit case of this it that I need to consider the user interface, the users themselves, the societal power structures within which the NLP system is embedded. And I do think, at some stage, these need to be part of NLP experimental conditions. We cannot consider the harmful effects of QA systems providing false information in absence of how it is displayed in a UI, and how much that UI encourages trust or overreliance \citep{bucinca_2021}. Bias research cannot consider stereotypes in absence of the power structures that make them harmful \citep{blodgett-etal-2021-stereotyping}. No more can most NLP systems be considered without these things, which all together make it increasingly complex to predict all of these things at an upstream stage. 

But we can get to a point where we understand better the effect of the choices we've made in the life-cycle of an NLP system. Which ones tend to make things worse, which better, and why. With that, we can better predict potential bias in new systems, and then allocate evaluations and mitigation methods accordingly. But first, we need to understand our systems as a whole.
%So that we can take a set of facts about a model, and then generalise from it. %like humans do





% NOTES:
% The role of fairness research is the understanding that industry doesn't have time to do but that can still have hope of being applied in practice of having bearing on a real world situation -- this is a characteristic of the field since it is necessarily grounded in real humans and their lives.


%We are building out the picture of the full ecosystem of what can matter over the course of the thesis




% WHAT else can I bring in about science about goodharts law about things I've learnt about doing this all in practice?


%%% Outline what I want to cover in the discussion
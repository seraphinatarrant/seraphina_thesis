\chapter{Background}\label{chapter:background}

The following sections give requisite background information common across either all works or multiple works in this thesis. Background that is relevant to only an individual work (probing methodologies, retrieval augmented generation, etc) will instead appear directly before that work.

\section{Defining Fairness}
Fairness is a relatively recent subject of study within the field of Machine Learning/AI research. As such it suffers from lack of standardisation in both definitions and methods of measurement. This is much to the detriment of this growing field. Many works fail to concretely define fairness \citep{blodgett-etal-2020-language, goldfarb-tarrant-etal-2023-prompt}. Often when doing a meta-analysis or review of fairness literature, it is unclear if conflicting results are the result of a methodological problem, an error in code or analysis, or just a disagreement in definitions and the set of works actually should not be compared.To avoid these pitfalls, all work in this thesis will concretely define what fairness means in the context of that particular research. As background to all of them, I will give a brief overview of the discipline of fairness in NLP, how it has grown, and what `fairness' tends to mean in different contexts. 
%its detriment, as any work must first define what it means by the term, and as idiomatic definitions and measurements across different works make meta-analysis difficult, and can hide contradictory results across different experiments. 
Fairness in AI began to gain attention in 2016, following the publication of a few high profile works within Machine Learning. The first was the popular book \textit{Weapons of Math Destruction} \citep{oneil2016weapons}, an expos√© of all the ways that Machine Learning systems are invisibly incorporated into parts of our society, and how the assumptions baked into them propagate injustice. The second is the NeurIPS research paper from the same year, \textit{Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings} \citep{bolukbasi}. This paper showed, via evocative word analogies from neural word embeddings (which was the standard measure of embedding performance at the time \citep{mikolov-etal-2013-linguistic, wordsim, drozd-etal-2016-word}), how career-based gender bias was learnt by these systems, even when trained on relatively innocuous (for the internet) data like Google News. The field of Machine Learning was galvanised by these works, and more work began to be done on fairness analysis and mitigation within the following few years.\footnote{It is worth noting at this stage that other fields had been aware of fairness issues in automated systems for some time, as education and hiring had been looking at statistical fairness for the previous half a century \citep{hutchinson_mitchell_2019}. ML works built off of this, though perhaps not as much as they should've, and we did do some reinventing of the wheel.} But definitions and methods are still being solidified.
%Since then, major NLP and AI conferences have added entire Ethics tracks, formed Ethics commmittees to review papers flagged for potential ethical concerns, encouraged ethics statements to be included in each work, and multiple fairness focused workshops have sprung up at each conference. But definitions and methods are still being solidified.


%Broadly, fairness can be categorised as one of two types, as mentioned in the Introduction. \textbf{Allocational fairness}, which requires that systems perform equivalently for different individuals, regardless of demographics, and \textbf{representational fairness}, which requires that the systems represents different demographics with equal dignity. How exactly "equivalently" is measured is what determines and differentiates the available fairness metrics. For intuition, an example of allocational unfairness is how Automated Speech Recognition (ASR), which is now absolutely everywhere, has different error rates for different dialects of English, with increasing error the farther that speaker is from a white male in his 20s from California \citep{tatman17_interspeech}. An example of representational unfairness is how generative language models disproportionately generate text about black men as criminals and gay men as druggies, as compared to white men \citep{sheng-etal-2019-woman}. It is possible to have some blurry boundaries between these two, as in \citet{zhao-etal-2017-men}, which found that image captioning models were inaccurate for counter-stereotypical gender activites, like men shopping or cleaning, and women riding motorcycles or programming/gaming. This work frames the fairness issues as allocative (research tends to pick one of the two areas), but it could also be considered representational, as any application, e.g. an image search, that relies on a model with these errors will produce only stereotypical images.

\section{Measuring Fairness}
\label{sec:measuring_fairness}

\textbf{Notation:} In all fairness metric definitions contained in this work, let $a \in A$ be the demographic variable in question, where $A = \{privileged, minoritised\}$ group, such as $\{male, female\}$ or $\{native, immigrant\}$\footnote{An obvious limitation of this is that privileged and minoritised is binary. This tends to be true of fairness work, including work in this thesis. There is insufficient work on extending fairness metrics and constraints to multiclass, either theoretically or empirically.} In classification tasks (all tasks until Part~\ref{part:generation}) let $Y$ be the true label, $\hat{Y}$ be the predicted label, and $R$ be the classifier score (which enables analysis independent of classifier threshold). 

\textbf{Representational fairness} has no codified metrics of measurement in NLP. This one of the clearest areas where NLP could learn from sociology and psychology, for they have been measuring representational fairness in media for quite some time \citep{dixon2017dangerous, black_rep_bias}, but we've yet to operationalise this in NLP research. NLP largely neglected to measure representational fairness until \citet{sheng-etal-2019-woman}, which proposed using a classifier to detect \textit{regard} for the subject of a passage in open-domain generation. \textit{Regard} captures how a reader of a text would esteem the subject. Using regard, they found GPT-2 \citep{radford2019language} to systematically generate content causing lower regard when generating about women, African-Americans, and gays. This work is conceptually satisfying, and important, but difficult to expand due to the reliance on the classifier, which 1) is limited to English and 2) can become out of date over time as language drifts (and at the time of writing already has). So there have been not many follow up replications of this work but it doesn't have broad adoption \citep{goldfarb-tarrant-etal-2023-prompt}.

Approaches that differ from \citet{sheng-etal-2019-woman} tend to use sentiment score of the text \citep{goldfarb-tarrant-etal-2023-prompt}, and the current cutting edge Large Language Model (LLM) work still does \cite{llama2, jiang2024mixtral}.\footnote{This is itself a bit interesting, because the benchmark that current LLMs use, \citet{Dhamala_2021}, notes the limitations of most automated metrics, and uses a combination of sentiment, regard, and toxicity, as well as other two other metrics that they define. But this nuance seems to have been lost.} This is unfortunate, since the relationship of sentiment to representational harm is not well-correlated; many stereotypes that are harmful in a societal or an HR context can have positive sentiment (e.g. women are nurturing) \citep{fraser-etal-2021-understanding}. The field is overdue for an analysis of the impact of this difference.


Other work on representational fairness that avoids using sentiment or regard classification focuses on discovery of language model stereotypes via challenge sets or customised prompts, and the likelihood of different generations \citep{smith-etal-2022-im}. Challenge sets like this makes up the majority of bias work done on generative models today \citep{goldfarb-tarrant-etal-2023-prompt}, as generative fairness tends to focus on representational harms.
%as measuring allocational fairness is not straightforward in generation. 
However, the most prevalent approaches to stereotype measurement for the past few years, from two benchmark datasets, have been shown to be so flawed in construction as to be essentially meaningless \citep{blodgett-etal-2021-stereotyping}. More recently, better and more reliable datasets and examinations have come out for representational fairness \citep{esiobu-etal-2023-robbie,  hosseini-etal-2023-empirical, smith-etal-2022-im, Dhamala_2021}. But for the timeline of the work in this thesis, as a result of lack of consensus and good resources, this thesis focuses on only \textbf{allocational fairness}.


The most comprehensive overviews of strategies for measurement of allocational fairness are \citet{hutchinson_mitchell_2019} and \citet{barocas-hardt-narayanan}. I will explain a subset of these that are important in this thesis. At a high level, allocational fairness can be measured as \textbf{individual fairness}, which answers the question `are the results for similar individuals equivalent' and \textbf{group fairness}, or `is the performance for demographic subgroups equivalent'. In the former, the work lies in defining the similarity function. \textit{What is similar? Are two individuals with the same university degree similar? Or only if you bucket by university prestige?} Individual similarity requires that you decide what does matter for similarity and what does not. In the latter, the work lies in selecting the demographic slices (what \textit{are} the subgroups that should be equal?), and in choosing the performance measure. Choosing the demographic slices does not get much attention in NLP literature. There are a few nods to intersectionality \citep{subramanian-etal-2021-evaluating,ma-etal-2023-intersectional,lalor-etal-2022-benchmarking, pmlr-v80-kearns18a} (i.e. `your groups may be more complicated') and to unsupervised demographic group discovery \citep{zhao-chang-2020-logan}: otherwise works assume that demographic groups are given, gold standard, and that discrimination against different demographic axes is independent--i.e. discrimination against women can be treated entirely separately from against African Americans. This is patently false, gender and racial biases are interdependent and cause new, distinct bias effects when they intersect \citep{borenstein-etal-2023-measuring}. There is much more attention given to how to measure performance disparity.
%than to how to select the groups
Most NLP work uses group fairness, and measures this performance disparity.  In classification, sometimes difference in F1 is used \citep{zhao-etal-2018-gender} but many works use more granular measures such as \textbf{equalised odds} \citep{hardt2016equality} which enforces equal false-positive rates (FPR) and true-positive rates (TPR) across groups.
\begin{align}\label{eq:fpr}
    P(\hat{Y} = 1, A=a, Y=0) (FPR)
\end{align}
\begin{align}\label{eq:tpr}
    P(\hat{Y} = 1, A=a, Y=1) (TPR)
\end{align}
where \ref{eq:fpr} and \ref{eq:tpr} should be equal $\forall a \in A$.

Note that the second constraint \ref{eq:tpr} is equivalent to recall, as recall can be expressed the same way:
\begin{align}\label{eq:recall}
    \frac{\hat{Y} = 1 | Y=1}{(\hat{Y} = 1 | Y=1) + \hat{Y} = 0 | Y=1}
\end{align}.
The second constraint (recall) is often used in isolation as \textbf{equality of opportunity} \citep{hardt2016equality}, a relaxation of \textbf{equalised odds}. 

Occasionally some works include related but different group fairness metrics, discussed in \citet{barocas-hardt-narayanan}, such as \textbf{independence}, \textbf{separation}, and (rarely) \textbf{sufficiency}. 

%\sgtcomment{Here I will insert my explanations of these from my other doc. TODO!}

In Chapter~\ref{chapter:intrinsic_bias_metrics} we look at differences in recall and in precision, the former is equivalent to \ref{eq:recall} above. In \ref{chapter:gender_bias_probing}, we use a broad number of metrics: difference in True Positives, difference in False Positives, difference in Precision, difference in F1, Independence, Separation, Sufficiency. The first author of that work goes on in \citet{orgad-belinkov-2022-choose} to show how there is often poor coverage of metrics that could be used for a given dataset or task, based partly on what we learnt in this work. 
%in order to establish the relationship between language model representations and application fairness metrics (also called \textbf{downstream} or \textbf{extrinsic} metrics). 

It is valuable for fairness analysis to report a broad set of metrics because fairness metrics in practice should be chosen based on the tasks in question. 
%Different notions of fairness are in tension with each other, and are provably mutually unsatisfiable \citet{barocas-hardt-narayanan}. So fairness metrics need to be chosen based on the application and what makes most sense. 
Choices of fairness metrics involve a normative judgment, whether implicit or explicit, though most research fails to acknowledge this. This is also too often left implicit, or made based on what some prior similar work has used, even if a different metric is both able to be used and would be more suited \citep{orgad-belinkov-2022-choose}. In other words, the logic of why one might choose one or another metric is hidden. 

In Part~\ref{part:crosslingual} we shift to using \textbf{invariance under a counterfactual} in a downstream task. \textbf{Invariance under a counterfactual} describes the assertion that model predictions should be invariant to a perturbation: the example sentence \textit{My sister had a wonderful day today} should be classified as positive sentiment, and this should not change if it is perturbed to be textit{My \underline{brother} had a wonderful day today}.

This approach is very different in the data it requires and the hypotheses it can and cannot prove, from the metrics discussed thus far. I describe the distinction between these two types of measurement as \textbf{interventional} for these, and \textbf{observational} for the previous metrics. I borrow this terminology from the medical field because it gives correct intuitions. Observational studies can tell you that phenomena is occurring, but not \textit{why}. Your model could be worse at recall of toxic content targeted at women, but it could be because female targeted toxic content is more diverse in your dataset, and thus more difficult to detect. Or maybe this isn't the case, but your classifier is poorly calibrated for this group compared to others. Interventional studies, by contrast, make a change (a perturbation) and observe the difference. Invariance under a counterfactual sets up a tasks where a perturbation \textit{should not} change a prediction, and then measures change as a failure. This approach was popularised in ML as a test for robustness to noise in vision tasks \citep{Zheng2016ImprovingTR}; an image correctly classified as a leopard should not change to being labelled as a butterfly when just a few pixels change, or when some noise is added that is imperceptible to a human observer. In NLP this is less easy to do, because it is harder to assert that labels should not change when working with the discrete space of language. But it works well when done carefully for fairness, where we can assert that changing the race, gender, or other demographic information of a name on a resume (from Emily Johnson to Lakisha Brown, as was done in the real life study of \citet{bertrand2004emily}) should not change an output label in a resume processing system. So invariance tests require these carefully paired data points, so they cannot generally be done on the same data as observational studies.\footnote{Note that Winobias \citep{zhao-etal-2018-gender}, which we use in both works in Part~\ref{part:measurement}, could have been framed similarly as a counterfactual (though one where the label \textit{should} flip, so not an invariance test. It was not framed this way, and instead was framed as subgroup fairness where the groups were `pro' and `anti' stereotypical. But it could have been.} The benefit of this type of study is that you don't have to be careful in slicing data--when you get a result, you know why. The downside is that the noising has to be done with care to only perturb where invariance \textit{should be true}. For this reason invariance under a counterfactual data is often synthetic, and may not be representative of the true distribution of data. This is the weakness of this method.
%These metrics are all ones that are applied on some downstream task. In Part~\ref{part:measurement} we analyse the relationship of these allocational fairness metrics to a number of novel measurements proposed in the NLP literature that can be applied to just representations. We will not survey them here, as what we use is specific to each work.

In the first work on measurement (\ref{chapter:intrinsic_bias_metrics}), we focus on gaps in precision and recall, as previous work upon which we built our analysis used F1 \citep{zhao-etal-2018-gender}, and factoring them out gives both more granular analysis and also comparability to the equality of opportunity measure \citep{hardt2016equality}. In the second, we use the full suite of possible metrics. In Part~\ref{part:crosslingual} we use don't use a subgroup metric, but instead use counterfactual examples that perturb one demographic variable, where we make an invariance assumption that values should not change under this perturbation, and the magnitude of the change is our metric. This method does not fit cleanly into individual or subgroup fairness, as it can be analysed on an individual example (which we do) but those examples have also been constructed to stand in for a demographic. E.g. in the counterfactual example: \textit{I made her feel relieved} vs. \textit{I made him feel relieved}, \textit{her} and \textit{him} are individual instances of bias, but also are stand-ins for the concept of gender. 
In Part~\ref{part:generation} we measure retrieval rather than classification, so we use performance gap in the most common retrieval metric.  We then construct a separate experiment for causality even though we have to use an observational metric, which brings the measurement approaches in Parts~\ref{part:measurement} (observational) and \ref{part:crosslingual} (interventional) together.

To end the measurement section, the two works on measurement were motivation by the following observation. It seems potentially obvious to state, but the main desirable characteristic of a measurement of fairness is that it \textbf{a}) accurately measures the concept that it purports to measure and \textbf{b}) has a reliable relationship to real world fairness. When \textbf{a} and \textbf{b} are both true, the measurement has \textbf{construct validity} -- a multi-faceted concept in the field of measurement modelling from the social sciences \citep{jacobsandwallach}, that attempts to define and make explicit the gaps between conceptualisation (e.g. my model should not discriminate based on race) and operationalisation (e.g. the performance gap between different racial groups, as identified by dialect identification).\footnote{Formally, \textbf{a} corresponds to \textbf{content validity} and \textbf{b} to \textbf{predictive validity}, as sub-concepts of \textbf{construct validity} }. Much of the work in Part~\ref{part:measurement} was motivated by my observation that these types of validity had not been examined and were assumed to be true. We thus set out to test them.

\section{Common Approaches to Debiasing}
Fairness literature, as well as measuring bias, will often propose methods of \textbf{debiasing}. Debiasing methods proliferate, but most new methods do not get widespread adoption, since they fail to build trust. Debiasing methods tend to be proven in only quite constrained settings, on only one or two models, only in English, and on a limited number of tasks.\footnote{I would like to here allocate appropriate blame to publication venues for requiring `novelty' such that new works tend to propose new methods rather than verifying existing methods, leading to the situation at the commencement of this thesis where we had zillions of methods that no one used}. This thesis therefore focuses on analysis, and does not propose any new methods. However, we will briefly survey existing methods that are used in analysis in Part~\ref{part:measurement} and \ref{part:generation}.

Debiasing approaches fall into high level categories of where they occur in the lifecycle of training an NLP model: pre-, mid (during), and post. \textbf{Preprocessing}\footnote{I use the term preprocessing rather than pre-training to distinguish from the now common terminology of pre-training/finetuning} approaches involve a processing step that modifies data before training a model, to reduce signal that can cause bias. For example, if a system used for resume filtering ought to be debiased with regard to binary gender, the data can be processed such that there is an equal co-occurrence of gender signifiers (pronouns, names, other words that encode gender information) alongside words that indicate profession or career information.\footnote{This is never easy to do fully, but can be quite successful in English with relatively coarse processing. It is not so easy in languages with much more gender marking, and this area is heavily under-researched. \citet{gonen-etal-2019-grammatical} looks into using morphological analysers for this.}
Preprocessing can be done on unstructured webtext that will be used to learn embeddings or train language models or on labelled data that is used for supervised finetuning. These are usually known as \textbf{dataset balancing}, and differences in dataset balancing approaches stem from both the chosen method of editing data and the axes along which the data is balanced (gender/profession, race/toxicity, religion/sentiment, etc). The method falls into broadly two approaches \citep{schwartz-stanovsky-2022-limitations}. If there is enough data that some can be removed without much performance penalty (more commonly true of unstructured text), it can be subsampled such that there is less but more balanced data \citep{wang2019balanced}. Other approaches oversample data such that some data is repeated \citep{chawla2002smote} in order to overweight those examples because that they occur more frequently in training data. This suffers from lack of diversity in the minority class, so other works opt to do a third option and instead create synthetic data for the minority class to remedy this \citep{Dixon2018MeasuringAM, zhao-etal-2018-gender}. While the preprocessing approach is most reliable, and best understood, it is only available to practitioners who actually train models, which was always a small class; increasingly smaller as models scale. There is also very little work that tries to balance multiple axes at once which highlights the biggest limitation of dataset balancing. To achieve a minimally biased representation, you have to simultaneously balance across all genders, races, ages, etc, which is increasingly infeasible in any non-synthetic approach. 
Simultaneously balancing multiple axes is a perfectly reasonable desire in real life applications, and for many cases is a required feature. Very few regulations (or systems of integrity) have the goal of systems that are gender-fair but racist and ageist. This is clearly important future work.

Debiasing can be done in \textbf{postprocessing} as well, generally on representations, though there is some preliminary work investigating utilising decoding parameters \citep{sheng-etal-2021-societal}. Both approaches are more complex than preprocessing, both conceptually and in implementation,  but do not require retraining a large and expensive model. Crucially, this enables debiasing to be done by parties further downstream who are then most connected to a downstream application.\footnote{We show this connection to downstream is necessary in Chapter~\ref{chapter:intrinsic_bias_metrics}.} It further allows more iteration and experimentation without extensive compute. \citet{ravfogel-etal-2020-null} operate on representations via nullspace projection -- they learn a linear classifier for a demographic (binary gender, race) and then project language model representations onto the nullspace of that classifier. \citet{iskander-etal-2023-shielded} extend this method to remove non-linearly encoded information. We use these methods for causal analysis of the impact of demographics (rather than debiasing) in Part~\ref{part:generation}. The other methods of debiasing representations operate on individual words and groups of words that stand in for concepts: \citet{mrksic-etal-2017-semantic} pushes word embeddings together or away from each other in representation space; we use this method in Chapter~\ref{chapter:intrinsic_bias_metrics}. More recently a number of works use  model-editing \citep{meng2022locating}, where individual neuron values can be changed in order to change one specific output string. This has some issues with scaling to a full demographic (edits are granular) but would be a promising new direction for very targeted interventions. It is a promising new direction, but post-dates the work in this thesis, and thus is not used. 

Debiasing during training, via constraints or costs to the learning method \citep{zhao-etal-2017-men}, is less commonly done, perhaps partly perhaps because it contains the disadvantages of both preprocessing and postprocessing -- it is conceptually more complex and requires tuning hyperparameters (as does postprocessing) but it also requires retraining a model. It also is made more complex by Transfer Learning, as it is unclear whether to do it at one or both stages. The formalisation is satisfying, because explicit constraints give quantifiable fairness outcomes, but with increasing scale it is increasingly impractical. We do no analysis on this kind of debiasing for this reason.

Recent hype around generative language model fairness focuses on a second stage training process, often called `alignment', which refers to the idea that human morality (it is never specified which human or which morality) can be instilled in a model via fine-tuning with a ranking loss over examples that are more or less moral. The majority of this thesis predates the alignment trend, and very little deals explicitly with generation, so we do not use any of these techniques. We note also that much of the alignment work has origins in robotics more than in fairness. We do, however, discuss current implications of this work in the Conclusion (\ref{chapter:conclusion}). 
%(as it is usually (though not always) applied via reinforcement learning). 
However, our work does have interesting similarities to point out at this stage. In Chapter~\ref{chapter:gender_bias_probing} we measure fairness via differences in distributions for different demographics, via KL or Wasserstein distance, one of the standard ways to measure it (\S \ref{sec:measuring_fairness}). This measurement is even sometimes directly optimised for (in a small violation of Goodhart's law) in works like \citet{huang-etal-2020-reducing}, who regularise output to have similar sentiment distributions between groups. \citet{korbak2022on} show that Reinforcement Learning from Human Feedback (RLHF), the most common method of alignment, can be equivalent to distribution matching and \citet{rafailov2023direct} transition the implementation of this to a new objective that does this explicitly, and many use it for its superior stability and ease of implementation. So when given an appropriate setup, RLHF could (theoretically) be directly optimising for a fairness constraint. Again, this postdates this thesis, and we do no alignment at all, but found it worthwhile to highlight the theoretical continuity in the approaches. 

\section{Fairness as Dataset Artifacts or as Failure to Generalise}
\label{sec:fairness_as_other_fields}
Fairness issues can often be seen as a special case of one of these two areas, though this is rarely discussed in most fairness work. 
%The allocational fairness measurements we use (\S \ref{sec:measuring_fairness}) can have a number of different causes. 
As an example, take racial bias from AAVE, in the two forms that we have already discussed in the background and introduction. In one of the cases, racial bias in toxicity detection has come from the model learning a dataset artifact, where labelled training data correlated African American dialect (AAVE) features with toxic content, as a result of an error or a bias in annotation \citep{sap-etal-2019-risk}
%, as is often also tested on \citet{blodgett-etal-2016-demographic}. 
But allocational racial bias can also come from insufficient training data in AAVE , resulting in higher error rates from that group, as \citet{tatman17_interspeech} measure for automatic captioning. Most work does not address this difference or disentangle these two causes, and lump both under "bias". This lack of distinction is one of the reasons fairness work can fail to have predictive validity. Correcting an anti-AAVE stereotype may not help allocational bias in a model if the root cause was simply that it modelled AAVE poorly. Even dataset artifacts can be further disentangled and split into two types of causes conceptually: dataset artifacts (or dataset biases) that replicate historical biases (most previous engineers hired were men, and so the dataset of successful resumes is mostly male resumes) and more indirect dataset artifacts such as a correlation between line-length of resumes and the 'hire' label, combined with a notable difference in resume length between different genders such that male resumes are more hireable, but only because of an odd artifact of this dataset that it is correlated with. In some sense these two  are the same, they are detectable via similar methods, and are a shortcut to a real task caused by a particular dataset construction -- but I make the distinction as I've found they may be differently anticipated by humans. One is predictable given knowledge of historic dataset biases, the other is difficult to anticipate, and often so surprising as to be comical, as when NLI contradiction could be largely predicted by the presence of words about cats \citep{gururangan-etal-2018-annotation}. They belong in the same category as far as causal effects, but the conceptual difference can have an impact on discoverability.

The collapse of the two causes--dataset artifacts and failure to generalise--into one measure is not necessarily bad, since the behaviour in an application is the same, and thus the real world impact on people is the same. But it would be beneficial for researchers to develop ways to split out these two causes to better suggest mitigations. Splitting them out has another important benefit--it shows the overlap between fairness research and other areas of NLP.  Dataset bias work has significant overlap with work on dataset artifacts and on `shortcutting' \citep{geirhos2020shortcut}, generalisation failures have overlap with research on robustness and generalisation \citep{hupkes2023taxonomy}. If we explicitly recognise and leverage this, the field can share approaches and progress quickly, more than is currently done. For instance, AFLite is an algorithm developed to search a dataset for artifacts (such as `cat' and `contradiction') \citep{LeBras2020AdversarialFO} and then filter them, and comes from the dataset artifacts literature. LOGAN \citep{zhao-chang-2020-logan} is an algorithm for unsupervised discovery of social biases, from the fairness literature. They are implemented differently, AFLite is conceptually similar to k-fold validation with targeted sampling for artifacts, and LOGAN is a modification of k-means. But they can both be used to solve the same goal of finding slices of a dataset that exhibit strong imbalances based on a feature that should not have an imbalance. Since this comes from different subfields, no one has compared them. Similarly, the aforementioned work on fairness showing that automatic captioning doesn't generalise well to accents beyond white Californian male accents \citep{tatman17_interspeech} and that facial recognition doesn't generalise to non-white skin \citep{buolamwini18a} has much conceptually in common with generalisation work showing that natural language inference doesn't generalise to new syntactic structures \citep{mccoy-etal-2020-berts}. The areas do not acknowledge each other currently, nor share mitigation or analysis techniques, but there is much room to do so.

Given this observation, in this thesis I attempt to take inspiration and techniques from these related fields and incorporate them into fairness research. In Chapter~\ref{chapter:gender_bias_probing} and Parts~\ref{part:crosslingual} and \ref{part:generation} we run all experiments on multiple random seed initialisations, and analyse the models separately by seed. This is rarely done in fairness work, but generalisation work has shown it to drastically affect results \citep{mccoy-etal-2020-berts, multiberts}. Our results corroborate this; different seeds do show drastically different fairness properties despite equivalent development set performance, just as was found in \citet{mccoy-etal-2020-berts}.

We hope that in future these fields have more dialogue and joint work. 

\section{Transfer Learning}
\label{sec:bg_transfer_learning}
This thesis is on Fairness for Transfer Learning, so Transfer Learning also deserves some background explanation. Before this thesis, Transfer Learning had no intersection with fairness research. This has changed, partly because of the work here and follow on work it inspired, and partly because, Transfer Learning is, at the point of this thesis being written, so common that it is not generally specified anymore and is the unstated default. Back when this thesis was in its infancy, both the fields of fairness and of Transfer Learning were very small but growing exponentially. It seems that Transfer Learning has won, as almost everything is Transfer Learning (though fairness is no small field anymore either).

The central premise of Transfer Learning is that it doesn't make sense to start from a tabula rasa randomly initialised weight matrix every time you want to learn a new task, which we used to do before, but that many of the concepts necessary for one NLP task may be in common with another. Toxicity detection and sentiment analysis both require knowledge of basic sentence structure, nouns, verbs, and negative connotations of different words, and so knowledge from one should be able to augment knowledge from the other. Even more dissimilar tasks like toxicity detection and coreference resolution still require a similar underlying knowledge of sentence structure. Early work in Transfer Learning often sought to transfer from task to task like this; this approach is called \textit{domain adaptation} when done in sequence, or \textit{multi-task learning} when done simultaneously \citep{ruder2019transfer}. Both these approaches are now less common, and the field of Transfer Learning has coalesced into one paradigm: sequential pre-training of a language model on unlabelled text, followed by task specific fine-tuning. This leverages the vast amount of unstructured text to build high quality representations of language that can then serve as initialisations for any downstream language task. This is the approach that we use throughout this work.

For additional detail, in Part~\ref{part:crosslingual}, we use a variant of Transfer Learning that combines aspects of both the pre-train-finetune paradigm and the domain adaptation paradigm, when we do cross-lingual transfer. In cross-lingual Transfer Learning, the language model pre-training stage is multilingual (contains text in a variety of languages) and the fine-tuning stage is in a high-resource language (usually English) that doesn't match the target language at inference time. This is strictly called \textit{zero-shot cross-lingual transfer} (ZS-XLT) since the model has never seen labelled data in the target language. There also exists \textit{few-shot cross-lingual transfer} (FS-XLT), where the high-resource fine-tuning is continued with a few examples in the target language (often only hundreds). FS-XLT has been shown to generally perform better than ZS-XLT for relatively low additional annotation cost \citep{lauscher-etal-2020-zero}, but we use exclusively ZS-XLT in this work, since FS-XLT adds many additional layers of tuning and variability to the already complex landscape of cross-lingual transfer, and the small additional bump in performance was not necessary for our analysis.

Early Transfer Learning research varied between whether to `freeze' the language model after the first stage, and just learn whatever new parameters need to be added for the desired final task output space (e.g. the classifier or coreference model or etc that takes in the representations) or to continue to train the language model along with the second task, to further refine the representations to best fit the task specific needs. 
%I think the idea for this was whether it would generalise better if you froze. Do I need to talk about this?
In this work we use both methods, whichever is most standard for the task and enabled ease of analysis. We always specify which we use in each work's respective methodology.  
\chapter{Background}\label{chapter:background}

The following sections give requisite background information that common across all research in this thesis. Background that is relevant to only an individual work will appear before that work.

\section{Defining Fairness}
Fairness is a relatively recent interest in the field of Machine Learning/AI research, and as such it suffers from lack of standardisation in both definitions and methods of measurement. Much to its detriment, as any work must first define what it means by the term, and as idiomatic definitions and measurements across different works make meta-analysis difficult, and can hide contradictory results across different experiments. Fairness began to gain attention in AI in 2016, following the publication of the popular book \textit{Weapons of Math Destruction} \citep{oneil2016weapons}, which detailed all the ways that Machine Learning systems propagate injustice and create inequity in society, and the NeurIPS research paper \textit{Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings} \citep{bolukbasi}, which showed, via evocative word analogies from neural word embeddings, how career based gender bias was learnt by these systems, even when trained on relatively innocuous (for the internet) content like Google News. Machine Learning was shocked and galvanised by these works, though it was, as usual with surprising new discoveries, late to a party that other fields had been aware of for some time, as education and hiring had been looking at statistical fairness for the previous half a century \citep{hutchinson_mitchell_2019}. Since then, major NLP and AI conferences have added entire Ethics tracks, formed Ethics commmittees to review papers flagged for potential ethical concerns, encouraged ethics statements to be included in each work, and multiple fairness focused workshops have sprung up at each conference. But definitions and methods are still being solidified.

Broadly, fairness can be categorised as one of two types, as mentioned in the Introduction. \textbf{Allocational fairness}, which requires that systems perform equivalently for different individuals, regardless of demographics, and \textbf{representational fairness}, which requires that the systems represents different demographics with equal dignity. How exactly "equivalently" is measured is what determines and differentiates the available fairness metrics. For intuition, an example of allocational unfairness is how Automated Speech Recognition (ASR), which is now absolutely everywhere, has different error rates for different dialects of English, with increasing error the farther that speaker is from a white male in his 20s from California \citep{tatman17_interspeech}. An example of representational unfairness is how generative language models disproportionately generate text about black men as criminals and gay men as druggies, as compared to white men \citep{sheng-etal-2019-woman}. It is possible to have some blurry boundaries between these two, as in \citet{zhao-etal-2017-men}, which found that image captioning models were inaccurate for counter-stereotypical gender activites, like men shopping or cleaning, and women riding motorcycles or programming/gaming. This work frames the fairness issues as allocative (research tends to pick one of the two areas), but it could also be considered representational, as any application, e.g. an image search, that relies on a model with these errors will produce only stereotypical images.

\section{Measuring Fairness}
\label{sec:measuring_fairness}
\sgtcomment{I should probably introduce invariance here rather than just performance gap, since I use invariance as as a measurement in Part II. the difference between this is  causality (interventional vs. observational) which I should establish and explain the pros and cons}

\textbf{Notation:} In all fairness metric definitions contained in this work, let $a \in A$ be the demographic variable in question, where $A = \{privileged, minoritised\}$ group, such as $\{male, female\}$ or $\{native, immigrant\}$\footnote{An obvious limitation of this is that privileged and minoritised is binary, and tends to be true of fairness work, including work in this thesis. There is insufficient work on extending fairness metrics and constraints to multiclass, either theoretically or empirically.} In classification tasks (all tasks until Part~\ref{part:generation}) let $Y$ be the true label, $\hat{Y}$ be the predicted label, and $R$ be the classifier score (which enables analysis independent of classifier threshold). 

\textbf{Representational fairness} has no codified metrics of measurement in NLP, which is perhaps the most obvious of the many areas where NLP could learn from sociology and psychology and has failed to, for they have been measuring representational fairness in media for quite some time \citep{}. The field largely neglected to measure this until \citet{sheng-etal-2019-woman}, which proposed using a classifier to detect \textit{regard} for the subject of a passage in open domain generation, and found GPT-2 to systematically generate content causing lower regard when generating about women, African-Americans, and gays. This work is conceptually satisfying, and important, but difficult to expand due to the reliance on the classifier, which is limited to English and can become out of date over time as language drifts, so there have been a few follow up replications of this work but not many \citep{goldfarb-tarrant-etal-2023-prompt}. Other work on representational fairness that exists focuses on discovery of language model stereotypes, which makes up the majority of bias work done on generative models today \citep{goldfarb-tarrant-etal-2023-prompt} as measuring allocational fairness is not straightforward in generation. However, the most prevalent approaches to stereotype measurement, from two benchmark datasets, have been shown to be so flawed in construction as to be essentially meaningless \citep{blodgett-etal-2021-stereotyping}. As a result, this work focuses on only \textbf{allocational fairness}.

Measurement of allocational fairness is best overviewed by \citet{hutchinson_mitchell_2019} and \citet{barocas-hardt-narayanan}. At a high level, allocational fairness can be measured as \textbf{individual fairness}, which answers the question "are the results for similar individuals equivalent" and \textbf{group fairness}, or "is the performance for demographic subgroups equivalent". In the former, the work lies in defining the similarity function, in the latter, the work lies in selecting the demographic slices, and in choosing the performance measure. Choosing the demographic slices does not get much attention in NLP literature; there are a few nods to intersectionality \citep{} and to unsupervised demographic group discovery \citep{zhao-chang-2020-logan} and otherwise works assume that demographic groups are given, gold, and that discrimination against different demographic axes is independent-- ie discrimination against women can be treated entirely separately from against African Americans. In real life, this is patently false \citep{some intersectionality thing}. It remains unexamined how well this assumption works for Machine Learning. Most ML and NLP work uses group fairness, and compares the difference in performance across subgroups.  In classification, performance used is sometimes difference in F1 \citep{zhao-etal-2018-gender} but is often more granular, such as \textbf{equalised odds} \citep{hardt2016equality} which enforces equal false-positive rates (FPR) or true-positive rates (TPR) across groups.
\begin{align}\label{eq:fpr}
    P(\hat{Y} = 1, A=a, Y=0) (FPR)
\end{align}
\begin{align}\label{eq:tpr}
    P(\hat{Y} = 1, A=a, Y=1) (TPR)
\end{align}
where \ref{eq:fpr} and \ref{eq:tpr} are equal $\forall a \in A$.

Note that the second constraint \ref{eq:tpr} is equivalent to recall, as recall can be expressed the same way:
\begin{align}\label{eq:recall}
    \frac{\hat{Y} = 1 | Y=1}{(\hat{Y} = 1 | Y=1) + \hat{Y} = 0 | Y=1}
\end{align}.
The second constraint is often used in isolation and is terms \textbf{equality of opportunity} \citep{hardt2016equality} as a relaxation of \textbf{equalised odds}. 

Occasionally some works include related but different group fairness metrics, discussed in \citet{barocas-hardt-narayanan}, such as \textbf{independence}, \textbf{separation}, and (rarely) \textbf{sufficiency}. 

We use independence and separation, as well as the others, in \ref{chapter:gender_bias_probing}, to use the broadest number of metrics possible in order to establish the relationship between language model representations and application fairness metrics (also called \textbf{downstream} or \textbf{extrinsic} metrics). 

We consider a broad set of metrics because fairness metrics should be chosen based on the tasks in question. Different notions of fairness are in tension with each other, and are provably mutually unsatisfiable \citet{barocas-hardt-narayanan}. So fairness metrics need to be chosen based on the application and what makes most sense. Choices of fairness metrics involve a normative judgment, whether implicit or explicit. This is too often left implicit, and is often made based on what some prior similar work has used, for comparison, even if a different metric is both able to be used and would be more suited \citep{orgad-belinkov-2022-choose}.

These metrics are all ones that are applied on some downstream task. In Part~\ref{part:measurement} we analyse the relationship of these allocational fairness metrics to a number of novel measurements proposed in the NLP literature that can be applied to just representations. We will not survey them here, as what we use is specific to each work.

It seems potentially obvious to state, but a desirable characteristic of a measurement of fairness is that it a) accurately measures the concept that it purports to measure and b) has a reliable relationship to real world fairness. When a and b are true, the measurement has \textit{construct validity} -- a multi-faceted concept in the field of measurement modelling from the social sciences \citep{jacobsandwallach}, that attempts to define and make explicit the gaps between conceptualisation (e.g. my model should not discriminate based on race) and operationalisation (e.g. the performance gap between different racial groups, as identified by dialect identification). a) corresponds to \textit{content validity} and b to \textit{predictive validity}. Much of the work in Part~\ref{part:measurement} is motivated by our observation that these types of validity had not been examined and were assumed to be true. We thus set out to test and improve upon these types of validity.

In the first work on measurement (\ref{chapter:intrinsic_bias_metrics}), we focus on gaps in precision and recall, as previous work upon which we built our analysis used F1 \citep{zhao-etal-2018-gender}, and factoring them out gives both more granular analysis and also comparability to the equality of opportunity measure \citep{hardt2016equality}. In the second, we use the full suite of possible metrics. In Part~\ref{part:crosslingual} we use don't use a subgroup metric, but instead use counterfactual examples that perturb one demographic variable, where we make an invariance assumption that values should not change under this perturbation, and the magnitude of the change is our metric. This method does not fit cleanly into individual or subgroup fairness, as it can be analysed on an individual example (which we do) but those examples have also been constructed to stand in for a demographic. E.g. in the counterfactual example: \textit{I made her feel relieved} vs. \textit{I made him feel relieved}, \textit{her} and \textit{him} are individual instances of bias, but also are stand-ins for the concept of gender. In Part~\ref{part:generation} we measure retrieval rather than classification, so we use performance gap in the most common retrieval metric.  

\section{Common Approaches to Debiasing}
Fairness literature, as well as \textit{measuring} bias, will often propose methods of \textbf{debiasing}. Different debiasing methods proliferate, but most new methods do not get widespread adoption, since they fail to build trust. Debiasing methods tend to be proven in only quite constrained settings, on only one or two models, only in English, and on a limited number of tasks. \footnote{There is probably also some effect on the bias in publishing on encouraging novelty that new works tend to propose new methods rather than verifying existing methods, such that we have zillions of methods that no one wants to use}. This thesis therefore focuses on analysis, and does not propose any new methods. However, we will briefly survey existing methods that are used in Chapters~\ref{chapter:intrinsic_bias_metrics}, \ref{chapter:gender_bias_probing} and \ref{part:generation}.

Debiasing approaches fall into high level categories of where they occur in the lifecycle of training an NLP model: pre-, mid (during), and post. \textbf{Preprocessing}\footnote{I use the term preprocessing rather than pretraining to distinguish from the now common terminology of pretraining/finetuning} approaches involve a processing step that modifies data before training the model to reduce signal that can cause bias. For example, if a system used for resume filtering should be debiased with regard to binary gender, the data can be processed such that there is an equal occurrence of gender signifiers (pronouns, names, other words that encode gender information)\footnote{This is never easy to do fully, but can be quite successful in English with relatively coarse processing. It is not so easy in languages with much more gender marking, and this area is heavily underresearched. \citet{} looks into using morphological analysers for this.} with words that indicate profession or career information. This can be done on unstructured webtext that is used to learn embeddings or train language models \citep{} or on labelled data that is used for supervised finetuning \citep{}. These are usually known as \textbf{dataset balancing}, and differences in dataset balancing approaches stem from the method of changing initial data and the axes along which the data is balanced (gender/profession, race/toxicity, religion/sentiment, etc). If there is excess data (more commonly true of unstructured text), it can be subsampled such that there is less data and it is more balanced \citep{}. Other approaches are to oversample data such that some data is repeated \citep{} in order to essentially overweight those examples because that they occur more frequently in training data. This suffers from lack of diversity in the minority class, so some works opt to create synthetic data \citep{}. While this approach is most reliable, it is only available to practitioners who actually train models, which was always a small class and increasingly smaller as models continue to scale. There is also very little work that tries to balance multiple axes at once [Is there any?], there are a few works on intersectionality (which is not the same thing but related) \citep{} but even those are rare. And the difficulty of subsampling for instance would likely become infeasible when balancing multiple biases, which is essentially never done in research, but is a perfectly reasonable desire in real life applications, and for many cases is a required feature, as very few regulations and systems of integrity specify just \textit{one} minoritised group. This is clearly important future work.

Debiasing can be done in \textit{postprocessing} as well, generally on representations, though there is some preliminary work investigating decoding \citep{sheng-etal-2021-societal}. Both approaches are conceptually more complex than preprocessing, but do not require retraining a potentially very large and expensive language model. This enables postprocessing approaches to be done by parties further downstream than those that pretrain language models. This is crucial for appropriate debiasing, as it is then most connected to a downstream application, which we show is necessary in Chapter~\ref{chapter:intrinsic_bias_metrics}. It also allows more iteration and experimentation without extensive compute. \citet{ravfogel-etal-2020-null} operate on representations via nullspace projection -- they learn a linear classifier for binary gender, and then project language model representations onto the nullspace of that classifier. \citet{iskander-etal-2023-shielded} extend this method to remove non-linearly encoded information. We use these methods for analysis of the impact of demographics (rather than debiasing) in Chapter~\ref{part:generation}. The other methods of debiasing representations operate on individual words and groups of words that stand in for concepts: \citet{mrksic-etal-2017-semantic} pushes words together or away from each other in representation space; we use this method in Chapter~\ref{chapter:intrinsic_bias_metrics}. More recently a number of works use  `model-editing'  \citep{meng2022locating}, where individual neuron values can be changes in order to change one specific string. This has some issues with scaling to a full demographic (edits are granular) but would be a promising new direction for very targeted interventions.

%\citet{huang-etal-2020-reducing} takes this a step farther and regularises output to have similar sentiment under a counterfactual

Debiasing during training, via constraints or costs to the learning method \citep{zhao-etal-2017-men}, is less commonly done, perhaps partly perhaps because it contains the disadvantages of both preprocessing and postprocessing -- it is conceptually more complex and requires tuning hyperparameters (as does postprocessing) but it also requires retraining a model. It also is made more complex by transfer learning, as it is unclear whether to do it at one or both stages. We leave out this kind of debiasing from all of our analysis.

Recent hype around generative Language Model fairness focuses on a second stage training process, often called `alignment', which refers to the idea that human morality (it is never specified which human or which morality) can be instilled in a model by essentially incorporating a fine-tuning with ranking loss stage where the objective is for the model to be able to predict the correct ranked order of generations that differ with respect to desirable and undesirable properties, from grammaticality to length to factuality to fairness. The majority of this thesis predates the alignment trend, and very little deals explicitly with generation, so we do not use any of these techniques. Much of the alignment work has origins more in robotics than in fairness (as it is usually (though not always) applied via reinforcement learning). However, our work does have interesting similarities. In Chapter~\ref{chapter:gender_bias_probing} we measure fairness via differences in distributions for different demographics, via KL or Wasserstein distance, one of the standard ways to measure it (\S \ref{sec:measuring_fairness}). \citet{korbak2022on} show that Reinforcement Learning from Human Feedback (RLHF), the most common method of alignment, can be equivalent to distribution matching. So when given an appropriate setup, RLHF could (theoretically) be directly optimising for a fairness constraint.


%\subsection{Causal interventions for analysis}
%This is not bias specific, but is relevant for both parts 2 and 3 so I think it belongs up here


\section{Transfer Learning}
Transfer learning is, at the point of this thesis being written, so common that it is not generally specified anymore, but is the unstated default. Back when the research in this thesis was in its infancy, both the fields of fairness and of transfer learning were very small but growing exponentially. It seems that transfer learning has won, as almost everything is transfer learning (though fairness is no small field anymore either, and it is now common for work that is about any given topic to include a section on fairness evaluation). 
The central premise of transfer learning is that it doesn't make sense to start from a tabula rasa randomly initialised weight matrix every time you want to learn a new task, but that many of the concepts necessary to learn for one NLP task may be in common with another. For instance, toxicity detection and sentiment analysis both require knowledge of basic sentence structure, nouns, verbs, and negative connotations of different words, and so knowledge from one should be able to augment knowledge from the other. Even more dissimilar tasks like toxicity detection and coreference resolution still require a similar underlying knowledge of sentence structure. Early work in transfer learning often sought to transfer from task to task like this; this approach is called \textit{domain adaptation}, or when done simultaneously rather than in sequence \textit{multi-task learning} \citep{ruder2019transfer}. This is now less common, and the field of transfer learning has coalesced into sequential pretraining of a language model on unlabelled text, followed by task specific fine-tuning, which is the approach taken in this work in Part~\ref{part:measurement}. This leverages the extremely large amount of unstructured text available to build high quality representations of language that can then serve as initialisations for any downstream language task. 

We also use a variant of transfer learning in our analysis that combines aspects of both the pretrain-finetune paradigm and the domain adaptation paradigm, when we do cross-lingual transfer in Part~\ref{part:crosslingual}. In cross-lingual transfer learning, the language model pretraining stage is multilingual (contains text in a variety of languages) and the fine-tuning stage is in a high-resource language (usually English) that doesn't match the target language at inference time. This is strictly called \textit{zero-shot cross-lingual transfer} (ZS-XLT)since the model has never seen labelled data in the target language. There also exists \textit{few-shot cross-lingual transfer} (FS-XLT), where the high-resource fine-tuning is continued with a few examples in the target language (often only hundreds). FS-XLT has been shown to generally perform better than ZS-XLT for relatively low additional annotation cost \citep{}, but we use exclusively ZS-XLT in this work, since FS-XLT adds many additional layers of tuning and variability to the already complex landscape of cross-lingual transfer and the small additional bump in performance was not necessary for our analysis.

Early transfer learning varied between whether to `freeze' the language model after the first stage, and just learn whatever new parameters need to be added for the desired final task output space (e.g. the classifier or coreference model or etc that takes in the representations) or to continue to train the language model along with the second task, to further refine the representations to best fit the task specific needs. \sgtcomment{I think the idea for this was whether it would generalise better if you froze? But I don't remember actually.} In this work we use both methods, depending on what is most standard for the task or what enabled ease of analysis. We always specify in each work's respective methodology.  


%Note: Can do a subsec "fairness in pretrained models in transfer", "fairness in downstream models in transfer" and then the lack of anything doing both will be helpful motivation

\section{Fairness as Dataset Artifacts or Failure to Generalise}
\label{sec:fairness_as_other_fields}
The allocational fairness measurements we use (\S \ref{sec:measuring_fairness}) can have a number of different causes. For instance, racial bias in toxicity detection can come from labelled training data correlating African American dialect (AAVE) features with toxic content, as a result of an error or a bias in annotation \citep{sap-etal-2019-risk}. But allocational racial bias could also come from insufficient training data in African American dialect , resulting in higher error rates from that group, as \citet{tatman17_interspeech} measure for automatic captioning. Most work does not address this difference or disentangle these two causes, and lump both under "bias". It is unsurprising then that so much work fails to have predictive validity: correcting an anti-AAVE stereotype may not help allocational bias in a model if the root cause was simply that it modelled AAVE poorly. I will refer to these as dataset bias and generalisation failures. Even dataset bias can actually be split into two types of causes conceptually: dataset biases that replicate historical biases (most previous engineers hired were men, and so the dataset of successful resumes is mostly male resumes) and dataset biases from 'dataset artifacts' such as a correlation between length of resumes in lines and 'hire' label, combined with a notable difference in resume length between different genders. These two are detectable via similar methods and in some sense they are the same -- they are a shortcut to a task supported by a dataset -- but they may be differently anticipated by humans, as one is predictable given knowledge of historic dataset biases, and the other is difficult to anticipate, and often downright hilarious, as when NLI contradiction could be largely predicted by the presence of words about cats \citep{gururangan-etal-2018-annotation}. They do belong in the same category as far as causal effects, but the conceptual difference can have an impact on discoverability.

The collapse of these two causes into one measure is not necessarily bad, since the fairness effect in an application is equivalent, though as above it can cause some confusion. It would be beneficial for the field to regularly develop ways to split out these two causes (though causal analysis is of course challenging as it requires interventional studies which often take much more time and effort than observational). Splitting them out also shows the overlap between fairness work and other areas of NLP, and is thus valuable in its own right. Dataset bias work has significant overlap with work on dataset artifacts and on `shortcutting' \citep{geirhos2020shortcut}, generalisation failures have overlap with research on robustness and generalisation. This is rarely recognised or leveraged. For instance, AFLite is an algorithm developed to search a dataset for artifacts (such as `cat' and `contradiction') \citep{LeBras2020AdversarialFO} and then filter them, and comes from the dataset artifacts literature. LOGAN \citep{zhao-chang-2020-logan} is an algorithm for unsupervised discovery of social biases, from the fairness literature. They are implemented differently, AFLite is conceptually similar to k-fold validation with targeted sampling for artifacts, and LOGAN is a modification of k-means. But they can both be used to solve the same goal of finding slices of a dataset that exhibit strong imbalances based on a feature that should not have an imbalance.  Similarly, work on fairness showing that automatic captioning doesn't generalise well to accents beyond white Californian male accents \citep{tatman17_interspeech} and that facial recognition doesn't generalise to non-white skin \citep{buolamwini18a} has much conceptually in common with generalisation work showing that natural language inference doesn't generalise to new syntactic structures \citep{mccoy-etal-2020-berts}. The areas do not acknowledge each other currently, nor share mitigation or analysis techniques, but there is much room to do so.

Given this observation, in my work I attempt to take inspiration and techniques from these related fields and incorporate them into fairness research. In Chapter~\ref{chapter:gender_bias_probing} and Parts~\ref{part:crosslingual} and \ref{part:generation} we run all experiments on multiple random seed initialisations, and analyse the models separately by seed which is rarely done in fairness work, but which generalisation work has shown to be vital \citep{mccoy-etal-2020-berts, multiberts}. Our results show this is crucial, and different seeds do show drastically different fairness properties despite equivalent development set performance, just as was found in \citep{mccoy-etal-2020-berts}.

We hope that in future these fields have more dialogue and joint work. 
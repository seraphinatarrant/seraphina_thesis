\chapter{Background}\label{chapter:background}

The following sections give requisite background information that common across all research in this thesis. Background that is relevant to only an individual work will appear before that work.

\section{Defining Fairness}
Fairness is a relatively recent interest in the field of Machine Learning/AI research, and as such it suffers from lack of standardisation in both definitions and methods of measurement. Much to its detriment, as any work must first define what it means by the term, and as idiomatic definitions and measurements across different works make meta-analysis difficult, and can hide contradictory results across different experiments. Fairness began to gain attention in AI in 2016, following the publication of the popular book \textit{Weapons of Math Destruction} \citep{cathy_oneil_book}, which detailed all the ways that Machine Learning systems propagate injustice and create inequity in society, and the NeurIPS research paper \textit{Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings} \citep{Bolukbasi2016}, which showed, via evocative word analogies from neural word embeddings, how career based gender bias was learnt by these systems, even when trained on relatively innocuous (for the internet) content like Google News. Machine Learning was shocked and galvanised by these works, though it was, as usual with surprising new discoveries, late to a party that other fields had been aware of for some time, as education and hiring had been looking at statistical fairness for the previous half a century \citep{hutchinson_mitchell_2019}. Since then, major NLP and AI conferences have added entire Ethics tracks, formed Ethics commmittees to review papers flagged for potential ethical concerns, encouraged ethics statements to be included in each work, and multiple fairness focused workshops have sprung up at each conference. But definitions and methods are still being solidified.

Broadly, fairness can be categorised as one of two types, as mentioned in the Introduction. \textbf{Allocational fairness}, which requires that systems perform equivalently for different individuals, regardless of demographics, and \textbf{representational fairness}, which requires that the systems represents different demographics with equal dignity. How exactly "equivalently" is measured is what determines and differentiates the available fairness metrics. For intuition, an example of allocational unfairness is how Automated Speech Recognition (ASR), which is now absolutely everywhere, has different error rates for different dialects of English, with increasing error the farther that speaker is from a white male in his 20s from California \citep{tatman_2017}. An example of representational unfairness is how generative language models disproportionately generate text about black men as criminals and gay men as druggies, as compared to white men \citep{sheng-etal-2019-woman}. It is possible to have some blurry boundaries between these two, as in \citet{zhao-etal-2017-men}, which found that image captioning models were inaccurate for counter-stereotypical gender activites, like men shopping or cleaning, and women riding motorcycles or programming/gaming. This work frames the fairness issues as allocative (research tends to pick one of the two areas), but it could also be considered representational, as any application, e.g. an image search, that relies on a model with these errors will produce only stereotypical images.

\section{Measuring Fairness}
\textbf{Representational fairness} has no codified metrics of measurement in NLP, which is perhaps the most obvious of the many areas where NLP could learn from sociology and psychology and has failed to, for they have been measuring representational fairness in media for quite some time \citep{}. The field largely neglected to measure this until \citet{sheng-etal-2019-woman}, which proposed using a classifier to detect \textit{regard} for the subject of a passage in open domain generation, and found GPT-2 to systematically generate content causing lower regard when generating about women, African-Americans, and gays. This work is conceptually satisfying, and important, but difficult to expand due to the reliance on the classifier, which is limited to English and can become out of date over time as language drifts, so there have been a few follow up replications of this work but not many \citep{goldfarb-tarrant-etal-2023-prompt}. Other work on representational fairness that exists focuses on discovery of language model stereotypes, which makes up the majority of bias work done on generative models today \citep{goldfarb-tarrant-etal-2023-prompt} as measuring allocational fairness is not straightforward in generation. However, the most prevalent approaches to stereotype measurement, from two benchmark datasets, have been shown to be so flawed in construction as to be essentially meaningless \citep{blodgett-etal-2021-stereotyping}. As a result, this work focuses on only \textbf{allocational fairness}.

Measurement of allocational fairness is best overviewed by \citet{hutchinson_mitchell_2019} and \citet{barocas-hardt-narayanan}. At a high level, allocational fairness can be measured as \textbf{individual fairness}, which answers the question "are the results for similar individuals equivalent" and \textbf{group fairness}, or "is the performance for demographic subgroups equivalent". In the former, the work lies in defining the similarity function, in the latter, the work lies in selecting the demographic slices, and in choosing the performance measure. Choosing the demographic slices does not get much attention in NLP literature; there are a few nods to intersectionality \citep{} and to unsupervised demographic group discovery \citep{zhao-chang-2020-logan}. Most group fairness performance metrics in use are a variant of \textbf{equalised odds} \citep{hardt2016equality} 



\subsection{Fairness as Generalisation issues vs. as Stereotyping}
This is really only important for discussing part 2 with my experiments on random seeds, and then as some of the setup for part 3. It is kind of a minor point of both papers, but it is important to me cause I am really into it and it did change my setup for both. But maybe that doesn't belong here?

\section{Measuring Fairness}
\subsection{Desirable characteristics of fairness measurement: types of validity}

\subsection{How fairness can be measured: approaches from different subfields, as well as strengths and weaknesses of each approach} Should be pretty self explanatory. This is important for all of Part 1.

\paragraph{Probing as analysis}
This is also not bias specific but it relevant for both chapter 4 and part 3 so I think it should be up here

\subsection{Causal interventions for analysis}
This is not bias specific, but is relevant for both parts 2 and 3 so I think it belongs up here


\section{Transfer Learning}
\subsection{transfer learning  cross-lingually}
Obvs only relevant for part 2.
\subsection{fairness in transfer learning}

Note: Can do a subsec "fairness in pretrained models in transfer", "fairness in downstream models in transfer" and then the lack of anything doing both will be helpful motivation



\section{Common Approaches to Debiasing}
This is important for chapters 3 and 4 of part 1. It could also include removing information, which I used in part 3 (as it is a debiasing method) though that type is not used in part 1. 
.

Then: More specific stuff that probably goes later

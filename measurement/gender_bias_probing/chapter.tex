\chapter{How Gender Debiasing Affects Internal Model Representations,
and Why It Matters}\label{chapter:gender_bias_probing}

The previous work showed that WEAT, the common measure of bias in embedding spaces, doesn't correlate with application bias, in a variety of settings. This does not mean that debiasing at the language model level cannot work, but it does mean that we cannot tell if it has worked via the measures we have available. At the end of that work, we recommend that bias be always tested in a downstream application. There were no promising trends of correlation at all, and no indication of a clear way to develop a metric that correlates better. A potential (though cheap) criticism of this finding could be that the embeddings were non-contextual (despite there being no indication that contextualising word representations would change this, as it only makes everything much more complex). But our work has also been replicated contextually in \citet{cao-etal-2022-intrinsic}, who study a less extensive set of demographics and of languages, but a more broad set of intrinsic and extrinsic metrics, for 19 contextualised models. They even attempt to make small modifications to intrinsic and extrinsic metrics to make them align better, and still find no reliable correlation. 

But as we stated at the start of this section, implementing all downstream systems is onerous and over time less and less possible as systems are being used everywhere and the size and prevalence of pretraining for transfer learning increases. So we studied the reverse direction from the previous work: we cannot yet tell in what way modifying a language model representation affects downstream bias, but what about reverse flow? We know that debiasing at the second stage of transfer learning works (this is fundamentally not different than just debiasing a single-stage non-transfer learning application). So perhaps this is a better place to start, and it will be more enlightening to look at how debiasing downstream (the thing we understand better) affects representations upstream (the thing we understand less well) rather than the reverse. 

This motivates the following work, where we study the impact of downstream debiasing on language model representations. We know that downstream debiasing does affect downstream bias metrics, and if the language model is not `frozen' (e.g. the downstream debiasing backpropagates to change the language model parameters) then this changes model representations as well. This proved fruitful. We found the CEAT metric (contextualised WEAT) to be as uncorrelated in this reverse direction of downstream task $\rightarrow$ to language model as we had found WEAT to be when going in the original direction of language model $\rightarrow$ to downstream task. But we found that information theoretic probing could be adopted as a good gender bias metric when probing for gender demographic information. Information theoretic probing had previously been used as a method of analysis of linguistic properties of a learned representation, such as whether a language model representation includes information about part-of-speech, or dependency parses. By this `reversed' method of the previous analysis we were able to adapt it into a bias metric.

This study also enabled further disentanglement of the role of the language model vs. the downstream classifier in fairness in transfer learning. Through information theoretic probing we were able to identify the language model's `potential' for gender bias, which then may or may not be realised by the classifier depending on the downstream fine-tuning data. If the language model has strong potential for gender bias, and the downstream fine-tuning data is very imbalanced such that gender $A$ is a strong predictor of labels $Y$, then the language model will be biased. But even with strong potential, if the fine-tuning data is not very imbalanced, then the potential will not be realised. Conversely, with a language model with lower potential, the imbalance in fine-tuning data has a smaller effect. In an interview with \href{https://about.me/sandra_kublik}{Sandra Kublik} about this work, she suggested the genetic analogy to explain this behaviour clearly to laypeople. I can have a genetic propensity for breast cancer, or for schizophrenia but that may or may not ever be realised depending on my environmental factors. I have used this analogy ever since. So this work shows that, just as with genetics, the full story cannot be determined from the language model, but it can be a helpful indication of potential.

Finally, in this work we used the full suite of ten bias metrics that are generally applied to classification, which is rarely if ever done. They tend to track with each other, but for one of the two tasks had very different magnitudes, so had we studied a smaller subset of them, we may have come to different conclusions. As before we assert that the fairness metric must be matched to the downstream application, but for work like this that analyses general relationships between metrics, we show how it is necessary to include the broad range to draw robust conclusions. 
We also find a pattern in a specific type of bias metric: Pearson correlation to real-word bias statistics, and show it to be flawed. Pearson correlation metrics correlate one of the performance gap metrics mentioned in \ref{sec:measuring_fairness} to real world disparities, which for these studies is the gender bias in profession classification or profession based co-reference resolution to gender disparities in professions in the real-world (usually the United States). We show that these metrics are extremely unreliable because they hide a confound: whether or not the statistics of the pre-training data reflect the statistics of the real world. So we recommend at the very least using one of the other metrics, unless the relationship to the real world is the object of study itself, rather than the bias behaviour of a language model. I did not include correlation based metrics in my overview of fairness metrics, because of these flaws. 

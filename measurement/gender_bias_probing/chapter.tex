\chapter{How Gender Debiasing Affects Internal Model Representations,
and Why It Matters}\label{chapter:gender_bias_probing}

The previous work showed that WEAT, the common measure of bias in embedding spaces, doesn't correlate with application bias. Debiasing at the language model could still sometimes work, but we showed that we cannot tell if it has worked without implementing a downstream system. We recommended that bias be always tested in a downstream application. 
%There were no promising trends of correlation at all, and no indication of a clear way to develop a metric that correlates better. A potential (though cheap) criticism of this finding could be that the embeddings were non-contextual (despite there being no indication that contextualising word representations would change this, as it only makes everything much more complex). 

This recommendation was strengthened when our work was later replicated with contextual embeddings in \citet{cao-etal-2022-intrinsic}, who study a less extensive set of demographics and of languages, but a more broad set of intrinsic and extrinsic metrics, for 19 different contextualised models. They still find no reliable correlation, thought they make small modifications to intrinsic and extrinsic metrics to try to make them align better. 

%But as we stated at the start of this section, implementing all downstream systems is onerous and over time less and less possible as systems are being used everywhere and the size and prevalence of pretraining for transfer learning increases. 
But implementing downstream systems is exactly what the field is trying to avoid. The increase in scale of pre-training over the past five years is only exacerbating this; less and less pre-training is done by people who deploy systems. I pre-trained the embeddings in the previous chapter (\ref{chapter:intrinsic_bias_metrics}) on a university cluster, but very few train new BERT models, and only a handful train LLMs.

So in the following, we make progress on understanding the relationship between intrinsic and extrinsic bias by studying the reverse direction. We cannot yet tell in what way modifying a language model representation affects downstream bias, so how does downstream debiasing affect an upstreadm langauge model? We know a priori that debiasing at the second stage of transfer learning works. 
%(this is not fundamentally different than just debiasing a single-stage non-transfer learning application). 
So perhaps this is a better place to start, and it will be more enlightening to look at how debiasing downstream (the thing we understand better) affects representations upstream (the thing we understand less well). 

This motivates the following work, we try to understand language model representations better by studying the impact of downstream debiasing. We know that downstream debiasing does improve downstream bias metrics. If the language model is not `frozen' (e.g. the downstream debiasing backpropagates to change the language model parameters) then this changes model representations as well. This worked. We found the CEAT metric (contextualised WEAT) to be as uncorrelated in this reverse direction of downstream task $\rightarrow$ language model as we and \citep{cao-etal-2022-intrinsic} had found WEAT and CEAT to be when going in the original direction of language model $\rightarrow$ downstream task. But we found that \textbf{information theoretic probing} could be adopted as a good gender bias metric when applied to gender demographic information. Information theoretic probing had previously been used as a method of analysis of compressibility of linguistic properties of a learned representation: POS tags, dependency parses, etc. By this `reversed' method of the previous analysis we were able to adapt it into an upstream language model bias metric.

In this work we also disentangled of the role of the language model vs. the downstream classifier for fairness in transfer learning. Through information theoretic probing we were able to identify a language model's `potential' for gender bias, which then may or may not be realised by the classifier depending on the downstream fine-tuning data. When \href{https://about.me/sandra_kublik}{Sandra Kublik} interviewed me and we discussed this work, she suggested a genetic analogy to give intuition for this behaviour clearly to laypeople. I can have a genetic propensity for breast cancer, or for schizophrenia, but that may or may not ever be realised depending on my environmental factors. 
Similarly, if the language model has strong \textit{potential} for gender bias, and the downstream fine-tuning data is imbalanced such that gender $A$ is a strong predictor of labels $Y$, then the language model will be biased. But even with strong potential, if the fine-tuning data is not imbalanced, then the potential will not be realised. Conversely, with a language model with lower potential, the imbalance in fine-tuning data has a smaller effect. 
%I have used this analogy ever since. 
So this work shows that, just as with genetics, the full story cannot be determined from the language model, but some of the story can be.

In this work we also used the full suite of ten bias metrics that are generally applied to classification, which is rarely done. They tend to track with each other, but for one of the two tasks had very different magnitudes, so had we studied a smaller subset of them, we may have come to different conclusions. Fairness metrics must be matched to downstream applications, as discussed in \S \ref{sec:measuring_fairness}, but for work that analyses general relationships between metrics, we show that it is necessary to include the broad range to draw robust conclusions. 
We also find a pattern in a specific type of bias metric: Pearson correlation to real-word bias statistics, and show it to be flawed. Pearson correlation metrics correlate one of the performance gap metrics mentioned in \S \ref{sec:measuring_fairness} to real world disparities, which for these studies is the gender bias in profession classification or profession based co-reference resolution to gender disparities in professions in the real-world (usually the United States). We show that these metrics are extremely unreliable because they hide a confound: whether or not the statistics of the pre-training data reflect the statistics of the real world. So we recommend at least using one of the other metrics, unless the relationship to the real world is the object of study itself, rather than the bias behaviour of a language model (this is rarely the case in work in ML venues).  I did not include correlation based metrics in my overview of fairness metrics \S \ref{sec:measuring_fairness} because of these flaws. 

\part{Measuring the Relationship between Fairness in Pretraining and Fairness in Downstream Applications}
\label{part:measurement}

In the sequential transfer learning paradigm -- the dominant approach to transfer learning --  a new difficulty for fairness research emerges. A language model, which is trained first, can be used in many different downstream applications. Any biases learnt by the language model, can propagate into many different applications. But testing in every single application would be onerous and is sometimes not possible: the engineer training a language model may not have the data and expertise to train models in toxicity detection and sentiment analysis and named entity recognition and information retrieval...and the many other tasks that are instantiated from a BERT model, still the most popular pretrained model at time of writing. 
Even if they can, they're unlikely to have the domain knowledge to reason about the types of algorithmic discrimination that are risks for each use case.
%or may not have the data or domain knowledge to implement a realistic system. Even if they do, they will not have domain knowledge of \textit{all} possible applications, so unless toxicity classification tracks exactly the same as named entity recognition and speech recognition, 
Accurate testing on a full set of downstream applications is impossible. So it is desirable to have a way to measure bias in the first stage, at the language model level, and to be able to predict effects downstream.

The NLP field did quickly search for a way to do this. At the same NeurIPS in 2016 one of the keynotes was on the exponential growth of transfer learning \citep{ng2016nuts}, and \citet{bolukbasi} published the first work on gender debiasing word embeddings using a post-processing method based on PCA. This work analysed bias via the lens of the word analogy task (man is to woman as king is to \textunderscore), which had been quite popular as an assessment of word embedding performance. This was the start of considering bias to be a property of word embedding geometry.
This was reinforced shortly thereafter, when a computational social science work showed that the Implicit Association Test (IAT) for human psychological biases could replicated via cosine similarities between word embeddings \citet{Caliskan2017SemanticsDA}. This new measure, WEAT, became used as a predictive measure of language model biases, and was used as the sole metric to support a plethora of new embedding debiasing algorithms \citep{list_alot}.  
In both of these, bias was operationalised as distances between word vectors in three hundred to one thousand dimensional space, or via finding the principal components associated with a demographic (usually gender) subspace \citep{ethayarajh-etal-2019-understanding}. 

The following work challenges the implicit assumption behind this measure -- that a measurement of embedding geometry, WEAT, is predictive of downstream bias measurements.
WEAT is observational, like the downstream extrinsic fairness metrics discussed in \S \ref{sec:measuring_fairness}, but unlike those metrics, it is not \textit{directly} observational of societal harm, the way True Positive Rate gaps are. It is observational of an embedding space, but the relationship between that space and societal harm has not been established. 

Prior to the work below it had only been established to reveal bias and imbalances in concepts in a dataset, as an additional tool for understanding sampling bias and reporting bias and even artifacts that are nonetheless in unstructured data and can be learnt from the collocational way that embeddings are trained. However, the body of bias work on embeddings used this measure with the assumption that it had predictive validity. Research implicitly assumed that downstream bias would track with WEAT metrics, or at the very least, that if WEAT bias measures went down (less bias) then downstream bias would go down.

Given the many possible definitions of bias and different underlying causes outlined in \S \ref{sec:measuring_fairness} and \S \ref{sec:fairness_as_other_fields}, this is quite a strong claim, too strong to assume to be true from intuition. On close examination in fact, Intuition would not necessarily support this. Some tasks do rely on concept proximity, like recommendation engines at the time, and for these intuition would suggest that a measure of embedding geometry might be predictive of downstream task bias. But this is an exception. For most tasks that involve transfer learning, the objective function used for training and the output space for a downstream classifier are very different. This is one of the hallmarks of transfer learning and makes it complex. In this case it seems unlikely that a simple measure like WEAT will be consistently predictive of the variety of downstream tasks.
%when the relationship between geometry and classification behaviour is unclear. 
The claim that debiasing an embedding space is helpful not only has unproven predictive validity, but lacks \textit{face validity}, which is the `sniff test' of whether it looks on the surface level to be plausible.  

This observation motivates the following work: to discover if and when WEAT has predictive validity of bias in downstream applications, and the utility of using it as a measure.  
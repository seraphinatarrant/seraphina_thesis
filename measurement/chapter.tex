\part{Measuring the Relationship between Fairness in Pretraining and Fairness in Downstream Applications}
\label{part:measurement}

In the sequential transfer learning paradigm, a new difficulty emerges: a language model, which is trained first, can be used in many different downstream applications. Thus if there are any biases learnt by the language model, they can be propagated at scale into many different applications. But testing in every single application would be onerous and is sometimes not even possible: the engineer training a language model may not be a sufficient enough expert in toxicity detection reason about possible pitfalls and avenues for algorithmic discrimination in that use case, or may not have the data or domain knowledge to implement a realistic system. Even if they do, they will not have domain knowledge of \textit{all} possible applications, so unless toxicity classification tracks exactly the same as named entity recognition and speech recognition, testing on a representative set of downstream applications will be impossible. It is therefore very desirable to have a way to measure bias in the first stage, at the language model level, before proceeding downstream. 

The field did quickly search for a way to do this. Transfer learning began to become popular in around 2016 when Andrew Ng gave a NeurIPS keynote on it \citep{ng2016nuts}, and in the same year (the same NeurIPS even) the seminal fairness in language models work was published \citep{bolukbasi}. This work analysed bias via the lens of the word analogy task (man is to woman as king it to \textunderscore), which had been quite popular as an assessment of word embedding performance. Bias and performance were operationalised as distances between word vectors in three hundred to one thousand dimensional space, or via finding the principal components associated with a demographic (usually gender) subspace \citep{ethayarajh-etal-2019-understanding}. Shortly thereafter, a computational social science work showed that the Implicit Association Test (IAT) for human psychological biases could replicated via cosine similarities between word embeddings \citet{Caliskan2017SemanticsDA}, and this new measure, WEAT, became used as a predictive measure of language model biases, and was used as the sole metric to support a plethora of new embedding debiasing algorithms \citep{list_alot}.  

However, like the downstream extrinsic fairness metrics discussed in \S \ref{sec:measuring_fairness}, WEAT is observational, not causal, but unlike those metrics, it is not directly observational of societal harm, the way true positive rate gaps are. It is observational of an embedding space, but the relationship between that space and societal harm has not been established. Prior to the work below it had only been established to reveal bias and imbalances in concepts in a dataset, as an additional tool for understanding sampling bias and reporting bias and even artifacts that are nonetheless in unstructured data and can be learnt from the collocational way that embeddings are trained. However, the body of bias work on embeddings used this measure with the assumption that it had predictive validity: that downstream bias would track with WEAT metrics, or at the very least, that if WEAT bias measures went down (less bias) then downstream bias would go down.

Given the many possible definitions of bias and different underlying causes outlined in \S \ref{sec:measuring_fairness} and \S \ref{sec:fairness_as_other_fields}, this is quite a strong claim, too strong to assume to be true from intuition. Intuition in fact, on close examination, would not necessarily support this claim. For some tasks that rely on concept proximity, like recommendations engines, intuition would suggest that a measure of embedding geometry like cosine similarity might be predictive. But many tasks that involve transfer learning there is a big difference between the objective function used for training and the output space when jumping from the language modelling to classification or other task. This is one of the hallmarks of transfer learning and makes it complex. Intuition for this case makes it seem unlikely that a simple measure like this will be consistently predictive of the variety of downstream tasks, when the relationship between geometry and classification behaviour is unclear. Thus the claim that debiasing an embedding space is helpful not only has unproven predictive validity, but does not even have \textit{face validity}, which it to say that it does not pass the sniff test and look on the surface level plausible.  

This observation motivates the following work: to discover if and when WEAT has predictive validity of bias in downstream applications, and the utility of using it as a measure.  
\part{Fairness in Transfer across Languages}
\label{part:crosslingual}

We now are armed with an understanding of the relationship between upstream and downstream fairness and both the predictive validity and considerations of using different measurements. We established in the second half of Part~\ref{part:measurement} that, within English, for gender bias, upstream potential can be realised in downstream behaviour, but is not always, depending on downstream data.

In Part~\ref{part:crosslingual} we look into more complex cases: more languages, more types of biases, and more complex transfer learning across \textit{languages} rather than just across objectives/tasks. We can ask questions like, `Do other languages behave like English with regard to fairness?' and `Does cross-lingual transfer affect fairness outcomes?'. 

Or at least we can ask these questions in theory. In practice, at the time this work was done, there were almost no resources available for testing bias downstream in multiple languages (with the exception of a multilingual version of WEAT \citep{}), which we established in \ref{chapter:intrinsic_bias_metrics} fails to have predictive validity). At the time of writing this thesis years later, there is surprisingly very, very little. This is all the more surprising given that models are by now, in the LLM-age, by default multilingual, even supposedly English only models will readily speak high resource languages, and are used to, because common use is determined by capabilities rather than by terms of use.

This work was made additionally challenging by the the lack of pre-existing research as well as of resources. When this work was done there were three works examining the effect of grammatical gender on gender bias in multilingual word embeddings \citep{} and nothing at all examining at cross-lingual transfer. My initial PhD proposal in 2018 proposed to look at this, and I have always found it surprising that no one had, and to date there are still only a handful more works. From first principles, different languages have different distributions of strings, concepts, function words, grammatical markers, and these affect the distribution learnt, what type of information is encoded, what is emphasised, what is compressed, what is lost. This is supported by the work on multilingual word embeddings, which showed that grammatical gender dramatically affects representations learned even when \textit{not} aligned to semantic gender. For example \textit{la amiga} in Spanish "the female friend" has grammatical gender aligned to semantic gender, the feminine grammar form expresses the real world gender of the referent, contrasted with \textit{el amigo} "the male friend", or in recent years the additional option of \textit{le amige}, for expressing non-binary gender or leaving gender underspecified. \textit{La tabla} "the table" in Spanish also expresses grammatical female gender, but not aligned to semantic gender (as we do not generally consider tables to have semantic gender in the cultures with which I am familiar)\citep{corbett? grammar chapter}. This previous work found that even though for most words grammatical gender has no semantic meaning, it is one of the signals most strongly encoded in representations. So we know that even if we entirely discount cultural differences (which is clearly nonsense, but measuring cultural differences is so difficult to do that in ML it is only ever done poorly) linguistic differences change the representations and vocabulary distributions learnt. And we also know that these same characteristics of distributions and representations: compression, what is encoded, etc, are strong causal factors in inequities and biases expressed by a model. So it seemed obvious to me that there would be some effect on fairness from superimposing one language onto another, or from in any way blending this data, as a stronger or more ambitious form of transfer learning with not just disjoint vocabularies (across domains) or labels (across tasks) but also structures (across languages). So this section builds to rigorously showing that intuition, expressed in the second title \textit{Cross-lingual Transfer Learning Can Worsen Biases in Sentiment Analysis}; I had a hypothesis that there would be an effect, and the effect turns out unsurprisingly to be very complex and difficult to disentangle. 


So in this section, we begin by creating a resource to answer some of our questions. We select a task that has data in many languages: sentiment analysis, and create an evaluation benchmark to test fairness properties for sentiment analysis in a number of these languages, using the methodology used for English in \citet{kiritchenko-mohammad-2018-examining}, which sets up counterfactual tests for the effect of demographics on sentiment.  Once we've created the resource, we ask set of research questions that will slowly build towards being able to ask question about cross-lingual transfer. The two works directly build on each other: the first examines how transfer learning affects fairness within one language, for four languages. The second then moves on, using the same resource, to asking more complex questions: how does cross-lingual transfer affect bias expressed? These more complex questions require many experimental controls, as disentangling causes necessitates a broad set of ablations, so there are many, though still less than I would prefer.

All research ends up with unanswered additional questions, small phenomena discovered along the way that I wish I had had more time and resources to investigate. This research below had the most: since in each case, when an additional question came up, there was no previous research at all to lean on or answer these side questions. So often a small footnote or single sentence hides a few days or a week of investigation that turned out to be too large and had to be abandoned for future work. I am proud of all of the work in this thesis, but more than any other section I hope this lays some groundwork and helps galvanise other researchers to take up the area of cross-lingual transfer and push this forward. 

Models are becoming more and more multilingual, such that many applications and use cases today by default use cross-lingual transfer, though they do not call it so, just as pretraining is now so common it is no longer called transfer learning. Research is not increasing commensurate with this, or even increasing much at all \citep{EMNLP paper}. We risk leaving other languages behind. It is always a question whether new technologies will benefit society and improve lives, or will increase inequalities. The answer is almost always a blend of these, but quite clearly if we ensure fair NLP technologies only in English, we will tip farther towards increasing inequalities. 

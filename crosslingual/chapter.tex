\part{Fairness in Transfer across Languages}
\label{part:crosslingual}

We now have an understanding of the relationship between upstream and downstream fairness. 
%and both the predictive validity and considerations of using different measurements. 
In the second half of Part~\ref{part:measurement} we established that, within English, for gender bias, upstream potential can be realised in downstream behaviour, but is not always, depending on downstream data.

In Part~\ref{part:crosslingual} we look into more complex cases: more languages, more types of biases, and more complex transfer learning across \textit{languages} rather than just across objectives/tasks. We ask two questions: `Do other languages behave like English with regard to fairness?' `Does cross-lingual transfer affect fairness outcomes?'. 

%Or at least we can ask these questions in theory. In practice, 
At the time this work was done, there were almost no resources available for testing bias downstream in multiple languages (with the exception of a multilingual version of WEAT \citep{lauscher-glavas-2019-consistently}), which we established in Chapter~\ref{chapter:intrinsic_bias_metrics} fails to have predictive validity). At the time of writing this thesis years later, there is still very little, despite that models are by now in the LLM-age, default multilingual. Even purportedly English only models readily speak high resource languages due to data contamination in the petabytes of pre-training data. Common use is determined by capabilities rather than by terms of use -- so all models are now multilingual.

There was also a lack of pre-existing research into this topic. When this work was done there were three works examining the effect of grammatical gender on gender bias in multilingual word embeddings \citep{gonen-etal-2019-grammatical, zhou-etal-2019-examining, McCurdy2017GrammaticalGA} and we built hypotheses and experiments upon this work. But there was nothing examining cross-lingual transfer. I initially proposed to look at this in 2018, so I have always found it surprising that still few have. To date there are only a handful more works. 

Different languages have different distributions of strings, concepts, function words, grammatical markers, and these determine many things that matter. They determine the distribution learnt, what type of information is encoded, what is emphasised, what is compressed, what is lost. The work on multilingual word embeddings mentioned above \citep{gonen-etal-2019-grammatical, zhou-etal-2019-examining, McCurdy2017GrammaticalGA} shows that grammatical gender dramatically affects representations learned even when \textit{not} aligned to semantic gender. \textit{La amiga} in Spanish "the female friend" has grammatical gender aligned to semantic gender, the feminine grammar form expresses the real world gender of the referent, contrasted with \textit{el amigo} "the male friend", or in recent years also \textit{le amige}, for expressing non-binary gender or leaving gender under-specified. \textit{La tabla} "the table" in Spanish also expresses grammatical female gender, but is not aligned to semantic gender (as we do not generally consider tables to have semantic gender in the cultures with which I am familiar)\citep{corbett_non_canonical, corbett_1991}. Those three works found that even though for most words grammatical gender has no semantic meaning, it is one of the signals most strongly encoded in representations, so it effects the learned semantics. \citet{gonen-etal-2022-analyzing} applied the PCA of \citet{bolukbasi} multilingually, and found that you cannot isolate gender to the first principle component in gender marking languages, as you can in English. In gender marking languages the information is encoded across more axes.

So we know that even if we entirely discount cultural differences (which is clearly nonsense, but measuring cultural differences notoriously difficult) %is so difficult to do that in ML it is only ever done poorly) 
linguistic differences change the representations and vocabulary distributions learnt. And we also know that these same characteristics of distributions and representations: compression, what is encoded, etc, are strong causal factors in inequities and biases expressed by a model. This is what we discovered in Chapter~\ref{chapter:gender_bias_probing}.  So is there some effect on fairness from superimposing one language onto another, or from blending language data? It is a more ambitious form of transfer learning with not just disjoint vocabularies (across domains) or labels (across tasks) but also \textit{structures} (across languages). This section builds to rigorously showing that there is an effect, expressed in the second title \textit{Cross-lingual Transfer Learning Can Worsen Biases in Sentiment Analysis}. This effect turns out to be very complex and difficult to disentangle from other confounds, but there is definitely not no effect. 

In this section, we begin by creating a resource to answer our questions. We select the task of sentiment analysis as it has data in many languages. We create an evaluation benchmark to test fairness properties for sentiment analysis in a number of these languages, using the methodology for English in \citet{kiritchenko-mohammad-2018-examining}. Their method sets up counterfactual tests for the effect of demographics on sentiment.  Once we've created the resource, we ask set of research questions that slowly build towards answering questions about cross-lingual transfer. The two works that follow directly build on each other: the first examines how transfer learning affects fairness \textit{within one language}, for four languages (+ English). The second moves on, using the same resource, to asking more complex questions: how does cross-lingual transfer affect bias? 
%These more complex questions require many experimental controls, as disentangling causes necessitates a broad set of ablations, so there are many, though still less than I would prefer.

%All research ends up with unanswered additional questions, small phenomena discovered along the way that I wish I had had more time and resources to investigate. This research below had the most. In each case, when an additional question came up, there was no previous research to lean on or answer these side questions. Often a small footnote or single sentence hides a week of investigation that turned out to be too large and had to be left for future work. More than any other section I hope this one lays some groundwork and helps galvanise other researchers to take up the area of cross-lingual transfer and push this forward. 

Models today by default use both transfer learning and cross-lingual transfer, though both things are now so common that they are generally not stated. Multilingual fairness research is not increasing commensurate with the growth of utilisation of multilingual models in practice, or even increasing much at all \citep{ruder-etal-2022-square, blasi-etal-2022-systematic}. 

As with bias \textit{analysis} as opposed to debiasing, publication processes disincentivise multilingual work. Multilingual work scales linearly in compute, experiment management, and analysis time in number of languages.

But we risk leaving other languages behind, in fairness particularly. What does it mean if an NLP system exists in one hundred languages, but is fair in only English? It is always a question whether new technologies will benefit society and improve lives, or will increase inequalities. The answer is almost always a blend of these, but quite clearly if we ensure fair NLP technologies only in English, we will tip farther towards increasing inequalities. 

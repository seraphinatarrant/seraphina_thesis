\chapter{Cross-lingual Transfer in Sentiment Analysis}
\label{chapter:multilingual_sentiment_analysis_pt2}

The next work directly follows on from the previous results: the experiments were planned together and directly inform each other, despite being published separately. 

In the previous work, we created the resources needed to do these experiments, as it was the first work on fairness in language models across multiple language families. We found that there is an effect on fairness from transfer learning within one language. We can't disentangle the exact \textit{causes} of this effect from those experiments: whether it is information contained in the data, or the additional stability of the model from the addition of more data, though we hypothesise the latter (stability from more data) is the cause. Regardless of the causes, the findings are useful in practice under resource constraints, if less scientifically satisfying than if we had controlled all variables. 

In the following work, we examine the more complex setting of cross-lingual transfer in all the same languages, and again ask how this setting changes fairness outcomes. However, we set up our experiments to control as many variables as possible and establish causes, without pre-training all new models from scratch (that is, we limit ourselves to fine-tuning only, as the multilingual setup has already extremely many variables and requirements on compute resources). The full set of experimental variables we consider is:
\begin{itemize}
    \item Type of bias. We look at gender bias and racial/country of origin bias. We might expect these to have different patterns of cross-lingual transfer as gender is encoded in some languages in a way that race is not (via gender agreement) and as gender biases tend to be global and common across languages in a way that racial biases are not (the minoritised racial groups differ culture to culture).
    \item Mono vs. multilingual pre-training. We examine what happens when changing from monolingual to multilingual pretraining \textit{without} changing the fine-tuning data. This would not be none in practice in a production system, but enables us to isolate the two types of data (pretraining and fine-tuning) that usually change when going from a monolingual transfer to a cross-lingual transfer model. In our first experiments, we hold fine-tuning data constant for each language and change only pretraining data.
    \item Target language fine-tuning vs. transfer language fine-tuning. In a monolingual transfer setup, a model applied to Spanish as the target language will be fine-tuned on Spanish. In cross-lingual transfer, it will be fine-tuned in another language (in this case English) and then applied to Spanish. In these next experiments, we hold the pretrained multilingual model constant and change the fine-tuning data.
    \item Random seed. All experiments report the majority vote over five random seeds for the weight initialisation of the classifier and the data shuffle for fine-tuning. We initially did an analysis by individual random seed, and found them to differ so strongly that sometimes even polarity of the bias flipped: that is, for random seed A there would be anti-female bias and for random seed B there would be anti-male bias. We take majority vote to indicate what would be the most likely thing to happen for a random seed picked out of a hat.
    \item Distillation. We do the same set of experiments for full-size (100-150 million parameters) and for distilled models, which are approcimately half the number of parameters. These experiments make our results more applicable in practice, as distilled models are commonly used in combination with cross-lingual transfer as both are methods to deal with insufficient data or resources.
    %\item Label balance. 
\end{itemize}

There are nonetheless two experimental variables that we consider important but were unable to include. We do not look at the effect of modifying pretraining data: it is an important variable in the manifestation of social bias, but it is the most difficult to experiment on because training a model from scratch is so challenging. It is also the one least likely for developers of NLP systems to modify in practice, for that very reason. We also do not look at the effect of domain match/mismatch. In practice many sentiment systems have a domain mismatch, since sentiment training data tends to be from domains where sentiment can be determined from freely available metadata without an annotation effort: movie, restaurant, and product reviews. Our experiments reflect this domain mismatch by training on product reviews and using standard text at inference. However, results may differ for in-domain data, and it would be possible to also create a bias evaluation dataset from in-domain data and observe the differences.

As a result of these ablations, this work focuses on two types of causal tests. The evaluation dataset is based on counterfactual pairs, which are causal tests that answer the question not just \textit{what changed} (as an observational study does) but also \textit{why did it change} (a property of an interventional or causal study). This is a different \textit{why} than is answered when we do all of our experiments, which are themselves a different kind of counterfactual. The evaluation data holds everything fixed save the demographic variable, such that any change is attributable to the perturbation of the demographic variable. The many experimental scenarios hold everything fixed save a specific difference in model training, so that difference becomes the variable that can establish causality.
We also leverage the analytical method from the previous work on using a counterfactual confusion matrix to visually inspect patterns of bias. 

The many experiments thus answer whether the observed behaviour came from pretraining or fine tuning as best as possible. There is a limitation, which is that this setup is unable to isolate any interaction effects (which there almost certainly is because pretraining sets inductive biases). It also doesn't answer what about each step caused the change (what segment of data, what hyperparameter). We are unaware of any work that can manage these questions, but we do want to call out that though this work is very rigorous on causal attribution, it is still able to establish causality to only a limited extent and far more research into this area is needed for it to be understood. We released all the models in the hopes that other researchers will do some of this work. 


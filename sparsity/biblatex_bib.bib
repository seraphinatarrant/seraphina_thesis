
@inproceedings{vig_analyzing_2019,
	location = {Florence, Italy},
	title = {Analyzing the Structure of Attention in a Transformer Language Model},
	url = {https://www.aclweb.org/anthology/W19-4808},
	doi = {10.18653/v1/W19-4808},
	abstract = {The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of {NLP} tasks. In this paper, we analyze the structure of attention in a Transformer language model, the {GPT}-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.},
	pages = {63--76},
	booktitle = {Proceedings of the 2019 {ACL} Workshop {BlackboxNLP}: Analyzing and Interpreting Neural Networks for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Vig, Jesse and Belinkov, Yonatan},
	urldate = {2021-01-04},
	date = {2019-08},
}

@article{limisiewicz_syntax_2020,
	title = {Syntax Representation in Word Embeddings and Neural Networks–A Survey},
	journaltitle = {{arXiv} preprint {arXiv}:2010.01063},
	author = {Limisiewicz, Tomasz and Marecek, David},
	date = {2020},
}

@article{geva_transformer_2020,
	title = {Transformer Feed-Forward Layers Are Key-Value Memories},
	url = {http://arxiv.org/abs/2012.14913},
	abstract = {Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.},
	journaltitle = {{arXiv}:2012.14913 [cs]},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	urldate = {2021-01-01},
	date = {2020-12-29},
	eprinttype = {arxiv},
	eprint = {2012.14913},
	keywords = {Computer Science - Computation and Language},
}

@article{celli_no-regret_2020,
	title = {No-Regret Learning Dynamics for Extensive-Form Correlated Equilibrium},
	url = {https://arxiv.org/abs/2004.00603v4},
	abstract = {The existence of simple, uncoupled no-regret dynamics that converge to
correlated equilibria in normal-form games is a celebrated result in the theory
of multi-agent systems. Specifically, it has been known for more than 20 years
that when all players seek to minimize their internal regret in a repeated
normal-form game, the empirical frequency of play converges to a normal-form
correlated equilibrium. Extensive-form (that is, tree-form) games generalize
normal-form games by modeling both sequential and simultaneous moves, as well
as private information. Because of the sequential nature and presence of
partial information in the game, extensive-form correlation has significantly
different properties than the normal-form counterpart, many of which are still
open research directions. Extensive-form correlated equilibrium ({EFCE}) has been
proposed as the natural extensive-form counterpart to normal-form correlated
equilibrium. However, it was currently unknown whether {EFCE} emerges as the
result of uncoupled agent dynamics. In this paper, we give the first uncoupled
no-regret dynamics that converge to the set of {EFCEs} in \$n\$-player general-sum
extensive-form games with perfect recall. First, we introduce a notion of
trigger regret in extensive-form games, which extends that of internal regret
in normal-form games. When each player has low trigger regret, the empirical
frequency of play is close to an {EFCE}. Then, we give an efficient
no-trigger-regret algorithm. Our algorithm decomposes trigger regret into local
subproblems at each decision point for the player, and constructs a global
strategy of the player from the local solutions at each decision point.},
	author = {Celli, Andrea and Marchesi, Alberto and Farina, Gabriele and Gatti, Nicola},
	urldate = {2020-12-31},
	date = {2020-04-01},
	langid = {english},
}

@article{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	url = {https://arxiv.org/abs/2005.14165v4},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current {NLP} systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train {GPT}-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, {GPT}-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. {GPT}-3
achieves strong performance on many {NLP} datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where {GPT}-3's few-shot learning still struggles,
as well as some datasets where {GPT}-3 faces methodological issues related to
training on large web corpora. Finally, we find that {GPT}-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of {GPT}-3 in general.},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2020-12-31},
	date = {2020-05-28},
	langid = {english},
}

@book{quiller-couch_art_1916,
	title = {On the art of writing: Lectures delivered in the University of Cambridge, 1913-1914},
	shorttitle = {On the art of writing},
	publisher = {University Press},
	author = {Quiller-Couch, Arthur Thomas},
	date = {1916},
}

@article{camburu_struggles_2020,
	title = {The Struggles of Feature-Based Explanations: Shapley Values vs. Minimal Sufficient Subsets},
	url = {http://arxiv.org/abs/2009.11023},
	shorttitle = {The Struggles of Feature-Based Explanations},
	abstract = {For neural models to garner widespread public trust and ensure fairness, we must have human-intelligible explanations for their predictions. Recently, an increasing number of works focus on explaining the predictions of neural models in terms of the relevance of the input features. In this work, we show that feature-based explanations pose problems even for explaining trivial models. We show that, in certain cases, there exist at least two ground-truth feature-based explanations, and that, sometimes, neither of them is enough to provide a complete view of the decision-making process of the model. Moreover, we show that two popular classes of explainers, Shapley explainers and minimal sufficient subsets explainers, target fundamentally different types of ground-truth explanations, despite the apparently implicit assumption that explainers should look for one specific feature-based explanation. These findings bring an additional dimension to consider in both developing and choosing explainers.},
	journaltitle = {{arXiv}:2009.11023 [cs]},
	author = {Camburu, Oana-Maria and Giunchiglia, Eleonora and Foerster, Jakob and Lukasiewicz, Thomas and Blunsom, Phil},
	urldate = {2020-12-22},
	date = {2020-12-14},
	eprinttype = {arxiv},
	eprint = {2009.11023},
	keywords = {Computer Science - Computation and Language},
}

@article{agrawal_investigating_2020,
	title = {Investigating Learning in Deep Neural Networks using Layer-Wise Weight Change},
	url = {http://arxiv.org/abs/2011.06735},
	abstract = {Understanding the per-layer learning dynamics of deep neural networks is of significant interest as it may provide insights into how neural networks learn and the potential for better training regimens. We investigate learning in Deep Convolutional Neural Networks ({CNNs}) by measuring the relative weight change of layers while training. Several interesting trends emerge in a variety of {CNN} architectures across various computer vision classification tasks, including the overall increase in relative weight change of later layers as compared to earlier ones.},
	journaltitle = {{arXiv}:2011.06735 [cs]},
	author = {Agrawal, Ayush Manish and Tendle, Atharva and Sikka, Harshvardhan and Singh, Sahib and Kayid, Amr},
	urldate = {2020-12-21},
	date = {2020-11-30},
	eprinttype = {arxiv},
	eprint = {2011.06735},
	keywords = {Computer Science - Machine Learning},
}

@article{natekar_representation_2020,
	title = {Representation Based Complexity Measures for Predicting Generalization in Deep Learning},
	url = {http://arxiv.org/abs/2012.02775},
	abstract = {Deep Neural Networks can generalize despite being significantly overparametrized. Recent research has tried to examine this phenomenon from various view points and to provide bounds on the generalization error or measures predictive of the generalization gap based on these viewpoints, such as norm-based, {PAC}-Bayes based, and margin-based analysis. In this work, we provide an interpretation of generalization from the perspective of quality of internal representations of deep neural networks, based on neuroscientific theories of how the human visual system creates invariant and untangled object representations. Instead of providing theoretical bounds, we demonstrate practical complexity measures which can be computed ad-hoc to uncover generalization behaviour in deep models. We also provide a detailed description of our solution that won the {NeurIPS} competition on Predicting Generalization in Deep Learning held at {NeurIPS} 2020. An implementation of our solution is available at https://github.com/parthnatekar/pgdl.},
	journaltitle = {{arXiv}:2012.02775 [cs]},
	author = {Natekar, Parth and Sharma, Manik},
	urldate = {2020-12-19},
	date = {2020-12-04},
	eprinttype = {arxiv},
	eprint = {2012.02775},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{chang_provable_2020,
	title = {Provable Benefits of Overparameterization in Model Compression: From Double Descent to Pruning Neural Networks},
	url = {http://arxiv.org/abs/2012.08749},
	shorttitle = {Provable Benefits of Overparameterization in Model Compression},
	abstract = {Deep networks are typically trained with many more parameters than the size of the training dataset. Recent empirical evidence indicates that the practice of overparameterization not only benefits training large models, but also assists - perhaps counterintuitively - building lightweight models. Specifically, it suggests that overparameterization benefits model pruning / sparsification. This paper sheds light on these empirical findings by theoretically characterizing the high-dimensional asymptotics of model pruning in the overparameterized regime. The theory presented addresses the following core question: "should one train a small model from the beginning, or first train a large model and then prune?". We analytically identify regimes in which, even if the location of the most informative features is known, we are better off fitting a large model and then pruning rather than simply training with the known informative features. This leads to a new double descent in the training of sparse models: growing the original model, while preserving the target sparsity, improves the test accuracy as one moves beyond the overparameterization threshold. Our analysis further reveals the benefit of retraining by relating it to feature correlations. We find that the above phenomena are already present in linear and random-features models. Our technical approach advances the toolset of high-dimensional analysis and precisely characterizes the asymptotic distribution of over-parameterized least-squares. The intuition gained by analytically studying simpler models is numerically verified on neural networks.},
	journaltitle = {{arXiv}:2012.08749 [cs, stat]},
	author = {Chang, Xiangyu and Li, Yingcong and Oymak, Samet and Thrampoulidis, Christos},
	urldate = {2020-12-19},
	date = {2020-12-16},
	eprinttype = {arxiv},
	eprint = {2012.08749},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{belkin_reconciling_2019,
	title = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
	volume = {116},
	rights = {© 2019 . https://www.pnas.org/site/aboutpnas/licenses.{xhtmlPublished} under the {PNAS} license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/32/15849},
	doi = {10.1073/pnas.1903070116},
	abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
	pages = {15849--15854},
	number = {32},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	urldate = {2020-12-18},
	date = {2019-08-06},
	langid = {english},
	pmid = {31341078},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {bias–variance trade-off, machine learning, neural networks},
}

@article{feldman_does_2019,
	title = {Does Learning Require Memorization? A Short Tale about a Long Tail},
	url = {http://arxiv.org/abs/1906.05271},
	shorttitle = {Does Learning Require Memorization?},
	abstract = {State-of-the-art results on image recognition tasks are achieved using over-parameterized learning algorithms that (nearly) perfectly fit the training set and are known to fit well even random labels. This tendency to memorize the labels of the training data is not explained by existing theoretical analyses. Memorization of the training data also presents significant privacy risks when the training data contains sensitive personal information and thus it is important to understand whether such memorization is necessary for accurate learning. We provide a simple conceptual explanation and a theoretical model demonstrating that for natural data distributions memorization of labels is necessary for achieving close-to-optimal generalization error. The model is motivated and supported by the results of several recent empirical works. In our model, data is sampled from a mixture of subpopulations and the frequencies of these subpopulations are chosen from some prior. The model allows to quantify the effect of not fitting the training data on the generalization performance of the learned classifier and demonstrates that memorization is necessary whenever frequencies are long-tailed. Image and text data are known to follow such distributions and therefore our results establish a formal link between these empirical phenomena. Our results also have concrete implications for the cost of ensuring differential privacy in learning.},
	journaltitle = {{arXiv}:1906.05271 [cs, stat]},
	author = {Feldman, Vitaly},
	urldate = {2020-12-18},
	date = {2019-12-11},
	eprinttype = {arxiv},
	eprint = {1906.05271},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mei_generalization_2020,
	title = {The generalization error of random features regression: Precise asymptotics and double descent curve},
	url = {http://arxiv.org/abs/1908.05355},
	shorttitle = {The generalization error of random features regression},
	abstract = {Deep learning methods operate in regimes that defy the traditional statistical mindset. Neural network architectures often contain more parameters than training samples, and are so rich that they can interpolate the observed labels, even if the latter are replaced by pure noise. Despite their huge complexity, the same architectures achieve small generalization error on real data. This phenomenon has been rationalized in terms of a so-called `double descent' curve. As the model complexity increases, the test error follows the usual U-shaped curve at the beginning, first decreasing and then peaking around the interpolation threshold (when the model achieves vanishing training error). However, it descends again as model complexity exceeds this threshold. The global minimum of the test error is found above the interpolation threshold, often in the extreme overparametrization regime in which the number of parameters is much larger than the number of samples. Far from being a peculiar property of deep neural networks, elements of this behavior have been demonstrated in much simpler settings, including linear regression with random covariates. In this paper we consider the problem of learning an unknown function over the \$d\$-dimensional sphere \${\textbackslash}mathbb S{\textasciicircum}\{d-1\}\$, from \$n\$ i.i.d. samples \$({\textbackslash}boldsymbol x\_i, y\_i){\textbackslash}in {\textbackslash}mathbb S{\textasciicircum}\{d-1\} {\textbackslash}times {\textbackslash}mathbb R\$, \$i{\textbackslash}le n\$. We perform ridge regression on \$N\$ random features of the form \${\textbackslash}sigma({\textbackslash}boldsymbol w\_a{\textasciicircum}\{{\textbackslash}mathsf T\} {\textbackslash}boldsymbol x)\$, \$a{\textbackslash}le N\$. This can be equivalently described as a two-layers neural network with random first-layer weights. We compute the precise asymptotics of the test error, in the limit \$N,n,d{\textbackslash}to {\textbackslash}infty\$ with \$N/d\$ and \$n/d\$ fixed. This provides the first analytically tractable model that captures all the features of the double descent phenomenon without assuming ad hoc misspecification structures.},
	journaltitle = {{arXiv}:1908.05355 [math, stat]},
	author = {Mei, Song and Montanari, Andrea},
	urldate = {2020-12-18},
	date = {2020-12-10},
	eprinttype = {arxiv},
	eprint = {1908.05355},
	keywords = {62J99, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{ben-david_learnability_2019,
	title = {Learnability can be undecidable},
	volume = {1},
	rights = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-018-0002-3},
	doi = {10.1038/s42256-018-0002-3},
	abstract = {The mathematical foundations of machine learning play a key role in the development of the field. They improve our understanding and provide tools for designing new learning paradigms. The advantages of mathematics, however, sometimes come with a cost. Gödel and Cohen showed, in a nutshell, that not everything is provable. Here we show that machine learning shares this fate. We describe simple scenarios where learnability cannot be proved nor refuted using the standard axioms of mathematics. Our proof is based on the fact the continuum hypothesis cannot be proved nor refuted. We show that, in some cases, a solution to the ‘estimating the maximum’ problem is equivalent to the continuum hypothesis. The main idea is to prove an equivalence between learnability and compression.},
	pages = {44--48},
	number = {1},
	journaltitle = {Nature Machine Intelligence},
	author = {Ben-David, Shai and Hrubeš, Pavel and Moran, Shay and Shpilka, Amir and Yehudayoff, Amir},
	urldate = {2020-12-18},
	date = {2019-01},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
}

@article{malach_proving_2020,
	title = {Proving the Lottery Ticket Hypothesis: Pruning is All You Need},
	url = {http://arxiv.org/abs/2002.00585},
	shorttitle = {Proving the Lottery Ticket Hypothesis},
	abstract = {The lottery ticket hypothesis (Frankle and Carbin, 2018), states that a randomly-initialized network contains a small subnetwork such that, when trained in isolation, can compete with the performance of the original network. We prove an even stronger hypothesis (as was also conjectured in Ramanujan et al., 2019), showing that for every bounded distribution and every target network with bounded weights, a sufficiently over-parameterized neural network with random weights contains a subnetwork with roughly the same accuracy as the target network, without any further training.},
	journaltitle = {{arXiv}:2002.00585 [cs, stat]},
	author = {Malach, Eran and Yehudai, Gilad and Shalev-Shwartz, Shai and Shamir, Ohad},
	urldate = {2020-12-18},
	date = {2020-02-03},
	eprinttype = {arxiv},
	eprint = {2002.00585},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{pensia_optimal_2020,
	title = {Optimal Lottery Tickets via {SubsetSum}: Logarithmic Over-Parameterization is Sufficient},
	url = {http://arxiv.org/abs/2006.07990},
	shorttitle = {Optimal Lottery Tickets via {SubsetSum}},
	abstract = {The strong \{{\textbackslash}it lottery ticket hypothesis\} ({LTH}) postulates that one can approximate any target neural network by only pruning the weights of a sufficiently over-parameterized random network. A recent work by Malach et al.{\textasciitilde}{\textbackslash}cite\{{MalachEtAl}20\} establishes the first theoretical analysis for the strong {LTH}: one can provably approximate a neural network of width \$d\$ and depth \$l\$, by pruning a random one that is a factor \$O(d{\textasciicircum}4l{\textasciicircum}2)\$ wider and twice as deep. This polynomial over-parameterization requirement is at odds with recent experimental research that achieves good approximation with networks that are a small factor wider than the target. In this work, we close the gap and offer an exponential improvement to the over-parameterization requirement for the existence of lottery tickets. We show that any target network of width \$d\$ and depth \$l\$ can be approximated by pruning a random network that is a factor \$O({\textbackslash}log(dl))\$ wider and twice as deep. Our analysis heavily relies on connecting pruning random {ReLU} networks to random instances of the {\textbackslash}textsc\{{SubsetSum}\} problem. We then show that this logarithmic over-parameterization is essentially optimal for constant depth networks. Finally, we verify several of our theoretical insights with experiments.},
	journaltitle = {{arXiv}:2006.07990 [cs, math, stat]},
	author = {Pensia, Ankit and Rajput, Shashank and Nagle, Alliot and Vishwakarma, Harit and Papailiopoulos, Dimitris},
	urldate = {2020-12-18},
	date = {2020-06-14},
	eprinttype = {arxiv},
	eprint = {2006.07990},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{vershynin_memory_2020,
	title = {Memory capacity of neural networks with threshold and {ReLU} activations},
	url = {http://arxiv.org/abs/2001.06938},
	abstract = {Overwhelming theoretical and empirical evidence shows that mildly overparametrized neural networks -- those with more connections than the size of the training data -- are often able to memorize the training data with \$100{\textbackslash}\%\$ accuracy. This was rigorously proved for networks with sigmoid activation functions and, very recently, for {ReLU} activations. Addressing a 1988 open question of Baum, we prove that this phenomenon holds for general multilayered perceptrons, i.e. neural networks with threshold activation functions, or with any mix of threshold and {ReLU} activations. Our construction is probabilistic and exploits sparsity.},
	journaltitle = {{arXiv}:2001.06938 [cs, math, stat]},
	author = {Vershynin, Roman},
	urldate = {2020-12-18},
	date = {2020-06-02},
	eprinttype = {arxiv},
	eprint = {2001.06938},
	keywords = {68Q32, 92B20, Computer Science - Information Theory, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{holtzman_curious_2019,
	title = {The Curious Case of Neural Text Degeneration},
	url = {https://openreview.net/forum?id=rygGQyrFvH},
	abstract = {Current language generation systems either aim for high likelihood and devolve into generic repetition or miscalibrate their stochasticity—we provide evidence of both and propose a solution:...},
	eventtitle = {International Conference on Learning Representations},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	urldate = {2020-12-18},
	date = {2019-09-25},
	langid = {english},
}

@article{wilson_bayesian_2020,
	title = {Bayesian Deep Learning and a Probabilistic Perspective of Generalization},
	url = {http://arxiv.org/abs/2002.08791},
	abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
	journaltitle = {{arXiv}:2002.08791 [cs, stat]},
	author = {Wilson, Andrew Gordon and Izmailov, Pavel},
	urldate = {2020-12-18},
	date = {2020-04-27},
	eprinttype = {arxiv},
	eprint = {2002.08791},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{mu_representing_2017,
	location = {Vancouver, Canada},
	title = {Representing Sentences as Low-Rank Subspaces},
	url = {https://www.aclweb.org/anthology/P17-2099},
	doi = {10.18653/v1/P17-2099},
	abstract = {Sentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications. We observe a simple geometry of sentences – the word representations of a given sentence (on average 10.23 words in all {SemEval} datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation, we represent a sentence by the low-rank subspace spanned by its word vectors. Such an unsupervised representation is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models, including skip-thought vectors, by 15\% on average.},
	eventtitle = {{ACL} 2017},
	pages = {629--634},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Mu, Jiaqi and Bhat, Suma and Viswanath, Pramod},
	urldate = {2020-12-18},
	date = {2017-07},
}

@article{postman_five_nodate,
	title = {Five Things We Need to Know About Technological Change},
	pages = {5},
	author = {Postman, Neil},
	langid = {english},
}

@article{elazar_amnesic_2020,
	title = {Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals},
	url = {http://arxiv.org/abs/2006.00995},
	shorttitle = {Amnesic Probing},
	abstract = {A growing body of work makes use of probing in order to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results, and offer an alternative method which focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention which removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, e.g. is part-of-speech information important for word prediction? We perform a series of analyses on {BERT} to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.},
	journaltitle = {{arXiv}:2006.00995 [cs]},
	author = {Elazar, Yanai and Ravfogel, Shauli and Jacovi, Alon and Goldberg, Yoav},
	urldate = {2020-12-16},
	date = {2020-12-05},
	eprinttype = {arxiv},
	eprint = {2006.00995},
	keywords = {Computer Science - Computation and Language},
}

@article{nam_learning_2020,
	title = {Learning from Failure: Training Debiased Classifier from Biased Classifier},
	url = {http://arxiv.org/abs/2007.02561},
	shorttitle = {Learning from Failure},
	abstract = {Neural networks often learn to make predictions that overly rely on spurious correlation existing in the dataset, which causes the model to be biased. While previous work tackles this issue by using explicit labeling on the spuriously correlated attributes or presuming a particular bias type, we instead utilize a cheaper, yet generic form of human knowledge, which can be widely applicable to various types of bias. We first observe that neural networks learn to rely on the spurious correlation only when it is "easier" to learn than the desired knowledge, and such reliance is most prominent during the early phase of training. Based on the observations, we propose a failure-based debiasing scheme by training a pair of neural networks simultaneously. Our main idea is twofold; (a) we intentionally train the first network to be biased by repeatedly amplifying its "prejudice", and (b) we debias the training of the second network by focusing on samples that go against the prejudice of the biased network in (a). Extensive experiments demonstrate that our method significantly improves the training of the network against various types of biases in both synthetic and real-world datasets. Surprisingly, our framework even occasionally outperforms the debiasing methods requiring explicit supervision of the spuriously correlated attributes.},
	journaltitle = {{arXiv}:2007.02561 [cs, stat]},
	author = {Nam, Junhyun and Cha, Hyuntak and Ahn, Sungsoo and Lee, Jaeho and Shin, Jinwoo},
	urldate = {2020-12-16},
	date = {2020-11-23},
	eprinttype = {arxiv},
	eprint = {2007.02561},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{noauthor_neural_nodate,
	title = {A neural network is a monference, not a model},
	url = {http://blog.jacobandreas.net/monference.html},
	urldate = {2020-12-16},
}

@article{murray_differentiation_2016,
	title = {Differentiation of the Cholesky decomposition},
	url = {http://arxiv.org/abs/1602.07527},
	abstract = {We review strategies for differentiating matrix-based computations, and derive symbolic and algorithmic update rules for differentiating expressions containing the Cholesky decomposition. We recommend new `blocked' algorithms, based on differentiating the Cholesky algorithm {DPOTRF} in the {LAPACK} library, which uses `Level 3' matrix-matrix operations from {BLAS}, and so is cache-friendly and easy to parallelize. For large matrices, the resulting algorithms are the fastest way to compute Cholesky derivatives, and are an order of magnitude faster than the algorithms in common usage. In some computing environments, symbolically-derived updates are faster for small matrices than those based on differentiating Cholesky algorithms. The symbolic and algorithmic approaches can be combined to get the best of both worlds.},
	journaltitle = {{arXiv}:1602.07527 [cs, stat]},
	author = {Murray, Iain},
	urldate = {2020-12-10},
	date = {2016-02-24},
	eprinttype = {arxiv},
	eprint = {1602.07527},
	keywords = {Computer Science - Mathematical Software, Statistics - Computation},
}

@article{mu_compositional_2020,
	title = {Compositional Explanations of Neurons},
	url = {http://arxiv.org/abs/2006.14032},
	abstract = {We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference ({NLI}), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while {NLI} neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple "copy-paste" adversarial examples that change model behavior in predictable ways.},
	journaltitle = {{arXiv}:2006.14032 [cs, stat]},
	author = {Mu, Jesse and Andreas, Jacob},
	urldate = {2020-12-10},
	date = {2020-06-24},
	eprinttype = {arxiv},
	eprint = {2006.14032},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{nakkiran_sgd_2019,
	title = {{SGD} on Neural Networks Learns Functions of Increasing Complexity},
	url = {http://arxiv.org/abs/1905.11604},
	abstract = {We perform an experimental study of the dynamics of Stochastic Gradient Descent ({SGD}) in learning deep neural networks for several real and synthetic classification tasks. We show that in the initial epochs, almost all of the performance improvement of the classifier obtained by {SGD} can be explained by a linear classifier. More generally, we give evidence for the hypothesis that, as iterations progress, {SGD} learns functions of increasing complexity. This hypothesis can be helpful in explaining why {SGD}-learned classifiers tend to generalize well even in the over-parameterized regime. We also show that the linear classifier learned in the initial stages is "retained" throughout the execution even if training is continued to the point of zero training error, and complement this with a theoretical result in a simplified model. Key to our work is a new measure of how well one classifier explains the performance of another, based on conditional mutual information.},
	journaltitle = {{arXiv}:1905.11604 [cs, stat]},
	author = {Nakkiran, Preetum and Kaplun, Gal and Kalimeris, Dimitris and Yang, Tristan and Edelman, Benjamin L. and Zhang, Fred and Barak, Boaz},
	urldate = {2020-12-10},
	date = {2019-05-28},
	eprinttype = {arxiv},
	eprint = {1905.11604},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{kunz_classifier_2020,
	location = {Barcelona, Spain (Online)},
	title = {Classifier Probes May Just Learn from Linear Context Features},
	url = {https://www.aclweb.org/anthology/2020.coling-main.450},
	abstract = {Classifiers trained on auxiliary probing tasks are a popular tool to analyze the representations learned by neural sentence encoders such as {BERT} and {ELMo}. While many authors are aware of the difficulty to distinguish between “extracting the linguistic structure encoded in the representations” and “learning the probing task,” the validity of probing methods calls for further research. Using a neighboring word identity prediction task, we show that the token embeddings learned by neural sentence encoders contain a significant amount of information about the exact linear context of the token, and hypothesize that, with such information, learning standard probing tasks may be feasible even without additional linguistic structure. We develop this hypothesis into a framework in which analysis efforts can be scrutinized and argue that, with current models and baselines, conclusions that representations contain linguistic structure are not well-founded. Current probing methodology, such as restricting the classifier's expressiveness or using strong baselines, can help to better estimate the complexity of learning, but not build a foundation for speculations about the nature of the linguistic structure encoded in the learned representations.},
	eventtitle = {{COLING} 2020},
	pages = {5136--5146},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	publisher = {International Committee on Computational Linguistics},
	author = {Kunz, Jenny and Kuhlmann, Marco},
	urldate = {2020-12-09},
	date = {2020-12},
}

@article{domingos_every_2020,
	title = {Every Model Learned by Gradient Descent Is Approximately a Kernel Machine},
	url = {http://arxiv.org/abs/2012.00152},
	abstract = {Deep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep network weights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.},
	journaltitle = {{arXiv}:2012.00152 [cs, stat]},
	author = {Domingos, Pedro},
	urldate = {2020-12-09},
	date = {2020-11-30},
	eprinttype = {arxiv},
	eprint = {2012.00152},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.6, I.5.1, Statistics - Machine Learning},
}

@article{yarotsky_phase_2020,
	title = {The phase diagram of approximation rates for deep neural networks},
	volume = {33},
	url = {https://proceedings.neurips.cc//paper_files/paper/2020/hash/979a3f14bae523dc5101c52120c535e9-Abstract.html},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Yarotsky, Dmitry and Zhevnerchuk, Anton},
	urldate = {2020-12-09},
	date = {2020},
	langid = {english},
}

@article{leavitt_towards_2020,
	title = {Towards falsifiable interpretability research},
	url = {http://arxiv.org/abs/2010.12016},
	abstract = {Methods for understanding the decisions of and mechanisms underlying deep neural networks ({DNNs}) typically rely on building intuition by emphasizing sensory or semantic features of individual examples. For instance, methods aim to visualize the components of an input which are "important" to a network's decision, or to measure the semantic properties of single neurons. Here, we argue that interpretability research suffers from an over-reliance on intuition-based approaches that risk-and in some cases have caused-illusory progress and misleading conclusions. We identify a set of limitations that we argue impede meaningful progress in interpretability research, and examine two popular classes of interpretability methods-saliency and single-neuron-based approaches-that serve as case studies for how overreliance on intuition and lack of falsifiability can undermine interpretability research. To address these concerns, we propose a strategy to address these impediments in the form of a framework for strongly falsifiable interpretability research. We encourage researchers to use their intuitions as a starting point to develop and test clear, falsifiable hypotheses, and hope that our framework yields robust, evidence-based interpretability methods that generate meaningful advances in our understanding of {DNNs}.},
	journaltitle = {{arXiv}:2010.12016 [cs, stat]},
	author = {Leavitt, Matthew L. and Morcos, Ari},
	urldate = {2020-12-09},
	date = {2020-10-22},
	eprinttype = {arxiv},
	eprint = {2010.12016},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{olah_building_2018,
	title = {The Building Blocks of Interpretability},
	volume = {3},
	issn = {2476-0757},
	url = {https://distill.pub/2018/building-blocks},
	doi = {10.23915/distill.00010},
	abstract = {Interpretability techniques are normally studied in isolation. We explore the powerful interfaces that arise when you combine them -- and the rich structure of this combinatorial space.},
	pages = {e10},
	number = {3},
	journaltitle = {Distill},
	shortjournal = {Distill},
	author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
	urldate = {2020-12-09},
	date = {2018-03-06},
	langid = {english},
}

@article{gelman_what_2020,
	title = {What are the most important statistical ideas of the past 50 years?},
	url = {http://arxiv.org/abs/2012.00174},
	abstract = {We argue that the most important statistical ideas of the past half century are: counterfactual causal inference, bootstrapping and simulation-based inference, overparameterized models and regularization, multilevel models, generic computation algorithms, adaptive decision analysis, robust inference, and exploratory data analysis. We discuss common features of these ideas, how they relate to modern computing and big data, and how they might be developed and extended in future decades. The goal of this article is to provoke thought and discussion regarding the larger themes of research in statistics and data science.},
	journaltitle = {{arXiv}:2012.00174 [stat]},
	author = {Gelman, Andrew and Vehtari, Aki},
	urldate = {2020-12-08},
	date = {2020-11-30},
	eprinttype = {arxiv},
	eprint = {2012.00174},
	keywords = {Statistics - Methodology},
}

@article{wu_when_2020,
	title = {When Do Curricula Work?},
	url = {http://arxiv.org/abs/2012.03107},
	abstract = {Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the {\textbackslash}emph\{implicit curricula\} resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of {\textbackslash}emph\{explicit curricula\}, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum can indeed improve the performance either with limited training time budget or in existence of noisy data.},
	journaltitle = {{arXiv}:2012.03107 [cs, eess, stat]},
	author = {Wu, Xiaoxia and Dyer, Ethan and Neyshabur, Behnam},
	urldate = {2020-12-08},
	date = {2020-12-05},
	eprinttype = {arxiv},
	eprint = {2012.03107},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}

@article{ghorbani_neuron_2020,
	title = {Neuron Shapley: Discovering the Responsible Neurons},
	url = {http://arxiv.org/abs/2002.09815},
	shorttitle = {Neuron Shapley},
	abstract = {We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on {ImageNet}. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Enabling all these applications is a new multi-arm bandit algorithm that we developed to efficiently estimate Neuron Shapley values.},
	journaltitle = {{arXiv}:2002.09815 [cs, stat]},
	author = {Ghorbani, Amirata and Zou, James},
	urldate = {2020-12-08},
	date = {2020-11-13},
	eprinttype = {arxiv},
	eprint = {2002.09815},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}
@article{chen_lottery_2020,
	title = {The Lottery Ticket Hypothesis for Pre-trained {BERT} Networks},
	volume = {33},
	url = {https://proceedings.neurips.cc//paper_files/paper/2020/hash/b6af2c9703f203a2794be03d443af2e3-Abstract.html},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
	urldate = {2020-12-08},
	date = {2020},
	langid = {english},
}

@online{noauthor_neurips_nodate,
	title = {{NeurIPS} 2020 : The Lottery Ticket Hypothesis for Pre-trained {BERT} Networks},
	url = {https://neurips.cc/virtual/2020/public/poster_b6af2c9703f203a2794be03d443af2e3.html},
	urldate = {2020-12-08},
}

@inproceedings{hall_maudslay_tale_2020,
	location = {Online},
	title = {A Tale of a Probe and a Parser},
	url = {https://www.aclweb.org/anthology/2020.acl-main.659},
	doi = {10.18653/v1/2020.acl-main.659},
	abstract = {Measuring what linguistic information is encoded in neural models of language has become popular in {NLP}. Researchers approach this enterprise by training “probes”—supervised models designed to extract linguistic structure from another model's output. One such probe is the structural probe (Hewitt and Manning, 2019), designed to quantify the extent to which syntactic information is encoded in contextualised word representations. The structural probe has a novel design, unattested in the parsing literature, the precise benefit of which is not immediately obvious. To explore whether syntactic probes would do better to make use of existing techniques, we compare the structural probe to a more traditional parser with an identical lightweight parameterisation. The parser outperforms structural probe on {UUAS} in seven of nine analysed languages, often by a substantial amount (e.g. by 11.1 points in English). Under a second less common metric, however, there is the opposite trend—the structural probe outperforms the parser. This begs the question: which metric should we prefer?},
	eventtitle = {{ACL} 2020},
	pages = {7389--7395},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Hall Maudslay, Rowan and Valvoda, Josef and Pimentel, Tiago and Williams, Adina and Cotterell, Ryan},
	urldate = {2020-12-07},
	date = {2020-07},
}

@article{saunshi_mathematical_2020,
	title = {A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks},
	url = {http://arxiv.org/abs/2010.03648},
	abstract = {Autoregressive language models pretrained on large corpora have been successful at solving downstream tasks, even with zero-shot usage. However, there is little theoretical justification for their success. This paper considers the following questions: (1) Why should learning the distribution of natural language help with downstream classification tasks? (2) Why do features learned using language modeling help solve downstream tasks with linear classifiers? For (1), we hypothesize, and verify empirically, that classification tasks of interest can be reformulated as next word prediction tasks, thus making language modeling a meaningful pretraining task. For (2), we analyze properties of the cross-entropy objective to show that \${\textbackslash}epsilon\$-optimal language models in cross-entropy (log-perplexity) learn features that are \${\textbackslash}mathcal\{O\}({\textbackslash}sqrt\{{\textbackslash}epsilon\})\$-good on natural linear classification tasks, thus demonstrating mathematically that doing well on language modeling can be beneficial for downstream tasks. We perform experiments to verify assumptions and validate theoretical results. Our theoretical insights motivate a simple alternative to the cross-entropy objective that performs well on some linear classification tasks.},
	journaltitle = {{arXiv}:2010.03648 [cs, stat]},
	author = {Saunshi, Nikunj and Malladi, Sadhika and Arora, Sanjeev},
	urldate = {2020-12-07},
	date = {2020-10-07},
	eprinttype = {arxiv},
	eprint = {2010.03648},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{frankle_early_2020,
	title = {The Early Phase of Neural Network Training},
	url = {http://arxiv.org/abs/2002.10365},
	abstract = {Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here, we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.},
	journaltitle = {{arXiv}:2002.10365 [cs, stat]},
	author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
	urldate = {2020-12-06},
	date = {2020-02-24},
	eprinttype = {arxiv},
	eprint = {2002.10365},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{lepori_picking_2020,
	title = {Picking {BERT}'s Brain: Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis},
	url = {http://arxiv.org/abs/2011.12073},
	shorttitle = {Picking {BERT}'s Brain},
	abstract = {As the name implies, contextualized representations of language are typically motivated by their ability to encode context. Which aspects of context are captured by such representations? We introduce an approach to address this question using Representational Similarity Analysis ({RSA}). As case studies, we investigate the degree to which a verb embedding encodes the verb's subject, a pronoun embedding encodes the pronoun's antecedent, and a full-sentence representation encodes the sentence's head word (as determined by a dependency parse). In all cases, we show that {BERT}'s contextualized embeddings reflect the linguistic dependency being studied, and that {BERT} encodes these dependencies to a greater degree than it encodes less linguistically-salient controls. These results demonstrate the ability of our approach to adjudicate between hypotheses about which aspects of context are encoded in representations of language.},
	journaltitle = {{arXiv}:2011.12073 [cs]},
	author = {Lepori, Michael A. and {McCoy}, R. Thomas},
	urldate = {2020-12-05},
	date = {2020-11-24},
	eprinttype = {arxiv},
	eprint = {2011.12073},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{hao_investigating_2020,
	location = {Suzhou, China},
	title = {Investigating Learning Dynamics of {BERT} Fine-Tuning},
	url = {https://www.aclweb.org/anthology/2020.aacl-main.11},
	abstract = {The recently introduced pre-trained language model {BERT} advances the state-of-the-art on many {NLP} tasks through the fine-tuning approach, but few studies investigate how the fine-tuning process improves the model performance on downstream tasks. In this paper, we inspect the learning dynamics of {BERT} fine-tuning with two indicators. We use {JS} divergence to detect the change of the attention mode and use {SVCCA} distance to examine the change to the feature extraction mode during {BERT} fine-tuning. We conclude that {BERT} fine-tuning mainly changes the attention mode of the last layers and modifies the feature extraction mode of the intermediate and last layers. Moreover, we analyze the consistency of {BERT} fine-tuning between different random seeds and different datasets. In summary, we provide a distinctive understanding of the learning dynamics of {BERT} fine-tuning, which sheds some light on improving the fine-tuning results.},
	eventtitle = {{AACL} 2020},
	pages = {87--92},
	booktitle = {Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
	urldate = {2020-12-05},
	date = {2020-12},
}

@article{bartoldson_generalization-stability_2020,
	title = {The Generalization-Stability Tradeoff In Neural Network Pruning},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/ef2ee09ea9551de88bc11fd7eeea93b0-Abstract.html},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Bartoldson, Brian and Morcos, Ari and Barbu, Adrian and Erlebacher, Gordon},
	urldate = {2020-12-04},
	date = {2020},
	langid = {english},
}

@inproceedings{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	url = {https://neurips.cc/virtual/2020/public/poster_1457c0d6bfcb4967418bfb8ac142f64a.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora.},
	eventtitle = {Neural Information Processing Systems Online Conference 2020},
	booktitle = {Proceedings of {NeurIPS} 2020},
	author = {Brown, Tom B. and Mann, Ben and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen M. and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2020-12-03},
	date = {2020-12},
	langid = {english},
}

@thesis{hupkes_hierarchy_nodate,
	title = {Hierarchy and interpretability in neural models of language processing},
	url = {https://dieuwkehupkes.nl/},
	type = {phdthesis},
	author = {Hupkes, Dieuwke},
	urldate = {2020-12-03},
	langid = {english},
}

@article{mu_compositional_2020,
	title = {Compositional Explanations of Neurons},
	url = {http://arxiv.org/abs/2006.14032},
	abstract = {We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference ({NLI}), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while {NLI} neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple "copy-paste" adversarial examples that change model behavior in predictable ways.},
	journaltitle = {{arXiv}:2006.14032 [cs, stat]},
	author = {Mu, Jesse and Andreas, Jacob},
	urldate = {2020-12-03},
	date = {2020-06-24},
	eprinttype = {arxiv},
	eprint = {2006.14032},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wang_sparsity_2020,
	location = {Online},
	title = {On the Sparsity of Neural Machine Translation Models},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.78},
	abstract = {Modern neural machine translation ({NMT}) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and {NMT} architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 {BLEU} points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.},
	eventtitle = {{EMNLP} 2020},
	pages = {1060--1066},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Yong and Wang, Longyue and Li, Victor and Tu, Zhaopeng},
	urldate = {2020-12-01},
	date = {2020-11},
}

@inproceedings{liu_understanding_2020,
	location = {Online},
	title = {Understanding the Difficulty of Training Transformers},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.463},
	abstract = {Transformers have proved effective in many {NLP} tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional {SGD} fails to train Transformers effectively). Our objective here is to understand \_\_what complicates Transformer training\_\_ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially—for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage's training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance},
	eventtitle = {{EMNLP} 2020},
	pages = {5747--5763},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
	urldate = {2020-12-01},
	date = {2020-11},
}

@online{noauthor_rethinking_nodate,
	title = {Rethinking Attention with Performers},
	url = {http://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html},
	abstract = {Posted by Krzysztof Choromanski and Lucy Colwell, Research Scientists, Google Research    Transformer models  have achieved state-of-the-art...},
	titleaddon = {Google {AI} Blog},
	urldate = {2020-12-01},
	langid = {english},
}

@article{ribeiro_beyond_2020,
	title = {Beyond exploding and vanishing gradients: analysing {RNN} training using attractors and smoothness},
	url = {http://arxiv.org/abs/1906.08482},
	shorttitle = {Beyond exploding and vanishing gradients},
	abstract = {The exploding and vanishing gradient problem has been the major conceptual principle behind most architecture and training improvements in recurrent neural networks ({RNNs}) during the last decade. In this paper, we argue that this principle, while powerful, might need some refinement to explain recent developments. We refine the concept of exploding gradients by reformulating the problem in terms of the cost function smoothness, which gives insight into higher-order derivatives and the existence of regions with many close local minima. We also clarify the distinction between vanishing gradients and the need for the {RNN} to learn attractors to fully use its expressive power. Through the lens of these refinements, we shed new light on recent developments in the {RNN} field, namely stable {RNN} and unitary (or orthogonal) {RNNs}.},
	journaltitle = {{arXiv}:1906.08482 [cs, math, stat]},
	author = {Ribeiro, Antônio H. and Tiels, Koen and Aguirre, Luis A. and Schön, Thomas B.},
	urldate = {2020-11-29},
	date = {2020-03-05},
	eprinttype = {arxiv},
	eprint = {1906.08482},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Dynamical Systems, Statistics - Machine Learning},
}

@article{pezeshki_gradient_2020,
	title = {Gradient Starvation: A Learning Proclivity in Neural Networks},
	url = {http://arxiv.org/abs/2011.09468},
	shorttitle = {Gradient Starvation},
	abstract = {We identify and formalize a fundamental gradient descent phenomenon resulting in a learning proclivity in over-parameterized neural networks. Gradient Starvation arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. This work provides a theoretical explanation for the emergence of such feature imbalance in neural networks. Using tools from Dynamical Systems theory, we identify simple properties of learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure in training data. Based on our proposed formalism, we develop guarantees for a novel regularization method aimed at decoupling feature learning dynamics, improving accuracy and robustness in cases hindered by gradient starvation. We illustrate our findings with simple and real-world out-of-distribution ({OOD}) generalization experiments.},
	journaltitle = {{arXiv}:2011.09468 [cs, math, stat]},
	author = {Pezeshki, Mohammad and Kaba, Sékou-Oumar and Bengio, Yoshua and Courville, Aaron and Precup, Doina and Lajoie, Guillaume},
	urldate = {2020-11-29},
	date = {2020-11-23},
	eprinttype = {arxiv},
	eprint = {2011.09468},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Statistics - Machine Learning},
}

@article{henighan_scaling_2020,
	title = {Scaling Laws for Autoregressive Generative Modeling},
	url = {http://arxiv.org/abs/2010.14701},
	abstract = {We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image\${\textbackslash}leftrightarrow\$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as \$S(\$True\$) + D\_\{{\textbackslash}mathrm\{{KL}\}\}(\$True\${\textbar}{\textbar}\$Model\$)\$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the {KL} divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the {YFCC}100M image distribution downsampled to an \$8{\textbackslash}times 8\$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie \$D\_\{{\textbackslash}mathrm\{{KL}\}\}\$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question "Is a picture worth a thousand words?"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for {ImageNet} classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.},
	journaltitle = {{arXiv}:2010.14701 [cs]},
	author = {Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B. and Dhariwal, Prafulla and Gray, Scott and Hallacy, Chris and Mann, Benjamin and Radford, Alec and Ramesh, Aditya and Ryder, Nick and Ziegler, Daniel M. and Schulman, John and Amodei, Dario and {McCandlish}, Sam},
	urldate = {2020-11-28},
	date = {2020-11-05},
	eprinttype = {arxiv},
	eprint = {2010.14701},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{lewkowycz_large_2020,
	title = {The large learning rate phase of deep learning: the catapult mechanism},
	url = {http://arxiv.org/abs/2003.02218},
	shorttitle = {The large learning rate phase of deep learning},
	abstract = {The choice of initial learning rate can have a profound effect on the performance of deep networks. We present a class of neural networks with solvable training dynamics, and confirm their predictions empirically in practical deep learning settings. The networks exhibit sharply distinct behaviors at small and large learning rates. The two regimes are separated by a phase transition. In the small learning rate phase, training can be understood using the existing theory of infinitely wide neural networks. At large learning rates the model captures qualitatively distinct phenomena, including the convergence of gradient descent dynamics to flatter minima. One key prediction of our model is a narrow range of large, stable learning rates. We find good agreement between our model's predictions and training dynamics in realistic deep learning settings. Furthermore, we find that the optimal performance in such settings is often found in the large learning rate phase. We believe our results shed light on characteristics of models trained at different learning rates. In particular, they fill a gap between existing wide neural network theory, and the nonlinear, large learning rate, training dynamics relevant to practice.},
	journaltitle = {{arXiv}:2003.02218 [cs, stat]},
	author = {Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
	urldate = {2020-11-25},
	date = {2020-03-04},
	eprinttype = {arxiv},
	eprint = {2003.02218},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kaplan_scaling_2020,
	title = {Scaling Laws for Neural Language Models},
	url = {http://arxiv.org/abs/2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	journaltitle = {{arXiv}:2001.08361 [cs, stat]},
	author = {Kaplan, Jared and {McCandlish}, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	urldate = {2020-11-25},
	date = {2020-01-22},
	eprinttype = {arxiv},
	eprint = {2001.08361},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ethayarajh_utility_2020,
	title = {Utility is in the Eye of the User: A Critique of {NLP} Leaderboards},
	url = {http://arxiv.org/abs/2009.13888},
	shorttitle = {Utility is in the Eye of the User},
	abstract = {Benchmarks such as {GLUE} have helped drive advances in {NLP} by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the {NLP} community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and {NLP} practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards -- in their current form -- can be poor proxies for the {NLP} community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model's utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).},
	journaltitle = {{arXiv}:2009.13888 [cs]},
	author = {Ethayarajh, Kawin and Jurafsky, Dan},
	urldate = {2020-11-25},
	date = {2020-10-14},
	eprinttype = {arxiv},
	eprint = {2009.13888},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{papadimitriou_learning_2020,
	location = {Online},
	title = {Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.554},
	shorttitle = {Learning Music Helps You Read},
	abstract = {We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train {LSTMs} on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that {LSTMs} can use for natural language. We find that training on non-linguistic data with latent structure ({MIDI} music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.},
	eventtitle = {{EMNLP} 2020},
	pages = {6829--6839},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Papadimitriou, Isabel and Jurafsky, Dan},
	urldate = {2020-11-24},
	date = {2020-11},
}

@article{pezeshki_gradient_2020-1,
	title = {Gradient Starvation: A Learning Proclivity in Neural Networks},
	url = {http://arxiv.org/abs/2011.09468},
	shorttitle = {Gradient Starvation},
	abstract = {We identify and formalize a fundamental gradient descent phenomenon resulting in a learning proclivity in over-parameterized neural networks. Gradient Starvation arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. This work provides a theoretical explanation for the emergence of such feature imbalance in neural networks. Using tools from Dynamical Systems theory, we identify simple properties of learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure in training data. Based on our proposed formalism, we develop guarantees for a novel regularization method aimed at decoupling feature learning dynamics, improving accuracy and robustness in cases hindered by gradient starvation. We illustrate our findings with simple and real-world out-of-distribution ({OOD}) generalization experiments.},
	journaltitle = {{arXiv}:2011.09468 [cs, math, stat]},
	author = {Pezeshki, Mohammad and Kaba, Sékou-Oumar and Bengio, Yoshua and Courville, Aaron and Precup, Doina and Lajoie, Guillaume},
	urldate = {2020-11-24},
	date = {2020-11-23},
	eprinttype = {arxiv},
	eprint = {2011.09468},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Statistics - Machine Learning},
}

@article{nguyen_word_nodate,
	title = {Do Word Embeddings Capture Spelling Variation?},
	abstract = {Analyses of word embeddings have primarily focused on semantic and syntactic properties. However, word embeddings have the potential to encode other properties as well. In this paper, we propose a new perspective on the analysis of word embeddings by focusing on spelling variation. In social media, spelling variation is abundant and often socially meaningful. Here, we analyze word embeddings trained on Twitter and Reddit data. We present three analyses using pairs of word forms covering seven types of spelling variation in English. Taken together, our results show that word embeddings encode spelling variation patterns of various types to some extent, even embeddings trained using the skipgram model which does not take spelling into account. Our results also suggest a link between the intentionality of the variation and the distance of the non-conventional spellings to their conventional spellings.},
	pages = {12},
	author = {Nguyen, Dong and Grieve, Jack},
	langid = {english},
}

@inproceedings{chrupala_symbolic_2019,
	location = {Florence, Italy},
	title = {Symbolic Inductive Bias for Visually Grounded Learning of Spoken Language},
	url = {https://www.aclweb.org/anthology/P19-1647},
	doi = {10.18653/v1/P19-1647},
	abstract = {A widespread approach to processing spoken language is to first automatically transcribe it into text. An alternative is to use an end-to-end approach: recent works have proposed to learn semantic embeddings of spoken language from images with spoken captions, without an intermediate transcription step. We propose to use multitask learning to exploit existing transcribed speech within the end-to-end setting. We describe a three-task architecture which combines the objectives of matching spoken captions with corresponding images, speech with text, and text with images. We show that the addition of the speech/text task leads to substantial performance improvements on image retrieval when compared to training the speech/image task in isolation. We conjecture that this is due to a strong inductive bias transcribed speech provides to the model, and offer supporting evidence for this.},
	eventtitle = {{ACL} 2019},
	pages = {6452--6462},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Chrupa\{{\textbackslash}textbackslash\}la, Grzegorz},
	urldate = {2020-11-23},
	date = {2019-07},
}

@inproceedings{chrupala_correlating_2019,
	location = {Florence, Italy},
	title = {Correlating Neural and Symbolic Representations of Language},
	url = {https://www.aclweb.org/anthology/P19-1283},
	doi = {10.18653/v1/P19-1283},
	abstract = {Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in {NLP}. Here we present two methods based on Representational Similarity Analysis ({RSA}) and Tree Kernels ({TK}) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees. We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics, and show that they exhibit the expected pattern of results. We then our methods to correlate neural representations of English sentences with their constituency parse trees.},
	eventtitle = {{ACL} 2019},
	pages = {2952--2962},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Chrupa\{{\textbackslash}textbackslash\}la, Grzegorz and Alishahi, Afra},
	urldate = {2020-11-23},
	date = {2019-07},
}

@article{kriegeskorte_representational_2008,
	title = {Representational similarity analysis - connecting the branches of systems neuroscience},
	volume = {2},
	issn = {1662-5137},
	url = {https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full},
	doi = {10.3389/neuro.06.004.2008},
	abstract = {A fundamental challenge for systems neuroscience is to quantitatively relate its three major branches of research: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g. single-cell recordings or voxels from functional magnetic resonance imaging ({fMRI}). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement, and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices, which characterize the information carried by a given representation in a brain or model. We propose a new experimental and data-analytical framework called representational similarity analysis ({RSA}), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing representational dissimilarity matrices. We demonstrate {RSA} by relating representations of visual objects as measured with {fMRI} to computational models spanning a wide range of complexities. We argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.},
	journaltitle = {Frontiers in Systems Neuroscience},
	shortjournal = {Front. Syst. Neurosci.},
	author = {Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter A.},
	urldate = {2020-11-23},
	date = {2008},
	note = {Publisher: Frontiers},
	keywords = {Electrophysiology, Similarity, computational modeling, {fMRI}, population code, representation},
}

@article{feder_causalm_2020,
	title = {{CausaLM}: Causal Model Explanation Through Counterfactual Language Models},
	url = {http://arxiv.org/abs/2005.13407},
	shorttitle = {{CausaLM}},
	abstract = {Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all {ML}-based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose {CausaLM}, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as {BERT} can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.},
	journaltitle = {{arXiv}:2005.13407 [cs]},
	author = {Feder, Amir and Oved, Nadav and Shalit, Uri and Reichart, Roi},
	urldate = {2020-11-23},
	date = {2020-06-14},
	eprinttype = {arxiv},
	eprint = {2005.13407},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{linzen_how_2020,
	title = {How Can We Accelerate Progress Towards Human-like Linguistic Generalization?},
	doi = {10.18653/v1/2020.acl-main.465},
	abstract = {This position paper describes and critiques the Pretraining-Agnostic Identically Distributed ({PAID}) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing {PAID} with paradigms that reward architectures that generalize as quickly and robustly as humans.},
	journaltitle = {{ACL}},
	author = {Linzen, Tal},
	date = {2020},
}

@article{hu_surprising_2020,
	title = {The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks},
	abstract = {Modern neural networks are often regarded as complex black-box functions whose behavior is difficult to understand owing to their nonlinear dependence on the data and the nonconvexity in their loss landscapes. In this work, we show that these common perceptions can be completely false in the early phase of learning. In particular, we formally prove that, for a class of well-behaved input distributions, the early-time learning dynamics of a two-layer fully-connected neural network can be mimicked by training a simple linear model on the inputs. We additionally argue that this surprising simplicity can persist in networks with more layers and with convolutional architecture, which we verify empirically. Key to our analysis is to bound the spectral norm of the difference between the Neural Tangent Kernel ({NTK}) at initialization and an affine transform of the data kernel; however, unlike many previous results utilizing the {NTK}, we do not require the network to have disproportionately large width, and the network is allowed to escape the kernel regime later in training.},
	journaltitle = {{ArXiv}},
	author = {Hu, Wei and Xiao, L. and Adlam, Ben and Pennington, Jeffrey},
	date = {2020},
}

@article{merchant_what_2020,
	title = {What Happens To {BERT} Embeddings During Fine-tuning?},
	url = {http://arxiv.org/abs/2004.14448},
	abstract = {While there has been much recent work studying how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis techniques (probing classifiers, Representational Similarity Analysis, and model ablations), we investigate how fine-tuning affects the representations of the {BERT} model. We find that while fine-tuning necessarily makes significant changes, it does not lead to catastrophic forgetting of linguistic phenomena. We instead find that fine-tuning primarily affects the top layers of {BERT}, but with noteworthy variation across tasks. In particular, dependency parsing reconfigures most of the model, whereas {SQuAD} and {MNLI} appear to involve much shallower processing. Finally, we also find that fine-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.},
	journaltitle = {{arXiv}:2004.14448 [cs]},
	author = {Merchant, Amil and Rahimtoroghi, Elahe and Pavlick, Ellie and Tenney, Ian},
	urldate = {2020-11-20},
	date = {2020-04-29},
	eprinttype = {arxiv},
	eprint = {2004.14448},
	keywords = {Computer Science - Computation and Language},
}

@article{liu_impact_2020,
	title = {Impact of Accuracy on Model Interpretations},
	url = {http://arxiv.org/abs/2011.09903},
	abstract = {Model interpretations are often used in practice to extract real world insights from machine learning models. These interpretations have a wide range of applications; they can be presented as business recommendations or used to evaluate model bias. It is vital for a data scientist to choose trustworthy interpretations to drive real world impact. Doing so requires an understanding of how the accuracy of a model impacts the quality of standard interpretation tools. In this paper, we will explore how a model's predictive accuracy affects interpretation quality. We propose two metrics to quantify the quality of an interpretation and design an experiment to test how these metrics vary with model accuracy. We find that for datasets that can be modeled accurately by a variety of methods, simpler methods yield higher quality interpretations. We also identify which interpretation method works the best for lower levels of model accuracy.},
	journaltitle = {{arXiv}:2011.09903 [cs]},
	author = {Liu, Brian and Udell, Madeleine},
	urldate = {2020-11-20},
	date = {2020-11-17},
	eprinttype = {arxiv},
	eprint = {2011.09903},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{voita_analyzing_2020,
	title = {Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation},
	url = {http://arxiv.org/abs/2010.10907},
	abstract = {In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of {NMT} models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation ({LRP}). Its underlying 'conservation principle' makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token's influence. We extend {LRP} to the Transformer and conduct an analysis of {NMT} models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.},
	journaltitle = {{arXiv}:2010.10907 [cs]},
	author = {Voita, Elena and Sennrich, Rico and Titov, Ivan},
	urldate = {2020-11-20},
	date = {2020-10-21},
	eprinttype = {arxiv},
	eprint = {2010.10907},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{mrini_rethinking_2020,
	location = {Online},
	title = {Rethinking Self-Attention: Towards Interpretability in Neural Parsing},
	url = {https://www.aclweb.org/anthology/2020.findings-emnlp.65},
	shorttitle = {Rethinking Self-Attention},
	abstract = {Attention mechanisms have improved the performance of {NLP} tasks while allowing models to remain explainable. Self-attention is currently widely used, however interpretability is difficult due to the numerous attention distributions. Recent work has shown that model representations can benefit from label-specific information, while facilitating interpretation of predictions. We introduce the Label Attention Layer: a new form of self-attention where attention heads represent labels. We test our novel layer by running constituency and dependency parsing experiments and show our new model obtains new state-of-the-art results for both tasks on both the Penn Treebank ({PTB}) and Chinese Treebank. Additionally, our model requires fewer self-attention layers compared to existing work. Finally, we find that the Label Attention heads learn relations between syntactic categories and show pathways to analyze errors.},
	eventtitle = {{EMNLP}-Findings 2020},
	pages = {731--742},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Mrini, Khalil and Dernoncourt, Franck and Tran, Quan Hung and Bui, Trung and Chang, Walter and Nakashole, Ndapa},
	urldate = {2020-11-20},
	date = {2020-11},
}

@inproceedings{ebrahimi_how_2020,
	location = {Online},
	title = {How Can Self-Attention Networks Recognize Dyck-n Languages?},
	url = {https://www.aclweb.org/anthology/2020.findings-emnlp.384},
	abstract = {We focus on the recognition of Dyck-n (Dn) languages with self-attention ({SA}) networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of {SA}, one with a starting symbol ({SA}+) and one without ({SA}-). Our results show that {SA}+ is able to generalize to longer sequences and deeper dependencies. For D2, we find that {SA}- completely breaks down on long sequences whereas the accuracy of {SA}+ is 58.82\%. We find attention maps learned by {SA}+ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of {SA} networks is at par with {LSTMs}, which provides evidence on the ability of {SA} to learn hierarchies without recursion.},
	eventtitle = {{EMNLP}-Findings 2020},
	pages = {4301--4306},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Ebrahimi, Javid and Gelda, Dhruv and Zhang, Wei},
	urldate = {2020-11-20},
	date = {2020-11},
}

@inproceedings{eisape_cloze_2020,
	location = {Online},
	title = {Cloze Distillation Improves Psychometric Predictive Power},
	url = {https://www.aclweb.org/anthology/2020.conll-1.49},
	abstract = {Contemporary autoregressive language models ({LMs}) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human next-word predictions. Here we evaluate several state-of-the-art language models for their match to human next-word predictions and to reading time behavior from eye movements. We then propose a novel method for distilling the linguistic information implicit in human linguistic predictions into pre-trained {LMs}: Cloze Distillation. We apply this method to a baseline neural {LM} and show potential improvement in reading time prediction and generalization to held-out human cloze data.},
	eventtitle = {{CoNLL} 2020},
	pages = {609--619},
	booktitle = {Proceedings of the 24th Conference on Computational Natural Language Learning},
	publisher = {Association for Computational Linguistics},
	author = {Eisape, Tiwalayo and Zaslavsky, Noga and Levy, Roger},
	urldate = {2020-11-18},
	date = {2020-11},
}

@inproceedings{yu_word_2020,
	location = {Online},
	title = {Word Frequency Does Not Predict Grammatical Knowledge in Language Models},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.331},
	abstract = {Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models' accuracy. Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models. Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun's performance on grammatical tasks. Finally, we find that a novel noun's grammatical properties can be few-shot learned from various types of training data. The results present a paradox: there should be less variation in grammatical performance than is actually observed.},
	eventtitle = {{EMNLP} 2020},
	pages = {4040--4054},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Charles and Sie, Ryan and Tedeschi, Nicolas and Bergen, Leon},
	urldate = {2020-11-17},
	date = {2020-11},
}

@article{rogers_primer_2020,
	title = {A Primer in {BERTology}: What we know about how {BERT} works},
	url = {http://arxiv.org/abs/2002.12327},
	shorttitle = {A Primer in {BERTology}},
	abstract = {Transformer-based models have pushed state of the art in many areas of {NLP}, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular {BERT} model. We review the current state of knowledge about how {BERT} works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
	journaltitle = {{arXiv}:2002.12327 [cs]},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	urldate = {2020-11-14},
	date = {2020-11-09},
	eprinttype = {arxiv},
	eprint = {2002.12327},
	keywords = {Computer Science - Computation and Language},
}
@inproceedings{dalvi_analyzing_2020,
	location = {Online},
	title = {Analyzing Redundancy in Pretrained Transformer Models},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.398},
	abstract = {Transformer-based deep {NLP} models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes: General Redundancy and Task-specific Redundancy. We dissect two popular pretrained models, {BERT} and {XLNet}, studying how much redundancy they exhibit at a representation-level and at a more fine-grained neuron-level. Our analysis reveals interesting insights, such as i) 85\% of the neurons across the network are redundant and ii) at least 92\% of them can be removed when optimizing towards a downstream task. Based on our analysis, we present an efficient feature-based transfer learning procedure, which maintains 97\% performance while using at-most 10\% of the original neurons.},
	eventtitle = {{EMNLP} 2020},
	pages = {4908--4926},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Dalvi, Fahim and Sajjad, Hassan and Durrani, Nadir and Belinkov, Yonatan},
	urldate = {2020-11-14},
	date = {2020-11},
}

@article{bhattamishra_practical_2020,
	title = {On the Practical Ability of Recurrent Neural Networks to Recognize Hierarchical Languages},
	url = {http://arxiv.org/abs/2011.03965},
	abstract = {While recurrent models have been effective in {NLP} tasks, their performance on context-free languages ({CFLs}) has been found to be quite weak. Given that {CFLs} are believed to capture important phenomena such as hierarchical structure in natural languages, this discrepancy in performance calls for an explanation. We study the performance of recurrent models on Dyck-n languages, a particularly important and well-studied class of {CFLs}. We find that while recurrent models generalize nearly perfectly if the lengths of the training and test strings are from the same range, they perform poorly if the test strings are longer. At the same time, we observe that recurrent models are expressive enough to recognize Dyck words of arbitrary lengths in finite precision if their depths are bounded. Hence, we evaluate our models on samples generated from Dyck languages with bounded depth and find that they are indeed able to generalize to much higher lengths. Since natural language datasets have nested dependencies of bounded depth, this may help explain why they perform well in modeling hierarchical dependencies in natural language data despite prior works indicating poor generalization performance on Dyck languages. We perform probing studies to support our results and provide comparisons with Transformers.},
	journaltitle = {{arXiv}:2011.03965 [cs]},
	author = {Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
	urldate = {2020-11-14},
	date = {2020-11-08},
	eprinttype = {arxiv},
	eprint = {2011.03965},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{warstadt_learning_2020,
	location = {Online},
	title = {Learning Which Features Matter: {RoBERTa} Acquires a Preference for Linguistic Generalizations (Eventually)},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.16},
	shorttitle = {Learning Which Features Matter},
	abstract = {One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called {MSGS} (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning. We pretrain {RoBERTa} from scratch on quantities of data ranging from 1M to 1B words and compare their performance on {MSGS} to the publicly available {RoBERTa}\_BASE. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, {RoBERTa}\_BASE does consistently demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.},
	eventtitle = {{EMNLP} 2020},
	pages = {217--235},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Warstadt, Alex and Zhang, Yian and Li, Xiaocheng and Liu, Haokun and Bowman, Samuel R.},
	urldate = {2020-11-14},
	date = {2020-11},
}

@inproceedings{adlam_understanding_2020,
	title = {Understanding Double Descent Requires a Fine-Grained Bias-Variance Decomposition},
	abstract = {Classical learning theory suggests that the optimal generalization performance of a machine learning model should occur at an intermediate model complexity, with simpler models exhibiting high bias and more complex models exhibiting high variance of the predictive function. However, such a simple trade-off does not adequately describe deep learning models that simultaneously attain low bias and variance in the heavily overparameterized regime. A primary obstacle in explaining this behavior is that deep learning algorithms typically involve multiple sources of randomness whose individual contributions are not visible in the total variance. To enable fine-grained analysis, we describe an interpretable, symmetric decomposition of the variance into terms associated with the randomness from sampling, initialization, and the labels. Moreover, we compute the high-dimensional asymptotic behavior of this decomposition for random feature kernel regression, and analyze the strikingly rich phenomenology that arises. We find that the bias decreases monotonically with the network width, but the variance terms exhibit non-monotonic behavior and can diverge at the interpolation boundary, even in the absence of label noise. The divergence is caused by the {\textbackslash}emph\{interaction\} between sampling and initialization and can therefore be eliminated by marginalizing over samples (i.e. bagging) {\textbackslash}emph\{or\} over the initial parameters (i.e. ensemble learning).},
	author = {Adlam, Ben and Pennington, Jeffrey},
	date = {2020},
}

@inproceedings{micheli_importance_2020,
	location = {Online},
	title = {On the Importance of Pre-training Data Volume for Compact Language Models},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.632},
	eventtitle = {{EMNLP} 2020},
	pages = {7853--7858},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Micheli, Vincent and d'Hoffschmidt, Martin and Fleuret, François},
	urldate = {2020-11-11},
	date = {2020-11},
}

@inproceedings{liu_understanding_2020,
	location = {Online},
	title = {Understanding the Difficulty of Training Transformers},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.463},
	eventtitle = {{EMNLP} 2020},
	pages = {5747--5763},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
	urldate = {2020-11-11},
	date = {2020-11},
}

@article{damour_underspecification_2020,
	title = {Underspecification Presents Challenges for Credibility in Modern Machine Learning},
	url = {http://arxiv.org/abs/2011.03395},
	abstract = {{ML} models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An {ML} pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern {ML} pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical {ML} pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
	journaltitle = {{arXiv}:2011.03395 [cs, stat]},
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and {McLean}, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
	urldate = {2020-11-09},
	date = {2020-11-06},
	eprinttype = {arxiv},
	eprint = {2011.03395},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bousquet_theory_nodate,
	title = {A Theory of Universal Learning},
	abstract = {How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its “learning curve”, that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the {PAC} model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free {PAC} model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically ﬁxed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy.},
	pages = {51},
	author = {Bousquet, Olivier and Hanneke, Steve and Moran, Shay},
	langid = {english},
}

@inproceedings{raghu_teaching_2020,
	title = {Teaching with Commentaries},
	abstract = {Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching : providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries, meta-learned information helpful for training on a particular task or dataset. We present an efficient and scalable gradient-based method to learn commentaries, leveraging recent work on implicit differentiation. We explore diverse applications of commentaries, from learning weights for individual training examples, to parameterizing label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. In these settings, we find that commentaries can improve training speed and/or performance and also provide fundamental insights about the dataset and training process.},
	author = {Raghu, Aniruddh and Raghu, M. and Kornblith, Simon and Duvenaud, D. and Hinton, Geoffrey E.},
	date = {2020},
}

@article{ren_compositional_2020,
	title = {Compositional Languages Emerge in a Neural Iterated Learning Model},
	url = {http://arxiv.org/abs/2002.01365},
	abstract = {The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents in language games. In this paper, we propose an effective neural iterated learning ({NIL}) algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide learning speed advantages to neural agents during training, which can be incrementally amplified via {NIL}. We provide a probabilistic model of {NIL} and an explanation of why the advantage of compositional language exist. Our experiments confirm our analysis, and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication.},
	journaltitle = {{arXiv}:2002.01365 [cs]},
	author = {Ren, Yi and Guo, Shangmin and Labeau, Matthieu and Cohen, Shay B. and Kirby, Simon},
	urldate = {2020-11-06},
	date = {2020-02-17},
	eprinttype = {arxiv},
	eprint = {2002.01365},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{hennigen_intrinsic_2020,
	title = {Intrinsic Probing through Dimension Selection},
	url = {http://arxiv.org/abs/2010.02812},
	abstract = {Most modern {NLP} systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe {fastText} and {BERT} for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with {fastText} concentrating its linguistic structure more than {BERT}.},
	journaltitle = {{arXiv}:2010.02812 [cs]},
	author = {Hennigen, Lucas Torroba and Williams, Adina and Cotterell, Ryan},
	urldate = {2020-11-02},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2010.02812},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{linzen_how_2020,
	title = {How Can We Accelerate Progress Towards Human-like Linguistic Generalization?},
	doi = {10.18653/v1/2020.acl-main.465},
	abstract = {This position paper describes and critiques the Pretraining-Agnostic Identically Distributed ({PAID}) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing {PAID} with paradigms that reward architectures that generalize as quickly and robustly as humans.},
	booktitle = {{ACL}},
	author = {Linzen, Tal},
	date = {2020},
}

@inproceedings{nguyen_wide_2020,
	title = {Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth},
	shorttitle = {Do Wide and Deep Networks Learn the Same Things?},
	abstract = {A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design has resulted in highly effective architectures for a variety of tasks. Nevertheless, there is limited understanding of effects of depth and width on the learned representations. In this paper, we study this fundamental question. We begin by investigating how varying depth and width affects model hidden representations, finding a characteristic block structure in the hidden representations of larger capacity (wider or deeper) models. We demonstrate that this block structure arises when model capacity is large relative to the size of the training set, and is indicative of the underlying layers preserving and propagating the dominant principal component of their representations. This discovery has important ramifications for features learned by different models, namely, representations outside the block structure are often similar across architectures with varying widths and depths, but the block structure is unique to each model. We analyze the output predictions of different model architectures, finding that even when the overall accuracy is similar, wide and deep models exhibit distinctive error patterns and variations across classes.},
	author = {Nguyen, Thao and Raghu, M. and Kornblith, Simon},
	date = {2020},
}

@article{fort_deep_2020,
	title = {Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel},
	url = {http://arxiv.org/abs/2010.15110},
	shorttitle = {Deep learning versus kernel learning},
	abstract = {In suitably initialized wide networks, small learning rates transform deep neural networks ({DNNs}) into neural tangent kernel ({NTK}) machines, whose training dynamics is well-approximated by a linear weight expansion of the network at initialization. Standard training, however, diverges from its linearization in ways that are poorly understood. We study the relationship between the training dynamics of nonlinear deep networks, the geometry of the loss landscape, and the time evolution of a data-dependent {NTK}. We do so through a large-scale phenomenological analysis of training, synthesizing diverse measures characterizing loss landscape geometry and {NTK} dynamics. In multiple neural architectures and datasets, we find these diverse measures evolve in a highly correlated manner, revealing a universal picture of the deep learning process. In this picture, deep network training exhibits a highly chaotic rapid initial transient that within 2 to 3 epochs determines the final linearly connected basin of low loss containing the end point of training. During this chaotic transient, the {NTK} changes rapidly, learning useful features from the training data that enables it to outperform the standard initial {NTK} by a factor of 3 in less than 3 to 4 epochs. After this rapid chaotic transient, the {NTK} changes at constant velocity, and its performance matches that of full network training in 15\% to 45\% of training time. Overall, our analysis reveals a striking correlation between a diverse set of metrics over training time, governed by a rapid chaotic to stable transition in the first few epochs, that together poses challenges and opportunities for the development of more accurate theories of deep learning.},
	journaltitle = {{arXiv}:2010.15110 [cs, stat]},
	author = {Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M. and Ganguli, Surya},
	urldate = {2020-10-30},
	date = {2020-10-28},
	eprinttype = {arxiv},
	eprint = {2010.15110},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{sennrich_revisiting_2019,
	location = {Florence, Italy},
	title = {Revisiting Low-Resource Neural Machine Translation: A Case Study},
	url = {https://www.aclweb.org/anthology/P19-1021},
	doi = {10.18653/v1/P19-1021},
	shorttitle = {Revisiting Low-Resource Neural Machine Translation},
	abstract = {It has been shown that the performance of neural machine translation ({NMT}) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation ({PBSMT}) and requiring large amounts of auxiliary data to achieve competitive results. In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. We discuss some pitfalls to be aware of when training low-resource {NMT} systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource {NMT}. In our experiments on German–English with different amounts of {IWSLT}14 training data, we show that, without the use of any auxiliary monolingual or multilingual data, an optimized {NMT} system can outperform {PBSMT} with far less data than previously claimed. We also apply these techniques to a low-resource Korean–English dataset, surpassing previously reported results by 4 {BLEU}.},
	eventtitle = {{ACL} 2019},
	pages = {211--221},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Zhang, Biao},
	urldate = {2020-10-30},
	date = {2019-07},
}

@article{li_branching_2020,
	title = {On the Branching Bias of Syntax Extracted from Pre-trained Language Models},
	url = {http://arxiv.org/abs/2010.02448},
	abstract = {Many efforts have been devoted to extracting constituency trees from pre-trained language models, often proceeding in two stages: feature definition and parsing. However, this kind of methods may suffer from the branching bias issue, which will inflate the performances on languages with the same branch it biases to. In this work, we propose quantitatively measuring the branching bias by comparing the performance gap on a language and its reversed language, which is agnostic to both language models and extracting methods. Furthermore, we analyze the impacts of three factors on the branching bias, namely parsing algorithms, feature definitions, and language models. Experiments show that several existing works exhibit branching biases, and some implementations of these three factors can introduce the branching bias.},
	journaltitle = {{arXiv}:2010.02448 [cs]},
	author = {Li, Huayang and Liu, Lemao and Huang, Guoping and Shi, Shuming},
	urldate = {2020-10-30},
	date = {2020-10-05},
	eprinttype = {arxiv},
	eprint = {2010.02448},
	keywords = {Computer Science - Computation and Language},
}

@article{hupkes_visualisation_2017,
	title = {Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure},
	url = {http://arxiv.org/abs/1711.10203},
	abstract = {We investigate how neural networks can learn and process languages with hierarchical, compositional semantics. To this end, we define the artificial task of processing nested arithmetic expressions, and study whether different types of neural networks can learn to compute their meaning. We find that recursive neural networks can find a generalising solution to this problem, and we visualise this solution by breaking it up in three steps: project, sum and squash. As a next step, we investigate recurrent neural networks, and show that a gated recurrent unit, that processes its input incrementally, also performs very well on this task. To develop an understanding of what the recurrent network encodes, visualisation techniques alone do not suffice. Therefore, we develop an approach where we formulate and test multiple hypotheses on the information encoded and processed by the network. For each hypothesis, we derive predictions about features of the hidden state representations at each time step, and train 'diagnostic classifiers' to test those predictions. Our results indicate that the networks follow a strategy similar to our hypothesised 'cumulative strategy', which explains the high accuracy of the network on novel expressions, the generalisation to longer expressions than seen in training, and the mild deterioration with increasing length. This is turn shows that diagnostic classifiers can be a useful technique for opening up the black box of neural networks. We argue that diagnostic classification, unlike most visualisation techniques, does scale up from small networks in a toy domain, to larger and deeper recurrent networks dealing with real-life data, and may therefore contribute to a better understanding of the internal dynamics of current state-of-the-art models in natural language processing.},
	journaltitle = {{arXiv}:1711.10203 [cs]},
	author = {Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
	urldate = {2018-05-15},
	date = {2017-11-28},
	eprinttype = {arxiv},
	eprint = {1711.10203},
	keywords = {Computer Science - Computation and Language},
}

@article{frazier_sausage_1978,
	title = {The sausage machine: A new two-stage parsing model},
	volume = {6},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/0010027778900021},
	doi = {10.1016/0010-0277(78)90002-1},
	shorttitle = {The sausage machine},
	abstract = {It is proposed that the human sentence parsing device assigns phrase structure to word strings in two steps. The first stage parser assigns lexical and phrasal nodes to substrings of roughly six words. The second stage parser then adds higher nodes to link these phrasal packages together into a complete phrase marker. This model of the parser is compared with {ATN} models, and with the two-stage models of Kimball (1973) and Fodor, Bever and Garrett (1974). Our assumption that the units which are shunted from the first stage to the second stage are defined by their length, rather than by their syntactic type, explains the effects of constituent length on perceptual complexity in center embedded sentences and in sentences of the kind that fall under Kimball's principle of Right Association. The particular division of labor between the two parsing units allows us to explain, without appeal to any ad hoc parsing strategies, why the parser makes certain ‘shortsighted’ errors even though, in general, it is able to make intelligent use of all the information that is available to it.
Résumé
Dans cet article on propose un mécanisme de segmentation des énoncés qui assigne en deux étapes une structure syntagmatique aux suites de mots. La première méthode de segmentation assigne des noeuds lexicaux et syntagmatiques à des suites de 6 mots environ. La seconde ajoute des noeuds à un niveau supérieur pour lier ces blocs syntagmatiques et obtenir ainsi un marqueur syntagmatique complet. Ce modèle de segmentation est comparé d'une part aux modèles {ATN} et d'autre part au modèle en deux étapes de Kimball (1973) et Fodor, Bever et Garrett (1974). Nous pensons que les unités qui passent du ler au 2è niveau sont caractérisées par leur longueur plutôt que par leur forme syntaxique. Ceci expliquerait les effects de la longueur des constituants sur la complexité perceptuelle des phrases enclassées et des phrases du type de celles qui tombent sous le principe de l'association à droite de Kimball. La distinction spécifique du travail entre les deux unités de segmentation permet d'expliquer, sans faire intervenir des stratégies ad hoc, certaines erreurs de segmentation même si, en général, il est possible de faire un usage intelligent de toutes les informations disponibles.},
	pages = {291--325},
	number = {4},
	journaltitle = {Cognition},
	shortjournal = {Cognition},
	author = {Frazier, Lyn and Fodor, Janet Dean},
	urldate = {2020-10-28},
	date = {1978-01-01},
	langid = {english},
}

@article{wanner_atn_1980,
	title = {The {ATN} and the sausage machine: Which one is baloney?},
	shorttitle = {The {ATN} and the sausage machine},
	journaltitle = {Cognition},
	author = {Wanner, Eric},
	date = {1980},
	note = {Publisher: Elsevier Science},
}

@article{shannon_mathematical_1948,
	title = {A Mathematical Theory of Communication},
	pages = {55},
	author = {Shannon, C E},
	date = {1948},
	langid = {english},
}

@article{mu_all-but--top_2017,
	title = {All-but-the-top: Simple and effective postprocessing for word representations},
	shorttitle = {All-but-the-top},
	journaltitle = {{arXiv} preprint {arXiv}:1702.01417},
	author = {Mu, Jiaqi and Bhat, Suma and Viswanath, Pramod},
	date = {2017},
}

@inproceedings{mimno_strange_2017,
	location = {Copenhagen, Denmark},
	title = {The strange geometry of skip-gram with negative sampling},
	url = {https://www.aclweb.org/anthology/D17-1308},
	doi = {10.18653/v1/D17-1308},
	abstract = {Despite their ubiquity, word embeddings trained with skip-gram negative sampling ({SGNS}) remain poorly understood. We find that vector positions are not simply determined by semantic similarity, but rather occupy a narrow cone, diametrically opposed to the context vectors. We show that this geometric concentration depends on the ratio of positive to negative examples, and that it is neither theoretically nor empirically inherent in related embedding algorithms.},
	eventtitle = {{EMNLP} 2017},
	pages = {2873--2878},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Thompson, Laure},
	urldate = {2020-10-28},
	date = {2017-09},
}

@article{wick_detecting_2020,
	title = {Detecting and Exorcising Statistical Demons from Language Models with Anti-Models of Negative Data},
	url = {http://arxiv.org/abs/2010.11855},
	abstract = {It's been said that "Language Models are Unsupervised Multitask Learners." Indeed, self-supervised language models trained on "positive" examples of English text generalize in desirable ways to many natural language tasks. But if such models can stray so far from an initial self-supervision objective, a wayward model might generalize in undesirable ways too, say to nonsensical "negative" examples of unnatural language. A key question in this work is: do language models trained on (positive) training data also generalize to (negative) test data? We use this question as a contrivance to assess the extent to which language models learn undesirable properties of text, such as n-grams, that might interfere with the learning of more desirable properties of text, such as syntax. We find that within a model family, as the number of parameters, training epochs, and data set size increase, so does a model's ability to generalize to negative n-gram data, indicating standard self-supervision generalizes too far. We propose a form of inductive bias that attenuates such undesirable signals with negative data distributions automatically learned from positive data. We apply the method to remove n-gram signals from {LSTMs} and find that doing so causes them to favor syntactic signals, as demonstrated by large error reductions (up to 46\% on the hardest cases) on a syntactic subject-verb agreement task.},
	journaltitle = {{arXiv}:2010.11855 [cs]},
	author = {Wick, Michael L. and Silverstein, Kate and Tristan, Jean-Baptiste and Pocock, Adam and Johnson, Mark},
	urldate = {2020-10-27},
	date = {2020-10-22},
	eprinttype = {arxiv},
	eprint = {2010.11855},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{chung_similarity_2020,
	title = {Similarity Analysis of Self-Supervised Speech Representations},
	url = {http://arxiv.org/abs/2010.11481},
	abstract = {Self-supervised speech representation learning has recently been a prosperous research topic. Many algorithms have been proposed for learning useful representations from large-scale unlabeled data, and their applications to a wide range of speech tasks have also been investigated. However, there has been little research focusing on understanding the properties of existing approaches. In this work, we aim to provide a comparative study of some of the most representative self-supervised algorithms. Specifically, we quantify the similarities between different self-supervised representations using existing similarity measures. We also design probing tasks to study the correlation between the models' pre-training loss and the amount of specific speech information contained in their learned representations. In addition to showing how various self-supervised models behave differently given the same input, our study also finds that the training objective has a higher impact on representation similarity than architectural choices such as building blocks ({RNN}/Transformer/{CNN}) and directionality (uni/bidirectional). Our results also suggest that there exists a strong correlation between pre-training loss and downstream performance for some self-supervised algorithms.},
	journaltitle = {{arXiv}:2010.11481 [cs, eess]},
	author = {Chung, Yu-An and Belinkov, Yonatan and Glass, James},
	urldate = {2020-10-27},
	date = {2020-10-22},
	eprinttype = {arxiv},
	eprint = {2010.11481},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{voita_analyzing_2020,
	title = {Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation},
	url = {http://arxiv.org/abs/2010.10907},
	abstract = {In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of {NMT} models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation ({LRP}). Its underlying 'conservation principle' makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token's influence. We extend {LRP} to the Transformer and conduct an analysis of {NMT} models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.},
	journaltitle = {{arXiv}:2010.10907 [cs]},
	author = {Voita, Elena and Sennrich, Rico and Titov, Ivan},
	urldate = {2020-10-27},
	date = {2020-10-21},
	eprinttype = {arxiv},
	eprint = {2010.10907},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{ethayarajh_how_2019,
	location = {Hong Kong, China},
	title = {How Contextual are Contextualized Word Representations? Comparing the Geometry of {BERT}, {ELMo}, and {GPT}-2 Embeddings},
	url = {https://www.aclweb.org/anthology/D19-1006},
	doi = {10.18653/v1/D19-1006},
	shorttitle = {How Contextual are Contextualized Word Representations?},
	abstract = {Replacing static word embeddings with contextualized word representations has yielded significant improvements on many {NLP} tasks. However, just how contextual are the contextualized representations produced by models such as {ELMo} and {BERT}? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of {LSTMs} produce more task-specific representations. In all layers of {ELMo}, {BERT}, and {GPT}-2, on average, less than 5\% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.},
	eventtitle = {{EMNLP}-{IJCNLP} 2019},
	pages = {55--65},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Ethayarajh, Kawin},
	urldate = {2020-10-26},
	date = {2019-11},
}

@article{bennett_authentic_nodate,
	title = {Authentic Intelligence: A {BLIND}},
	pages = {5},
	author = {Bennett, Cynthia L},
	langid = {english},
}

@article{merrill_parameter_2020,
	title = {Parameter Norm Growth During Training of Transformers},
	url = {http://arxiv.org/abs/2010.09697},
	abstract = {The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically some variant of gradient descent ({GD}). To better understand this bias, we study the tendency of transformer parameters to grow in magnitude during training. We find, both theoretically and empirically, that, in certain contexts, {GD} increases the parameter \$L\_2\$ norm up to a threshold that itself increases with training-set accuracy. This means increasing training accuracy over time enables the norm to increase. Empirically, we show that the norm grows continuously over pretraining for T5 (Raffel et al., 2019). We show that pretrained T5 approximates a semi-discretized network with saturated activation functions. Such "saturated" networks are known to have a reduced capacity compared to the original network family that can be described in automata-theoretic terms. This suggests saturation is a new characterization of an inductive bias implicit in {GD} that is of particular interest for {NLP}. While our experiments focus on transformers, our theoretical analysis extends to other architectures with similar formal properties, such as feedforward {ReLU} networks.},
	journaltitle = {{arXiv}:2010.09697 [cs]},
	author = {Merrill, William and Ramanujan, Vivek and Goldberg, Yoav and Schwartz, Roy and Smith, Noah},
	urldate = {2020-10-26},
	date = {2020-10-19},
	eprinttype = {arxiv},
	eprint = {2010.09697},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@online{noauthor_rethinking_nodate,
	title = {Rethinking Attention with Performers},
	url = {http://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html},
	abstract = {Posted by Krzysztof Choromanski and Lucy Colwell, Research Scientists, Google Research    Transformer models  have achieved state-of-the-art...},
	titleaddon = {Google {AI} Blog},
	urldate = {2020-10-26},
	langid = {english},
}

@article{choromanski_rethinking_2020,
	title = {Rethinking Attention with Performers},
	url = {http://arxiv.org/abs/2009.14794},
	abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach ({FAVOR}+), which may be of independent interest for scalable kernel methods. {FAVOR}+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
	journaltitle = {{arXiv}:2009.14794 [cs, stat]},
	author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
	urldate = {2020-10-26},
	date = {2020-09-30},
	eprinttype = {arxiv},
	eprint = {2009.14794},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dziugaite_search_2020,
	title = {In Search of Robust Measures of Generalization},
	url = {http://arxiv.org/abs/2010.11924},
	abstract = {One of the principal scientific challenges in deep learning is explaining generalization, i.e., why the particular way the community now trains networks to achieve small training error also leads to small error on held-out data from the same population. It is widely appreciated that some worst-case theories -- such as those based on the {VC} dimension of the class of predictors induced by modern neural network architectures -- are unable to explain empirical performance. A large volume of work aims to close this gap, primarily by developing bounds on generalization error, optimization error, and excess risk. When evaluated empirically, however, most of these bounds are numerically vacuous. Focusing on generalization bounds, this work addresses the question of how to evaluate such bounds empirically. Jiang et al. (2020) recently described a large-scale empirical study aimed at uncovering potential causal relationships between bounds/measures and generalization. Building on their study, we highlight where their proposed methods can obscure failures and successes of generalization measures in explaining generalization. We argue that generalization measures should instead be evaluated within the framework of distributional robustness.},
	journaltitle = {{arXiv}:2010.11924 [cs, stat]},
	author = {Dziugaite, Gintare Karolina and Drouin, Alexandre and Neal, Brady and Rajkumar, Nitarshan and Caballero, Ethan and Wang, Linbo and Mitliagkas, Ioannis and Roy, Daniel M.},
	urldate = {2020-10-23},
	date = {2020-10-22},
	eprinttype = {arxiv},
	eprint = {2010.11924},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{marecek_balustrades_2019,
	location = {Florence, Italy},
	title = {From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions},
	url = {https://www.aclweb.org/anthology/W19-4827},
	doi = {10.18653/v1/W19-4827},
	shorttitle = {From Balustrades to Pierre Vinken},
	abstract = {We inspect the multi-head self-attention in Transformer {NMT} encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall.},
	pages = {263--275},
	booktitle = {Proceedings of the 2019 {ACL} Workshop {BlackboxNLP}: Analyzing and Interpreting Neural Networks for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Mareček, David and Rosa, Rudolf},
	urldate = {2020-10-22},
	date = {2019-08},
}

@article{yu_assessing_2020,
	title = {Assessing Phrasal Representation and Composition in Transformers},
	url = {http://arxiv.org/abs/2010.03763},
	abstract = {Deep transformer models have pushed performance on {NLP} tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models.},
	journaltitle = {{arXiv}:2010.03763 [cs]},
	author = {Yu, Lang and Ettinger, Allyson},
	urldate = {2020-10-22},
	date = {2020-10-13},
	eprinttype = {arxiv},
	eprint = {2010.03763},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{qian_investigating_2016,
	location = {Berlin, Germany},
	title = {Investigating Language Universal and Specific Properties in Word Embeddings},
	url = {https://www.aclweb.org/anthology/P16-1140},
	doi = {10.18653/v1/P16-1140},
	eventtitle = {{ACL} 2016},
	pages = {1478--1488},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Qian, Peng and Qiu, Xipeng and Huang, Xuanjing},
	urldate = {2020-10-22},
	date = {2016-08},
}

@article{phang_investigating_2020,
	title = {Investigating and Simplifying Masking-based Saliency Methods for Model Interpretability},
	url = {http://arxiv.org/abs/2010.09750},
	abstract = {Saliency maps that identify the most informative regions of an image for a classifier are valuable for model interpretability. A common approach to creating saliency maps involves generating input masks that mask out portions of an image to maximally deteriorate classification performance, or mask in an image to preserve classification performance. Many variants of this approach have been proposed in the literature, such as counterfactual generation and optimizing over a Gumbel-Softmax distribution. Using a general formulation of masking-based saliency methods, we conduct an extensive evaluation study of a number of recently proposed variants to understand which elements of these methods meaningfully improve performance. Surprisingly, we find that a well-tuned, relatively simple formulation of a masking-based saliency model outperforms many more complex approaches. We find that the most important ingredients for high quality saliency map generation are (1) using both masked-in and masked-out objectives and (2) training the classifier alongside the masking model. Strikingly, we show that a masking model can be trained with as few as 10 examples per class and still generate saliency maps with only a 0.7-point increase in localization error.},
	journaltitle = {{arXiv}:2010.09750 [cs]},
	author = {Phang, Jason and Park, Jungkyu and Geras, Krzysztof J.},
	urldate = {2020-10-21},
	date = {2020-10-19},
	eprinttype = {arxiv},
	eprint = {2010.09750},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chiang_pretrained_2020,
	title = {Pretrained Language Model Embryology: The Birth of {ALBERT}},
	url = {http://arxiv.org/abs/2010.02480},
	shorttitle = {Pretrained Language Model Embryology},
	abstract = {While behaviors of pretrained language models ({LMs}) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that {ALBERT} learns to reconstruct and predict tokens of different parts of speech ({POS}) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at https://github.com/d223302/albert-embryology.},
	journaltitle = {{arXiv}:2010.02480 [cs]},
	author = {Chiang, David C. and Huang, Sung-Feng and Lee, Hung-yi},
	urldate = {2020-10-20},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2010.02480},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{jacovi_towards_2020,
	location = {Online},
	title = {Towards Faithfully Interpretable {NLP} Systems: How Should We Define and Evaluate Faithfulness?},
	url = {https://www.aclweb.org/anthology/2020.acl-main.386},
	doi = {10.18653/v1/2020.acl-main.386},
	shorttitle = {Towards Faithfully Interpretable {NLP} Systems},
	abstract = {With the growing popularity of deep-learning based {NLP} models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is “defined” by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.},
	eventtitle = {{ACL} 2020},
	pages = {4198--4205},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Jacovi, Alon and Goldberg, Yoav},
	urldate = {2020-10-20},
	date = {2020-07},
}

@article{ebrahimi_how_2020,
	title = {How Can Self-Attention Networks Recognize Dyck-n Languages?},
	url = {http://arxiv.org/abs/2010.04303},
	abstract = {We focus on the recognition of Dyck-n (\${\textbackslash}mathcal\{D\}\_n\$) languages with self-attention ({SA}) networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of {SA}, one with a starting symbol ({SA}\${\textasciicircum}+\$) and one without ({SA}\${\textasciicircum}-\$). Our results show that {SA}\${\textasciicircum}+\$ is able to generalize to longer sequences and deeper dependencies. For \${\textbackslash}mathcal\{D\}\_2\$, we find that {SA}\${\textasciicircum}-\$ completely breaks down on long sequences whereas the accuracy of {SA}\${\textasciicircum}+\$ is 58.82\${\textbackslash}\%\$. We find attention maps learned by \${\textbackslash}text\{{SA}\}\{{\textasciicircum}+\}\$ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of {SA} networks is at par with {LSTMs}, which provides evidence on the ability of {SA} to learn hierarchies without recursion.},
	journaltitle = {{arXiv}:2010.04303 [cs]},
	author = {Ebrahimi, Javid and Gelda, Dhruv and Zhang, Wei},
	urldate = {2020-10-19},
	date = {2020-10-08},
	eprinttype = {arxiv},
	eprint = {2010.04303},
	keywords = {Computer Science - Computation and Language, Computer Science - Formal Languages and Automata Theory, Computer Science - Machine Learning},
}
@article{hewitt_rnns_2020,
	title = {{RNNs} can generate bounded hierarchical languages with optimal memory},
	url = {http://arxiv.org/abs/2010.07515},
	abstract = {Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that {RNNs} can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax. We introduce Dyck-(\$k\$,\$m\$), the language of well-nested brackets (of \$k\$ types) and \$m\$-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax. The best known results use \$O(k{\textasciicircum}\{{\textbackslash}frac\{m\}\{2\}\})\$ memory (hidden units) to generate these languages. We prove that an {RNN} with \$O(m {\textbackslash}log k)\$ hidden units suffices, an exponential reduction in memory, by an explicit construction. Finally, we show that no algorithm, even with unbounded computation, can suffice with \$o(m {\textbackslash}log k)\$ hidden units.},
	journaltitle = {{arXiv}:2010.07515 [cs]},
	author = {Hewitt, John and Hahn, Michael and Ganguli, Surya and Liang, Percy and Manning, Christopher D.},
	urldate = {2020-10-19},
	date = {2020-10-15},
	eprinttype = {arxiv},
	eprint = {2010.07515},
	keywords = {Computer Science - Computation and Language},
}

@article{swayamdipta_dataset_2020,
	title = {Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics},
	url = {http://arxiv.org/abs/2009.10795},
	shorttitle = {Dataset Cartography},
	abstract = {Large datasets have become commonplace in {NLP} research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of "ambiguous" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are "easy to learn" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds "hard to learn"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.},
	journaltitle = {{arXiv}:2009.10795 [cs]},
	author = {Swayamdipta, Swabha and Schwartz, Roy and Lourie, Nicholas and Wang, Yizhong and Hajishirzi, Hannaneh and Smith, Noah A. and Choi, Yejin},
	urldate = {2020-10-16},
	date = {2020-10-15},
	eprinttype = {arxiv},
	eprint = {2009.10795},
	keywords = {Computer Science - Computation and Language},
}

@article{jacovi_formalizing_2020,
	title = {Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in {AI}},
	url = {http://arxiv.org/abs/2010.07487},
	shorttitle = {Formalizing Trust in Artificial Intelligence},
	abstract = {Trust is a central component of the interaction between people and {AI}, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in {AI}? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we cause these prerequisites and goals, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, sociology's interpersonal trust (i.e., trust between people). This model rests on two key properties of the vulnerability of the user and the ability to anticipate the impact of the {AI} model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an {AI} is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (which detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We then present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy {AI}, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and {XAI} using our formalization.},
	journaltitle = {{arXiv}:2010.07487 [cs]},
	author = {Jacovi, Alon and Marasović, Ana and Miller, Tim and Goldberg, Yoav},
	urldate = {2020-10-16},
	date = {2020-10-14},
	eprinttype = {arxiv},
	eprint = {2010.07487},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@inproceedings{brunner_identifiability_2019,
	title = {On Identifiability in Transformers},
	url = {https://openreview.net/forum?id=BJg1f6EFDB},
	abstract = {We investigate the identifiability and interpretability of attention distributions and tokens within contextual embeddings in the self-attention based {BERT} model.},
	eventtitle = {International Conference on Learning Representations},
	author = {Brunner, Gino and Liu, Yang and Pascual, Damian and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger},
	urldate = {2020-10-15},
	date = {2019-09-25},
	langid = {english},
}

@inproceedings{pimentel_pareto_2020,
	title = {Pareto Probing: Trading Off Accuracy for Complexity},
	url = {http://arxiv.org/abs/2010.02180},
	shorttitle = {Pareto Probing},
	abstract = {The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention. In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments with such metrics show that probe's performance curves often fail to align with widely accepted rankings between language representations (with, e.g., non-contextual representations outperforming contextual ones). These results lead us to argue, second, that common simplistic probe tasks such as {POS} labeling and dependency arc labeling, are inadequate to evaluate the properties encoded in contextual word representations. We propose full dependency parsing as an example probe task, and demonstrate it with the Pareto hypervolume. In support of our arguments, the results of this illustrative experiment conform closer to accepted rankings among contextual word representations.},
	booktitle = {{EMNLP}},
	author = {Pimentel, Tiago and Saphra, Naomi and Williams, Adina and Cotterell, Ryan},
	urldate = {2020-10-14},
	date = {2020-10-05},
	eprinttype = {arxiv},
	eprint = {2010.02180},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{saphra_lstms_2020,
	title = {{LSTMs} Compose (and Learn) Bottom-Up},
	url = {http://arxiv.org/abs/2010.04650},
	abstract = {Recent work in {NLP} shows that {LSTM} language models capture hierarchical structure in language data. In contrast to existing work, we consider the {\textbackslash}textit\{learning\} process that leads to their compositional behavior. For a closer look at how an {LSTM}'s sequential representations are composed hierarchically, we present a related measure of Decompositional Interdependence ({DI}) between word meanings in an {LSTM}, based on their gate interactions. We connect this measure to syntax with experiments on English language data, where {DI} is higher on pairs of words with lower syntactic distance. To explore the inductive biases that cause these compositional representations to arise during training, we conduct simple experiments on synthetic data. These synthetic experiments support a specific hypothesis about how hierarchical structures are discovered over the course of training: that {LSTM} constituent representations are learned bottom-up, relying on effective representations of their shorter children, rather than learning the longer-range relations independently from children.},
	booktitle = {Findings of {EMNLP}},
	author = {Saphra, Naomi and Lopez, Adam},
	urldate = {2020-10-12},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2010.04650},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{parr_matrix_2018,
	title = {The Matrix Calculus You Need For Deep Learning},
	url = {http://arxiv.org/abs/1802.01528},
	abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai},
	journaltitle = {{arXiv}:1802.01528 [cs, stat]},
	author = {Parr, Terence and Howard, Jeremy},
	urldate = {2020-10-13},
	date = {2018-07-02},
	eprinttype = {arxiv},
	eprint = {1802.01528},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bhattamishra_ability_2020,
	title = {On the Ability and Limitations of Transformers to Recognize Formal Languages},
	url = {http://arxiv.org/abs/2009.11264},
	abstract = {Transformers have supplanted recurrent models in a large number of {NLP} tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that {LSTMs} generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to {LSTMs}, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.},
	journaltitle = {{arXiv}:2009.11264 [cs]},
	author = {Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
	urldate = {2020-10-12},
	date = {2020-10-08},
	eprinttype = {arxiv},
	eprint = {2009.11264},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{dasgupta_analyzing_2019,
	title = {Analyzing machine-learned representations: A natural language case study},
	url = {http://arxiv.org/abs/1909.05885},
	shorttitle = {Analyzing machine-learned representations},
	abstract = {As modern deep networks become more complex, and get closer to human-like capabilities in certain domains, the question arises of how the representations and decision rules they learn compare to the ones in humans. In this work, we study representations of sentences in one such artificial system for natural language processing. We first present a diagnostic test dataset to examine the degree of abstract composable structure represented. Analyzing performance on these diagnostic tests indicates a lack of systematicity in the representations and decision rules, and reveals a set of heuristic strategies. We then investigate the effect of the training distribution on learning these heuristic strategies, and study changes in these representations with various augmentations to the training set. Our results reveal parallels to the analogous representations in people. We find that these systems can learn abstract rules and generalize them to new contexts under certain circumstances -- similar to human zero-shot reasoning. However, we also note some shortcomings in this generalization behavior -- similar to human judgment errors like belief bias. Studying these parallels suggests new ways to understand psychological phenomena in humans as well as informs best strategies for building artificial intelligence with human-like language understanding.},
	journaltitle = {{arXiv}:1909.05885 [cs, stat]},
	author = {Dasgupta, Ishita and Guo, Demi and Gershman, Samuel J. and Goodman, Noah D.},
	urldate = {2020-10-12},
	date = {2019-09-12},
	eprinttype = {arxiv},
	eprint = {1909.05885},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ettinger_what_2020,
	title = {What {BERT} is not: Lessons from a new suite of psycholinguistic diagnostics for language models},
	url = {http://arxiv.org/abs/1907.13528},
	shorttitle = {What {BERT} is not},
	abstract = {Pre-training by language modeling has become a popular and successful approach to {NLP} tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about the information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular {BERT} model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inferences and role-based event prediction -- and in particular, it shows clear insensitivity to the contextual impacts of negation.},
	journaltitle = {{arXiv}:1907.13528 [cs]},
	author = {Ettinger, Allyson},
	urldate = {2020-10-12},
	date = {2020-07-13},
	eprinttype = {arxiv},
	eprint = {1907.13528},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{mosbach_interplay_2020,
	title = {On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers},
	url = {http://arxiv.org/abs/2010.02616},
	abstract = {Fine-tuning pre-trained contextualized embedding models has become an integral part of the {NLP} pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: {BERT}, {RoBERTa}, and {ALBERT}, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.},
	journaltitle = {{arXiv}:2010.02616 [cs]},
	author = {Mosbach, Marius and Khokhlova, Anna and Hedderich, Michael A. and Klakow, Dietrich},
	urldate = {2020-10-10},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2010.02616},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{miaschi_linguistic_2020,
	title = {Linguistic Profiling of a Neural Language Model},
	url = {http://arxiv.org/abs/2010.01869},
	abstract = {In this paper we investigate the linguistic knowledge learned by a Neural Language Model ({NLM}) before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that {BERT} is able to encode a wide range of linguistic characteristics, but it tends to lose this information when trained on specific downstream tasks. We also find that {BERT}'s capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information, the higher will be its capacity of predicting the correct label.},
	journaltitle = {{arXiv}:2010.01869 [cs]},
	author = {Miaschi, Alessio and Brunato, Dominique and Dell'Orletta, Felice and Venturi, Giulia},
	urldate = {2020-10-10},
	date = {2020-10-05},
	eprinttype = {arxiv},
	eprint = {2010.01869},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{saunshi_mathematical_2020,
	title = {A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks},
	abstract = {Autoregressive language models pretrained on large corpora have been successful at solving downstream tasks, even with zero-shot usage. However, there is little theoretical justification for their success. This paper considers the following questions: (1) Why should learning the distribution of natural language help with downstream classification tasks? (2) Why do features learned using language modeling help solve downstream tasks with linear classifiers? For (1), we hypothesize, and verify empirically, that classification tasks of interest can be reformulated as next word prediction tasks, thus making language modeling a meaningful pretraining task. For (2), we analyze properties of the cross-entropy objective to show that -optimal language models in cross-entropy (log-perplexity) learn features that are O( √ )-good on natural linear classification tasks, thus demonstrating mathematically that doing well on language modeling can be beneficial for downstream tasks. We perform experiments to verify assumptions and validate theoretical results. Our theoretical insights motivate a simple alternative to the cross-entropy objective that performs well on some linear classification tasks.},
	author = {Saunshi, Nikunj and Malladi, Sadhika and Arora, S.},
	date = {2020},
}

@article{yu_assessing_2020,
	title = {Assessing Phrasal Representation and Composition in Transformers},
	url = {http://arxiv.org/abs/2010.03763},
	abstract = {Deep transformer models have pushed performance on {NLP} tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models.},
	journaltitle = {{arXiv}:2010.03763 [cs]},
	author = {Yu, Lang and Ettinger, Allyson},
	urldate = {2020-10-09},
	date = {2020-10-08},
	eprinttype = {arxiv},
	eprint = {2010.03763},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{durrani_analyzing_2020,
	title = {Analyzing Individual Neurons in Pre-trained Language Models},
	abstract = {While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep {NLP} models, very little attention has been paid towards individual neurons. We carry out a neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pretrained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study reveals interesting cross architectural comparisons. For example, we found neurons in {XLNet} to be more localized and disjoint when predicting properties compared to {BERT} and others, where they are more distributed and coupled.},
	author = {Durrani, Nadir and Sajjad, Hassan and Dalvi, F. and Belinkov, Yonatan},
	date = {2020},
}

@article{li_reconciling_2020,
	title = {Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate},
	url = {http://arxiv.org/abs/2010.02916},
	shorttitle = {Reconciling Modern Deep Learning with Traditional Optimization Analyses},
	abstract = {Recent works (e.g., (Li and Arora, 2020)) suggest that the use of popular normalization schemes (including Batch Normalization) in today's deep learning can move it far from a traditional optimization viewpoint, e.g., use of exponentially increasing learning rates. The current paper highlights other ways in which behavior of normalized nets departs from traditional viewpoints, and then initiates a formal framework for studying their mathematics via suitable adaptation of the conventional framework namely, modeling {SGD}-induced training trajectory via a suitable stochastic differential equation ({SDE}) with a noise term that captures gradient noise. This yields: (a) A new ' intrinsic learning rate' parameter that is the product of the normal learning rate and weight decay factor. Analysis of the {SDE} shows how the effective speed of learning varies and equilibrates over time under the control of intrinsic {LR}. (b) A challenge -- via theory and experiments -- to popular belief that good generalization requires large learning rates at the start of training. (c) New experiments, backed by mathematical intuition, suggesting the number of steps to equilibrium (in function space) scales as the inverse of the intrinsic learning rate, as opposed to the exponential time convergence bound implied by {SDE} analysis. We name it the Fast Equilibrium Conjecture and suggest it holds the key to why Batch Normalization is effective.},
	journaltitle = {{arXiv}:2010.02916 [cs]},
	author = {Li, Zhiyuan and Lyu, Kaifeng and Arora, Sanjeev},
	urldate = {2020-10-07},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2010.02916},
	keywords = {Computer Science - Machine Learning},
}

@article{mannelli_complex_2020,
	title = {Complex Dynamics in Simple Neural Networks: Understanding Gradient Flow in Phase Retrieval},
	url = {http://arxiv.org/abs/2006.06997},
	shorttitle = {Complex Dynamics in Simple Neural Networks},
	abstract = {Despite the widespread use of gradient-based algorithms for optimizing high-dimensional non-convex functions, understanding their ability of finding good minima instead of being trapped in spurious ones remains to a large extent an open problem. Here we focus on gradient flow dynamics for phase retrieval from random measurements. When the ratio of the number of measurements over the input dimension is small the dynamics remains trapped in spurious minima with large basins of attraction. We find analytically that above a critical ratio those critical points become unstable developing a negative direction toward the signal. By numerical experiments we show that in this regime the gradient flow algorithm is not trapped; it drifts away from the spurious critical points along the unstable direction and succeeds in finding the global minimum. Using tools from statistical physics we characterize this phenomenon, which is related to a {BBP}-type transition in the Hessian of the spurious minima.},
	journaltitle = {{arXiv}:2006.06997 [cond-mat, stat]},
	author = {Mannelli, Stefano Sarao and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborová, Lenka},
	urldate = {2020-10-07},
	date = {2020-06-12},
	eprinttype = {arxiv},
	eprint = {2006.06997},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@online{noauthor_dynamics_nodate,
	title = {{ON} {THE} {DYNAMICS} {OF} {TRAINING} {ATTENTION} {MODELS}},
	url = {https://openreview.net/pdf?id=1OCTOShAmqB},
	urldate = {2020-10-07},
}

@article{fort_emergent_2019,
	title = {Emergent properties of the local geometry of neural loss landscapes},
	url = {http://arxiv.org/abs/1910.05929},
	abstract = {The local geometry of high dimensional neural network loss landscapes can both challenge our cherished theoretical intuitions as well as dramatically impact the practical success of neural network training. Indeed recent works have observed 4 striking local properties of neural loss landscapes on classification tasks: (1) the landscape exhibits exactly \$C\$ directions of high positive curvature, where \$C\$ is the number of classes; (2) gradient directions are largely confined to this extremely low dimensional subspace of positive Hessian curvature, leaving the vast majority of directions in weight space unexplored; (3) gradient descent transiently explores intermediate regions of higher positive curvature before eventually finding flatter minima; (4) training can be successful even when confined to low dimensional \{{\textbackslash}it random\} affine hyperplanes, as long as these hyperplanes intersect a Goldilocks zone of higher than average curvature. We develop a simple theoretical model of gradients and Hessians, justified by numerical experiments on architectures and datasets used in practice, that \{{\textbackslash}it simultaneously\} accounts for all \$4\$ of these surprising and seemingly unrelated properties. Our unified model provides conceptual insights into the emergence of these properties and makes connections with diverse topics in neural networks, random matrix theory, and spin glasses, including the neural tangent kernel, {BBP} phase transitions, and Derrida's random energy model.},
	journaltitle = {{arXiv}:1910.05929 [cs, stat]},
	author = {Fort, Stanislav and Ganguli, Surya},
	urldate = {2020-10-07},
	date = {2019-10-14},
	eprinttype = {arxiv},
	eprint = {1910.05929},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{hu_surprising_2020,
	title = {The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks},
	url = {http://arxiv.org/abs/2006.14599},
	abstract = {Modern neural networks are often regarded as complex black-box functions whose behavior is difficult to understand owing to their nonlinear dependence on the data and the nonconvexity in their loss landscapes. In this work, we show that these common perceptions can be completely false in the early phase of learning. In particular, we formally prove that, for a class of well-behaved input distributions, the early-time learning dynamics of a two-layer fully-connected neural network can be mimicked by training a simple linear model on the inputs. We additionally argue that this surprising simplicity can persist in networks with more layers and with convolutional architecture, which we verify empirically. Key to our analysis is to bound the spectral norm of the difference between the Neural Tangent Kernel ({NTK}) at initialization and an affine transform of the data kernel; however, unlike many previous results utilizing the {NTK}, we do not require the network to have disproportionately large width, and the network is allowed to escape the kernel regime later in training.},
	journaltitle = {{arXiv}:2006.14599 [cs, stat]},
	author = {Hu, Wei and Xiao, Lechao and Adlam, Ben and Pennington, Jeffrey},
	urldate = {2020-10-07},
	date = {2020-06-25},
	eprinttype = {arxiv},
	eprint = {2006.14599},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{lewkowycz_training_2020,
	title = {On the training dynamics of deep networks with \$L\_2\$ regularization},
	url = {http://arxiv.org/abs/2006.08643},
	abstract = {We study the role of \$L\_2\$ regularization in deep learning, and uncover simple relations between the performance of the model, the \$L\_2\$ coefficient, the learning rate, and the number of training steps. These empirical relations hold when the network is overparameterized. They can be used to predict the optimal regularization parameter of a given model. In addition, based on these observations we propose a dynamical schedule for the regularization parameter that improves performance and speeds up training. We test these proposals in modern image classification settings. Finally, we show that these empirical relations can be understood theoretically in the context of infinitely wide networks. We derive the gradient flow dynamics of such networks, and compare the role of \$L\_2\$ regularization in this context with that of linear models.},
	journaltitle = {{arXiv}:2006.08643 [cs, stat]},
	author = {Lewkowycz, Aitor and Gur-Ari, Guy},
	urldate = {2020-10-07},
	date = {2020-06-15},
	eprinttype = {arxiv},
	eprint = {2006.08643},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{shibata_how_2020,
	title = {How {LSTM} Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text},
	url = {http://arxiv.org/abs/2010.00363},
	shorttitle = {How {LSTM} Encodes Syntax},
	abstract = {Long Short-Term Memory recurrent neural network ({LSTM}) is widely used and known to capture informative long-term syntactic dependencies. However, how such information are reflected in its internal vectors for natural text has not yet been sufficiently investigated. We analyze them by learning a language model where syntactic structures are implicitly given. We empirically show that the context update vectors, i.e. outputs of internal gates, are approximately quantized to binary or ternary values to help the language model to count the depth of nesting accurately, as Suzgun et al. (2019) recently show for synthetic Dyck languages. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as {VP} and {NP}. Moreover, with an \$L\_1\$ regularization, we also found that it can accurately predict whether a word is inside a phrase structure or not from a small number of components of the context vector. Even for the case of learning from raw text, context vectors are shown to still correlate well with the phrase structures. Finally, we show that natural clusters of the functional words and the part of speeches that trigger phrases are represented in a small but principal subspace of the context-update vector of {LSTM}.},
	journaltitle = {{arXiv}:2010.00363 [cs]},
	author = {Shibata, Chihiro and Uchiumi, Kei and Mochihashi, Daichi},
	urldate = {2020-10-06},
	date = {2020-10-01},
	eprinttype = {arxiv},
	eprint = {2010.00363},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{saphra_understanding_2019,
	location = {Minneapolis, Minnesota},
	title = {Understanding Learning Dynamics Of Language Models with {SVCCA}},
	url = {https://www.aclweb.org/anthology/N19-1329},
	doi = {10.18653/v1/N19-1329},
	abstract = {Research has shown that neural models implicitly encode linguistic features, but there has been no research showing how these encodings arise as the models are trained. We present the first study on the learning dynamics of neural language models, using a simple and flexible analysis method called Singular Vector Canonical Correlation Analysis ({SVCCA}), which enables us to compare learned representations across time and across models, without the need to evaluate directly on annotated data. We probe the evolution of syntactic, semantic, and topic representations, finding, for example, that part-of-speech is learned earlier than topic; that recurrent layers become more similar to those of a tagger during training; and embedding layers less similar. Our results and methods could inform better learning algorithms for {NLP} models, possibly to incorporate linguistic information more effectively.},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {3257--3267},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Saphra, Naomi and Lopez, Adam},
	urldate = {2020-10-06},
	date = {2019-06},
}

@article{saxe_mathematical_2019,
	title = {A mathematical theory of semantic development in deep neural networks},
	volume = {116},
	rights = {© 2019 . https://www.pnas.org/site/aboutpnas/licenses.{xhtmlPublished} under the {PNAS} license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/23/11537},
	doi = {10.1073/pnas.1820226116},
	abstract = {An extensive body of empirical research has revealed remarkable regularities in the acquisition, organization, deployment, and neural representation of human semantic knowledge, thereby raising a fundamental conceptual question: What are the theoretical principles governing the ability of neural networks to acquire, organize, and deploy abstract knowledge by integrating across many individual experiences? We address this question by mathematically analyzing the nonlinear dynamics of learning in deep linear networks. We find exact solutions to this learning dynamics that yield a conceptual explanation for the prevalence of many disparate phenomena in semantic cognition, including the hierarchical differentiation of concepts through rapid developmental transitions, the ubiquity of semantic illusions between such transitions, the emergence of item typicality and category coherence as factors controlling the speed of semantic processing, changing patterns of inductive projection over development, and the conservation of semantic similarity in neural representations across species. Thus, surprisingly, our simple neural model qualitatively recapitulates many diverse regularities underlying semantic development, while providing analytic insight into how the statistical structure of an environment can interact with nonlinear deep-learning dynamics to give rise to these regularities.},
	pages = {11537--11546},
	number = {23},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Saxe, Andrew M. and {McClelland}, James L. and Ganguli, Surya},
	urldate = {2020-10-06},
	date = {2019-06-04},
	langid = {english},
	pmid = {31101713},
	note = {Publisher: National Academy of Sciences
Section: {PNAS} Plus},
	keywords = {deep learning, generative models, neural networks, semantic cognition},
}

@inproceedings{hupkes_compositionality_2020,
	title = {Compositionality Decomposed: How do Neural Networks Generalise? (Extended Abstract)},
	volume = {5},
	url = {https://www.ijcai.org/proceedings/2020/708},
	doi = {10.24963/ijcai.2020/708},
	shorttitle = {Compositionality Decomposed},
	abstract = {Electronic proceedings of {IJCAI} 2020},
	eventtitle = {Twenty-Ninth International Joint Conference on Artificial Intelligence},
	pages = {5065--5069},
	booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, \{{IJCAI}-20\}},
	author = {Hupkes, Dieuwke and Dankers, Verna and Bruni, Elia and Mul, Mathijs},
	urldate = {2020-10-06},
	date = {2020-07-09},
	langid = {english},
	note = {{ISSN}: 1045-0823},
}

@article{hupkes_compositionality_2020-1,
	title = {Compositionality decomposed: how do neural networks generalise?},
	url = {http://arxiv.org/abs/1908.08351},
	shorttitle = {Compositionality decomposed},
	abstract = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub {PCFG} {SET} and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
	journaltitle = {{arXiv}:1908.08351 [cs, stat]},
	author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
	urldate = {2020-10-06},
	date = {2020-02-23},
	eprinttype = {arxiv},
	eprint = {1908.08351},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{moss_boss_2020,
	title = {{BOSS}: Bayesian Optimization over String Spaces},
	url = {http://arxiv.org/abs/2010.00979},
	shorttitle = {{BOSS}},
	abstract = {This article develops a Bayesian optimization ({BO}) method which acts directly over raw strings, proposing the first uses of string kernels and genetic algorithms within {BO} loops. Recent applications of {BO} over strings have been hindered by the need to map inputs into a smooth and unconstrained latent space. Learning this projection is computationally and data-intensive. Our approach instead builds a powerful Gaussian process surrogate model based on string kernels, naturally supporting variable length inputs, and performs efficient acquisition function maximization for spaces with syntactical constraints. Experiments demonstrate considerably improved optimization over existing approaches across a broad range of constraints, including the popular setting where syntax is governed by a context-free grammar.},
	journaltitle = {{arXiv}:2010.00979 [cs, stat]},
	author = {Moss, Henry B. and Beck, Daniel and Gonzalez, Javier and Leslie, David S. and Rayson, Paul},
	urldate = {2020-10-06},
	date = {2020-10-02},
	eprinttype = {arxiv},
	eprint = {2010.00979},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{blevins_deep_2018,
	location = {Melbourne, Australia},
	title = {Deep {RNNs} Encode Soft Hierarchical Syntax},
	url = {https://www.aclweb.org/anthology/P18-2003},
	doi = {10.18653/v1/P18-2003},
	abstract = {We present a set of experiments to demonstrate that deep recurrent neural networks ({RNNs}) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision.},
	eventtitle = {{ACL} 2018},
	pages = {14--19},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
	urldate = {2020-10-04},
	date = {2018-07},
}

@article{thompson_computational_2020,
	title = {The Computational Limits of Deep Learning},
	url = {http://arxiv.org/abs/2007.05558},
	abstract = {Deep learning's recent history has been one of achievement: from triumphing over humans in the game of Go to world-leading performance in image recognition, voice recognition, translation, and other tasks. But this progress has come with a voracious appetite for computing power. This article reports on the computational demands of Deep Learning applications in five prominent application areas and shows that progress in all five is strongly reliant on increases in computing power. Extrapolating forward this reliance reveals that progress along current lines is rapidly becoming economically, technically, and environmentally unsustainable. Thus, continued progress in these applications will require dramatically more computationally-efficient methods, which will either have to come from changes to deep learning or from moving to other machine learning methods.},
	journaltitle = {{arXiv}:2007.05558 [cs, stat]},
	author = {Thompson, Neil C. and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
	urldate = {2020-09-29},
	date = {2020-07-10},
	eprinttype = {arxiv},
	eprint = {2007.05558},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{atanasova_diagnostic_2020,
	title = {A Diagnostic Study of Explainability Techniques for Text Classification},
	url = {http://arxiv.org/abs/2009.13295},
	abstract = {Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.},
	journaltitle = {{arXiv}:2009.13295 [cs]},
	author = {Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle},
	urldate = {2020-09-29},
	date = {2020-09-25},
	eprinttype = {arxiv},
	eprint = {2009.13295},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7, cs.{CL}, cs.{AI}},
}

@article{bhattamishra_ability_2020-1,
	title = {On the Ability of Self-Attention Networks to Recognize Counter Languages},
	url = {http://arxiv.org/abs/2009.11264},
	abstract = {Transformers have supplanted recurrent models in a large number of {NLP} tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that {LSTMs} generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to {LSTMs}, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behavior and the influence of positional encoding schemes on the learning and generalization ability of the model.},
	journaltitle = {{arXiv}:2009.11264 [cs]},
	author = {Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
	urldate = {2020-09-29},
	date = {2020-09-23},
	eprinttype = {arxiv},
	eprint = {2009.11264},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{hooker_hardware_2020,
	title = {The Hardware Lottery},
	url = {http://arxiv.org/abs/2009.06489},
	abstract = {Hardware, systems and algorithms research communities have historically had different incentive structures and fluctuating motivation to engage with each other explicitly. This historical treatment is odd given that hardware and software have frequently determined which research ideas succeed (and fail). This essay introduces the term hardware lottery to describe when a research idea wins because it is suited to the available software and hardware and not because the idea is superior to alternative research directions. Examples from early computer science history illustrate how hardware lotteries can delay research progress by casting successful ideas as failures. These lessons are particularly salient given the advent of domain specialized hardware which makes it increasingly costly to stray off of the beaten path of research ideas.},
	journaltitle = {{arXiv}:2009.06489 [cs]},
	author = {Hooker, Sara},
	urldate = {2020-09-18},
	date = {2020-09-14},
	eprinttype = {arxiv},
	eprint = {2009.06489},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Hardware Architecture, Computer Science - Machine Learning, read},
}

@article{mcallester_formal_2020,
	title = {Formal Limitations on the Measurement of Mutual Information},
	url = {http://arxiv.org/abs/1811.04251},
	abstract = {Measuring mutual information from finite data is difficult. Recent work has considered variational methods maximizing a lower bound. In this paper, we prove that serious statistical limitations are inherent to any method of measuring mutual information. More specifically, we show that any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O(ln N ).},
	journaltitle = {{arXiv}:1811.04251 [cs, math, stat]},
	author = {{McAllester}, David and Stratos, Karl},
	urldate = {2020-09-25},
	date = {2020-05-20},
	eprinttype = {arxiv},
	eprint = {1811.04251},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{rosenfeld_constructive_2019,
	title = {A Constructive Prediction of the Generalization Error Across Scales},
	url = {https://openreview.net/forum?id=ryenvpEKDr},
	abstract = {We predict the generalization error and specify the model which attains it across model/data scales.},
	eventtitle = {International Conference on Learning Representations},
	author = {Rosenfeld, Jonathan S. and Rosenfeld, Amir and Belinkov, Yonatan and Shavit, Nir},
	urldate = {2020-09-24},
	date = {2019-09-25},
	langid = {english},
}

@incollection{jacot_neural_2018,
	title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
	url = {http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf},
	shorttitle = {Neural Tangent Kernel},
	pages = {8571--8580},
	booktitle = {Advances in Neural Information Processing Systems 31},
	publisher = {Curran Associates, Inc.},
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	urldate = {2020-09-24},
	date = {2018},
}
@article{wang_can_2019,
	title = {Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling},
	url = {http://arxiv.org/abs/1812.10860},
	shorttitle = {Can You Tell Me How to Get Past Sesame Street?},
	abstract = {Natural language understanding has recently seen a surge of progress with the use of sentence encoders like {ELMo} (Peters et al., 2018a) and {BERT} (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In {ELMo}'s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning {BERT} on an intermediate task often negatively impacts downstream transfer. In a more positive trend, we see modest gains from multitask training, suggesting the development of more sophisticated multitask and transfer learning techniques as an avenue for further research.},
	journaltitle = {{arXiv}:1812.10860 [cs]},
	author = {Wang, Alex and Hula, Jan and Xia, Patrick and Pappagari, Raghavendra and {McCoy}, R. Thomas and Patel, Roma and Kim, Najoung and Tenney, Ian and Huang, Yinghui and Yu, Katherin and Jin, Shuning and Chen, Berlin and Van Durme, Benjamin and Grave, Edouard and Pavlick, Ellie and Bowman, Samuel R.},
	urldate = {2020-09-24},
	date = {2019-07-22},
	eprinttype = {arxiv},
	eprint = {1812.10860},
	keywords = {Computer Science - Computation and Language},
}

@article{frankle_pruning_2020,
	title = {Pruning Neural Networks at Initialization: Why are We Missing the Mark?},
	url = {http://arxiv.org/abs/2009.08576},
	shorttitle = {Pruning Neural Networks at Initialization},
	abstract = {Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: {SNIP} (Lee et al., 2019), {GraSP} (Wang et al., 2020), {SynFlow} (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, they remain below the accuracy of magnitude pruning after training, and we endeavor to understand why. We show that, unlike pruning after training, accuracy is the same or higher when randomly shuffling which weights these methods prune within each layer or sampling new initial values. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property undermines the claimed justifications for these methods and suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both.},
	journaltitle = {{arXiv}:2009.08576 [cs, stat]},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
	urldate = {2020-09-22},
	date = {2020-09-17},
	eprinttype = {arxiv},
	eprint = {2009.08576},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{raghu_rapid_2020,
	title = {Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of {MAML}},
	url = {http://arxiv.org/abs/1909.09157},
	shorttitle = {Rapid Learning or Feature Reuse?},
	abstract = {An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning ({MAML}), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite {MAML}'s popularity, a fundamental open question remains -- is the effectiveness of {MAML} due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the {ANIL} (Almost No Inner Loop) algorithm, a simplification of {MAML} where we remove the inner loop for all but the (task-specific) head of a {MAML}-trained network. {ANIL} matches {MAML}'s performance on benchmark few-shot image classification and {RL} and offers computational improvements over {MAML}. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the {NIL} algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.},
	journaltitle = {{arXiv}:1909.09157 [cs, stat]},
	author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
	urldate = {2020-09-22},
	date = {2020-02-12},
	eprinttype = {arxiv},
	eprint = {1909.09157},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wu_similarity_2020,
	location = {Online},
	title = {Similarity Analysis of Contextual Word Representation Models},
	url = {https://www.aclweb.org/anthology/2020.acl-main.422},
	doi = {10.18653/v1/2020.acl-main.422},
	abstract = {This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.},
	eventtitle = {{ACL} 2020},
	pages = {4638--4655},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Wu, John and Belinkov, Yonatan and Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Glass, James},
	urldate = {2020-09-22},
	date = {2020-07},
}

@article{zhu_information_2020,
	title = {An information theoretic view on selecting linguistic probes},
	url = {http://arxiv.org/abs/2009.07364},
	abstract = {There is increasing interest in assessing the linguistic knowledge encoded in neural representations. A popular approach is to attach a diagnostic classifier -- or "probe" -- to perform supervised classification from internal representations. However, how to select a good probe is in debate. Hewitt and Liang (2019) showed that a high performance on diagnostic classification itself is insufficient, because it can be attributed to either "the representation being rich in knowledge", or "the probe learning the task", which Pimentel et al. (2020) challenged. We show this dichotomy is valid information-theoretically. In addition, we find that the methods to construct and select good probes proposed by the two papers, *control task* (Hewitt and Liang, 2019) and *control function* (Pimentel et al., 2020), are equivalent -- the errors of their approaches are identical (modulo irrelevant terms). Empirically, these two selection criteria lead to results that highly agree with each other.},
	journaltitle = {{arXiv}:2009.07364 [cs]},
	author = {Zhu, Zining and Rudzicz, Frank},
	urldate = {2020-09-22},
	date = {2020-09-17},
	eprinttype = {arxiv},
	eprint = {2009.07364},
	keywords = {Computer Science - Computation and Language},
}

@article{mccoy_universal_2020,
	title = {Universal linguistic inductive biases via meta-learning},
	url = {http://arxiv.org/abs/2006.16324},
	abstract = {How do learners acquire languages from the limited data available to them? This process must involve some inductive biases - factors that affect how a learner generalizes - but it is unclear which inductive biases can explain observed patterns in language acquisition. To facilitate computational modeling aimed at addressing this question, we introduce a framework for giving particular linguistic inductive biases to a neural network model; such a model can then be used to empirically explore the effects of those inductive biases. This framework disentangles universal inductive biases, which are encoded in the initial values of a neural network's parameters, from non-universal factors, which the neural network must learn from data in a given language. The initial state that encodes the inductive biases is found with meta-learning, a technique through which a model discovers how to acquire new languages more easily via exposure to many possible languages. By controlling the properties of the languages that are used during meta-learning, we can control the inductive biases that meta-learning imparts. We demonstrate this framework with a case study based on syllable structure. First, we specify the inductive biases that we intend to give our model, and then we translate those inductive biases into a space of languages from which a model can meta-learn. Finally, using existing analysis techniques, we verify that our approach has imparted the linguistic inductive biases that it was intended to impart.},
	journaltitle = {{arXiv}:2006.16324 [cs]},
	author = {{McCoy}, R. Thomas and Grant, Erin and Smolensky, Paul and Griffiths, Thomas L. and Linzen, Tal},
	urldate = {2020-09-21},
	date = {2020-06-29},
	eprinttype = {arxiv},
	eprint = {2006.16324},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{de_cao_how_2020,
	title = {How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking},
	url = {http://arxiv.org/abs/2004.14992},
	shorttitle = {How do Decisions Emerge across Layers in Neural Models?},
	abstract = {Attribution methods assess the contribution of inputs (e.g., words) to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the model prediction. Despite its conceptual simplicity, erasure is not commonly used in practice. First, the objective is generally intractable, and approximate search or leave-one-out estimates are typically used instead; both approximations may be inaccurate and remain very expensive with modern deep (e.g., {BERT}-based) {NLP} models. Second, the method is susceptible to the hindsight bias: the fact that a token can be dropped does not mean that the model `knows' it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these two challenges, we introduce Differentiable Masking. {DiffMask} relies on learning sparse stochastic gates (i.e., masks) to completely mask-out subsets of the input while maintaining end-to-end differentiability. The decision to include or disregard an input token is made with a simple linear model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient at test time because we predict rather than search. Second, as with probing classifiers, this reveals what the network `knows' at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use {DiffMask} to study {BERT} models on sentiment classification and question answering.},
	journaltitle = {{arXiv}:2004.14992 [cs, stat]},
	author = {De Cao, Nicola and Schlichtkrull, Michael and Aziz, Wilker and Titov, Ivan},
	urldate = {2020-09-16},
	date = {2020-04-30},
	eprinttype = {arxiv},
	eprint = {2004.14992},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
}

@inproceedings{chen_generating_2020,
	location = {Online},
	title = {Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection},
	url = {https://www.aclweb.org/anthology/2020.acl-main.494},
	doi = {10.18653/v1/2020.acl-main.494},
	abstract = {Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness. In natural language processing, existing methods usually provide important features which are words or phrases selected from an input text as an explanation, but ignore the interactions between them. It poses challenges for humans to interpret an explanation and connect it to model prediction. In this work, we build hierarchical explanations by detecting feature interactions. Such explanations visualize how words and phrases are combined at different levels of the hierarchy, which can help users understand the decision-making of black-box models. The proposed method is evaluated with three neural text classifiers ({LSTM}, {CNN}, and {BERT}) on two benchmark datasets, via both automatic and human evaluations. Experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans.},
	eventtitle = {{ACL} 2020},
	pages = {5578--5593},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Hanjie and Zheng, Guangtao and Ji, Yangfeng},
	urldate = {2020-09-15},
	date = {2020-07},
}

@book{chomsky_language_1988,
	title = {Language and problems of knowledge: The Managua lectures},
	volume = {16},
	shorttitle = {Language and problems of knowledge},
	publisher = {{MIT} press},
	author = {Chomsky, Noam and Keyser, Samuel Jay},
	date = {1988},
}

@inproceedings{hewitt_structural_2019,
	location = {Minneapolis, Minnesota},
	title = {A Structural Probe for Finding Syntax in Word Representations},
	url = {https://www.aclweb.org/anthology/N19-1419},
	doi = {10.18653/v1/N19-1419},
	abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both {ELMo} and {BERT} but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {4129--4138},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Hewitt, John and Manning, Christopher D.},
	urldate = {2020-09-15},
	date = {2019-06},
}

@inproceedings{peters_deep_2018,
	location = {New Orleans, Louisiana},
	title = {Deep Contextualized Word Representations},
	url = {https://www.aclweb.org/anthology/N18-1202},
	doi = {10.18653/v1/N18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model ({biLM}), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging {NLP} problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	eventtitle = {{NAACL}-{HLT} 2018},
	pages = {2227--2237},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	urldate = {2020-09-15},
	date = {2018-06},
}

@article{rosenfeld_two_2000,
	title = {Two decades of statistical language modeling: where do we go from here?},
	volume = {88},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/880083/},
	doi = {10.1109/5.880083},
	shorttitle = {Two decades of statistical language modeling},
	abstract = {Statistical Language Models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies. Since the ﬁrst signiﬁcant model was proposed in 1980, many attempts have been made to improve the state of the art. We review them here, point to a few promising directions, and argue for a Bayesian approach to integration of linguistic theories with data.},
	pages = {1270--1278},
	number = {8},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Rosenfeld, R.},
	urldate = {2020-09-15},
	date = {2000-08},
	langid = {english},
}

@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Comput.},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2020-09-15},
	date = {1997-11-01},
}

@article{kadar_representation_2016,
	title = {Representation of linguistic form and function in recurrent neural networks},
	url = {http://arxiv.org/abs/1602.08952},
	abstract = {We present novel methods for analysing the activation patterns of {RNNs} and identifying the types of linguistic structure they learn. As a case study, we use a multi-task gated recurrent network model consisting of two parallel pathways with shared word embeddings trained on predicting the representations of the visual scene corresponding to an input sentence, and predicting the next word in the same sentence. We show that the image prediction pathway is sensitive to the information structure of the sentence, and pays selective attention to lexical categories and grammatical functions that carry semantic information. It also learns to treat the same input token differently depending on its grammatical functions in the sentence. The language model is comparatively more sensitive to words with a syntactic function. Our analysis of the function of individual hidden units shows that each pathway contains specialized units tuned to patterns informative for the task, some of which can carry activations to later time steps to encode long-term dependencies.},
	journaltitle = {{arXiv}:1602.08952 [cs]},
	author = {Kádár, Akos and Chrupała, Grzegorz and Alishahi, Afra},
	urldate = {2016-04-05},
	date = {2016-02-29},
	eprinttype = {arxiv},
	eprint = {1602.08952},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Machine Learning},
}

@inproceedings{van_schijndel_quantity_2019,
	location = {Hong Kong, China},
	title = {Quantity doesn't buy quality syntax with neural language models},
	url = {https://www.aclweb.org/anthology/D19-1592},
	doi = {10.18653/v1/D19-1592},
	abstract = {Recurrent neural networks can learn to predict upcoming words remarkably well on average; in syntactically complex contexts, however, they often assign unexpectedly high probabilities to ungrammatical words. We investigate to what extent these shortcomings can be mitigated by increasing the size of the network and the corpus on which it is trained. We find that gains from increasing network size are minimal beyond a certain point. Likewise, expanding the training corpus yields diminishing returns; we estimate that the training corpus would need to be unrealistically large for the models to match human performance. A comparison to {GPT} and {BERT}, Transformer-based models trained on billions of words, reveals that these models perform even more poorly than our {LSTMs} in some constructions. Our results make the case for more data efficient architectures.},
	eventtitle = {{EMNLP}-{IJCNLP} 2019},
	pages = {5831--5837},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {van Schijndel, Marten and Mueller, Aaron and Linzen, Tal},
	urldate = {2020-09-14},
	date = {2019-11},
}

@article{hu_systematic_2020,
	title = {A Systematic Assessment of Syntactic Generalization in Neural Language Models},
	url = {http://arxiv.org/abs/2005.03692},
	abstract = {While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M--40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.},
	journaltitle = {{arXiv}:2005.03692 [cs]},
	author = {Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger P.},
	urldate = {2020-09-14},
	date = {2020-05-22},
	eprinttype = {arxiv},
	eprint = {2005.03692},
	keywords = {Computer Science - Computation and Language},
}

@article{wilcox_predictive_2020,
	title = {On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior},
	url = {http://arxiv.org/abs/2006.01912},
	abstract = {Human reading behavior is tuned to the statistics of natural language: the time it takes human subjects to read a word can be predicted from estimates of the word's probability in context. However, it remains an open question what computational architecture best characterizes the expectations deployed in real time by humans that determine the behavioral signatures of reading. Here we test over two dozen models, independently manipulating computational architecture and training dataset size, on how well their next-word expectations predict human reading time behavior on naturalistic text corpora. We find that across model architectures and training dataset sizes the relationship between word log-probability and reading time is (near-)linear. We next evaluate how features of these models determine their psychometric predictive power, or ability to predict human reading behavior. In general, the better a model's next-word expectations, the better its psychometric predictive power. However, we find nontrivial differences across model architectures. For any given perplexity, deep Transformer models and n-gram models generally show superior psychometric predictive power over {LSTM} or structurally supervised neural models, especially for eye movement data. Finally, we compare models' psychometric predictive power to the depth of their syntactic knowledge, as measured by a battery of syntactic generalization tests developed using methods from controlled psycholinguistic experiments. Once perplexity is controlled for, we find no significant relationship between syntactic knowledge and predictive power. These results suggest that different approaches may be required to best model human real-time language comprehension behavior in naturalistic reading versus behavior for controlled linguistic materials designed for targeted probing of syntactic knowledge.},
	journaltitle = {{arXiv}:2006.01912 [cs]},
	author = {Wilcox, Ethan Gotlieb and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger},
	urldate = {2020-09-14},
	date = {2020-06-02},
	eprinttype = {arxiv},
	eprint = {2006.01912},
	keywords = {Computer Science - Computation and Language},
}

@online{blog_understanding_nodate,
	title = {Understanding the Neural Tangent Kernel},
	url = {https://rajatvd.github.io/NTK/},
	abstract = {My attempt at distilling the ideas behind the neural tangent kernel that is making waves in recent theoretical deep learning research.},
	author = {Blog, Rajat's},
	urldate = {2020-09-11},
}

@inproceedings{tran_importance_2018,
	location = {Brussels, Belgium},
	title = {The Importance of Being Recurrent for Modeling Hierarchical Structure},
	url = {https://www.aclweb.org/anthology/D18-1503},
	doi = {10.18653/v1/D18-1503},
	abstract = {Recent work has shown that recurrent neural networks ({RNNs}) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many {NLP} tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures—recurrent versus non-recurrent—with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at https://github.com/ ketranm/fan\_vs\_rnn},
	eventtitle = {{EMNLP} 2018},
	pages = {4731--4736},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Tran, Ke and Bisazza, Arianna and Monz, Christof},
	urldate = {2020-09-10},
	date = {2018-10},
}

@incollection{goldt_dynamics_2019,
	title = {Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
	url = {http://papers.nips.cc/paper/8921-dynamics-of-stochastic-gradient-descent-for-two-layer-neural-networks-in-the-teacher-student-setup.pdf},
	pages = {6981--6991},
	booktitle = {Advances in Neural Information Processing Systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Goldt, Sebastian and Advani, Madhu and Saxe, Andrew M and Krzakala, Florent and Zdeborová, Lenka},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d\{{\textbackslash}textbackslash\}textquotesingle and Fox, E. and Garnett, R.},
	urldate = {2020-09-09},
	date = {2019},
}

@incollection{mcclelland_critique_2015,
	title = {A Critique of Pure Hierarchy: Uncovering Cross-Cutting Structure in a Natural Dataset},
	volume = {Volume 22},
	isbn = {978-981-4699-33-4},
	url = {https://www.worldscientific.com/doi/abs/10.1142/9789814699341_0004},
	series = {Progress in Neural Processing},
	shorttitle = {A Critique of Pure Hierarchy},
	volumes = {0},
	pages = {51--68},
	number = {Volume 22},
	booktitle = {Neurocomputational Models of Cognitive Development and Processing},
	publisher = {{WORLD} {SCIENTIFIC}},
	author = {{McClelland}, J. L. and Sadeghi, Z. and Saxe, A. M.},
	urldate = {2020-09-09},
	date = {2015-05-13},
	doi = {10.1142/9789814699341_0004},
}

@inproceedings{xu_theory_2019,
	title = {A Theory of Usable Information under Computational Constraints},
	url = {https://openreview.net/forum?id=r1eBeyHFDH},
	abstract = {We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon’s information theory that takes into account the...},
	eventtitle = {International Conference on Learning Representations},
	author = {Xu, Yilun and Zhao, Shengjia and Song, Jiaming and Stewart, Russell and Ermon, Stefano},
	urldate = {2020-09-09},
	date = {2019-09-25},
}

@article{ravichander_probing_2020,
	title = {Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?},
	url = {http://arxiv.org/abs/2005.00719},
	shorttitle = {Probing the Probing Paradigm},
	abstract = {Much recent attention has been devoted to analyzing sentence representations learned by neural encoders, through the paradigm of 'probing' tasks. This is often motivated by an interest to understand the information a model uses to make its decision. However, to what extent is the information encoded in a sentence representation actually used for the task which the encoder is trained on? In this work, we examine this probing paradigm through a case-study in Natural Language Inference, showing that models learn to encode linguistic properties even when not needed for a task. We identify that pre-trained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level even when distributed as random noise, calling into question the interpretation of absolute claims on probing tasks.},
	journaltitle = {{arXiv}:2005.00719 [cs]},
	author = {Ravichander, Abhilasha and Belinkov, Yonatan and Hovy, Eduard},
	urldate = {2020-09-09},
	date = {2020-05-02},
	eprinttype = {arxiv},
	eprint = {2005.00719},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{kim_probing_2019,
	location = {Minneapolis, Minnesota},
	title = {Probing What Different {NLP} Tasks Teach Machines about Function Word Comprehension},
	url = {https://www.aclweb.org/anthology/S19-1026},
	doi = {10.18653/v1/S19-1026},
	abstract = {We introduce a set of nine challenge tasks that test for the understanding of function words. These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of speciﬁc types of function words (e.g., prepositions, wh-words). Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, {CCG} supertagging and natural language inference ({NLI})) on the learned representations. Our results show that pretraining on language modeling performs the best on average across our probing tasks, supporting its widespread use for pretraining state-of-the-art {NLP} models, and {CCG} supertagging and {NLI} pretraining perform comparably. Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives, e.g., that {NLI} helps the comprehension of negation.},
	eventtitle = {Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019)},
	pages = {235--249},
	booktitle = {Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Najoung and Patel, Roma and Poliak, Adam and Xia, Patrick and Wang, Alex and {McCoy}, Tom and Tenney, Ian and Ross, Alexis and Linzen, Tal and Van Durme, Benjamin and Bowman, Samuel R. and Pavlick, Ellie},
	urldate = {2020-09-09},
	date = {2019},
	langid = {english},
}

@article{abnar_transferring_2020,
	title = {Transferring Inductive Biases through Knowledge Distillation},
	url = {http://arxiv.org/abs/2006.00555},
	abstract = {Having the right inductive biases can be crucial in many tasks or scenarios where data or computing resources are a limiting factor, or where training data is not perfectly representative of the conditions at test time. However, deﬁning, designing and efﬁciently adapting inductive biases is not necessarily straightforward. In this paper, we explore the power of knowledge distillation for transferring the effect of inductive biases from one model to another. We consider families of models with different inductive biases, {LSTMs} vs. Transformers and {CNNs} vs. {MLPs}, in the context of tasks and scenarios where having the right inductive biases is critical. We study how the effect of inductive biases is transferred through knowledge distillation, in terms of not only performance but also different aspects of converged solutions.},
	journaltitle = {{arXiv}:2006.00555 [cs, stat]},
	author = {Abnar, Samira and Dehghani, Mostafa and Zuidema, Willem},
	urldate = {2020-09-08},
	date = {2020-06-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.00555},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{noauthor_abstract_nodate,
	title = {Abstract categories or limited-scope formulae? The case of children's determiners - {ProQuest}},
	url = {https://search.proquest.com/docview/221327053?fromopenview=true&pq-origsite=gscholar},
	shorttitle = {Abstract categories or limited-scope formulae?},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the {ProQuest} Platform.},
	urldate = {2020-09-04},
	langid = {english},
}

@article{goodfellow_qualitatively_2015,
	title = {Qualitatively characterizing neural network optimization problems},
	url = {http://arxiv.org/abs/1412.6544},
	abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
	journaltitle = {{arXiv}:1412.6544 [cs, stat]},
	author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
	urldate = {2020-09-02},
	date = {2015-05-21},
	eprinttype = {arxiv},
	eprint = {1412.6544},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{manning_emergent_2020,
	title = {Emergent linguistic structure in artificial neural networks trained by self-supervision},
	rights = {© 2020 . https://www.pnas.org/site/aboutpnas/licenses.{xhtmlPublished} under the {PNAS} license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/early/2020/06/02/1907367117},
	doi = {10.1073/pnas.1907367117},
	abstract = {This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
	urldate = {2020-08-31},
	date = {2020-06-03},
	langid = {english},
	pmid = {32493748},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {artificial neural netwok, learning, self-supervision, syntax},
}

@inproceedings{neyshabur_what_2020,
	title = {What is being transferred in transfer learning?},
	abstract = {One desired capability for machines is the ability to transfer their knowledge of one domain to another where data is (usually) scarce. Despite ample adaptation of transfer learning in various deep learning applications, we yet do not understand what enables a successful transfer and which part of the network is responsible for that. In this paper, we provide new tools and analyses to address these fundamental questions. Through a series of analyses on transferring to block-shuffled images, we separate the effect of feature reuse from learning low-level statistics of data and show that some benefit of transfer learning comes from the latter. We present that when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.},
	author = {Neyshabur, Behnam and Sedghi, H. and Zhang, Chiyuan},
	date = {2020},
}

@inproceedings{agarwal_estimating_2020,
	title = {Estimating Example Difficulty using Variance of Gradients},
	abstract = {In machine learning, a question of great interest is understanding what examples are challenging for a model to classify. Identifying atypical examples helps inform safe deployment of models, isolates examples that require further human inspection, and provides interpretability into model behavior. In this work, we propose Variance of Gradients (V {OG}) as a proxy metric for detecting outliers in the data distribution. We provide quantitative and qualitative support that V {OG} is a meaningful way to rank data by difficulty and to surface a tractable subset of the most challenging examples for human-in-the-loop auditing. Data points with high V {OG} scores are more difficult for the model to classify and over-index on examples that require memorization.},
	author = {Agarwal, Chirag and Hooker, Sara},
	date = {2020},
}

@article{zhu_understanding_2020,
	title = {Understanding Learning Dynamics for Neural Machine Translation},
	url = {http://arxiv.org/abs/2004.02199},
	abstract = {Despite the great success of {NMT}, there still remains a severe challenge: it is hard to interpret the internal dynamics during its training process. In this paper we propose to understand learning dynamics of {NMT} by using a recent proposed technique named Loss Change Allocation ({LCA}) (Lan et al., 2019). As {LCA} requires calculating the gradient on an entire dataset for each update, we instead present an approximate to put it into practice in {NMT} scenario. Our simulated experiment shows that such approximate calculation is efﬁcient and is empirically proved to deliver consistent results to the brute-force implementation. In particular, extensive experiments on two standard translation benchmark datasets reveal some valuable ﬁndings.},
	journaltitle = {{arXiv}:2004.02199 [cs]},
	author = {Zhu, Conghui and Li, Guanlin and Liu, Lemao and Zhao, Tiejun and Shi, Shuming},
	urldate = {2020-08-24},
	date = {2020-04-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.02199},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{nielsen_elementary_2018,
	title = {An elementary introduction to information geometry},
	abstract = {We describe the fundamental diﬀerential-geometric structures of information manifolds, state the fundamental theorem of information geometry, and illustrate some uses of these information manifolds in information sciences. The exposition is self-contained by concisely introducing the necessary concepts of diﬀerential geometry with proofs omitted for brevity.},
	author = {Nielsen, Frank},
	date = {2018-08-28},
}

@article{wadia_whitening_2020,
	title = {Whitening and second order optimization both destroy information about the dataset, and can make generalization impossible},
	url = {http://arxiv.org/abs/2008.07545},
	abstract = {Machine learning is predicated on the concept of generalization: a model achieving low error on a sufficiently large training set should also perform well on novel samples from the same distribution. We show that both data whitening and second order optimization can harm or entirely prevent generalization. In general, model training harnesses information contained in the sample-sample second moment matrix of a dataset. We prove that for models with a fully connected first layer, the information contained in this matrix is the only information which can be used to generalize. Models trained using whitened data, or with certain second order optimization schemes, have less access to this information; in the high dimensional regime they have no access at all, producing models that generalize poorly or not at all. We experimentally verify these predictions for several architectures, and further demonstrate that generalization continues to be harmed even when theoretical requirements are relaxed. However, we also show experimentally that regularized second order optimization can provide a practical tradeoff, where training is still accelerated but less information is lost, and generalization can in some circumstances even improve.},
	journaltitle = {{arXiv}:2008.07545 [cs, stat]},
	author = {Wadia, Neha S. and Duckworth, Daniel and Schoenholz, Samuel S. and Dyer, Ethan and Sohl-Dickstein, Jascha},
	urldate = {2020-08-22},
	date = {2020-08-17},
	eprinttype = {arxiv},
	eprint = {2008.07545},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sokol_information_2020,
	title = {{INFORMATION} {GEOMETRY} {OF} {ORTHOGONAL} {INITIALIZATIONS} {AND} {TRAINING}},
	abstract = {Recently mean ﬁeld theory has been successfully used to analyze properties of wide, random neural networks. It gave rise to a prescriptive theory for initializing feed-forward neural networks with orthogonal weights, which ensures that both the forward propagated activations and the backpropagated gradients are near 2 isometries and as a consequence training is orders of magnitude faster. Despite strong empirical performance, the mechanisms by which critical initializations confer an advantage in the optimization of deep neural networks are poorly understood. Here we show a novel connection between the maximum curvature of the optimization landscape (gradient smoothness) as measured by the Fisher information matrix ({FIM}) and the spectral radius of the input-output Jacobian, which partially explains why more isometric networks can train much faster. Furthermore, given that orthogonal weights are necessary to ensure that gradient norms are approximately preserved at initialization, we experimentally investigate the beneﬁts of maintaining orthogonality throughout training, and we conclude that manifold optimization of weights performs well regardless of the smoothness of the gradients. Moreover, we observe a surprising yet robust behavior of highly isometric initializations — even though such networks have a lower {FIM} condition number at initialization, and therefore by analogy to convex functions should be easier to optimize, experimentally they prove to be much harder to train with stochastic gradient descent. We conjecture the {FIM} condition number plays a non-trivial role in the optimization.},
	pages = {17},
	author = {Sokół, Piotr Aleksander and Park, Il Memming},
	date = {2020},
	langid = {english},
}

@article{popel_training_2018,
	title = {Training Tips for the Transformer Model},
	volume = {110},
	issn = {1804-0462},
	url = {http://arxiv.org/abs/1804.00247},
	doi = {10.2478/pralin-2018-0002},
	abstract = {This article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We examine some of the critical parameters that aﬀect the ﬁnal translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to conﬁrming the general mantra “more data and larger models”, we address scaling to multiple {GPUs} and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.},
	pages = {43--70},
	number = {1},
	journaltitle = {The Prague Bulletin of Mathematical Linguistics},
	author = {Popel, Martin and Bojar, Ondřej},
	urldate = {2020-08-11},
	date = {2018-04-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.00247},
	keywords = {Computer Science - Computation and Language},
}

@article{suzgun_evaluating_2018,
	title = {On Evaluating the Generalization of {LSTM} Models in Formal Languages},
	url = {http://arxiv.org/abs/1811.01001},
	abstract = {Recurrent Neural Networks ({RNNs}) are theoretically Turing-complete and established themselves as a dominant model for language processing. Yet, there still remains an uncertainty regarding their language learning capabilities. In this paper, we empirically evaluate the inductive learning capabilities of Long Short-Term Memory networks, a popular extension of simple {RNNs}, to learn simple formal languages, in particular \$a{\textasciicircum}nb{\textasciicircum}n\$, \$a{\textasciicircum}nb{\textasciicircum}nc{\textasciicircum}n\$, and \$a{\textasciicircum}nb{\textasciicircum}nc{\textasciicircum}nd{\textasciicircum}n\$. We investigate the influence of various aspects of learning, such as training data regimes and model capacity, on the generalization to unobserved samples. We find striking differences in model performances under different training settings and highlight the need for careful analysis and assessment when making claims about the learning capabilities of neural network models.},
	journaltitle = {{arXiv}:1811.01001 [cs]},
	author = {Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M.},
	urldate = {2020-08-11},
	date = {2018-11-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1811.01001},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, F.4.3, I.2.6, I.2.7},
}

@article{gotmare_closer_2018,
	title = {A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation},
	url = {http://arxiv.org/abs/1810.13243},
	shorttitle = {A Closer Look at Deep Learning Heuristics},
	abstract = {The convergence rate and ﬁnal performance of common deep learning models have signiﬁcantly beneﬁted from heuristics such as learning rate schedules, knowledge distillation, skip connections, and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit such analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz., mode connectivity and canonical correlation analysis ({CCA}), and hypothesize reasons for the success of the heuristics. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectivity and {CCA}. Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed to the deeper layers.},
	journaltitle = {{arXiv}:1810.13243 [cs, stat]},
	author = {Gotmare, Akhilesh and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
	urldate = {2020-08-11},
	date = {2018-10-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.13243},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{couillet_word_nodate,
	title = {Word Representations Concentrate and This is Good News!},
	abstract = {This article establishes that, unlike the legacy tf*idf representation, modern natural language representations (word embedding vectors) exhibit a so-called concentration of measure phenomenon, in the sense that, as the representation size p and database size n are both large, their behavior is similar to that of large dimensional Gaussian random vectors. This phenomenon has profound consequences: machine learning algorithms for natural language data become tractable and amenable to improvement, thereby establishing ﬁrst solid theoretical results in the ﬁeld of natural language processing.},
	pages = {10},
	author = {Couillet, Romain and Cinar, Yagmur Gizem and Gaussier, Eric and Imran, Muhammad},
	langid = {english},
}

@article{mccoy_rnns_2019,
	title = {{RNNs} Implicitly Implement Tensor Product Representations},
	url = {http://arxiv.org/abs/1812.08718},
	abstract = {Recurrent neural networks ({RNNs}) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that {RNNs} that show such regularities implicitly compile symbolic structures into tensor product representations ({TPRs}; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing ﬁllers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks ({TPDNs}), which use {TPRs} to approximate existing vector representations. We demonstrate using synthetic data that {TPDNs} can successfully approximate linear and tree-based {RNN} autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead {RNNs} to induce such structuresensitive representations. By contrast, further {TPDN} experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag of words, with only marginal improvements from more sophisticated structures. We conclude that {TPDNs} provide a powerful method for interpreting vector representations, and that standard {RNNs} can induce compositional sequence representations that are remarkably well approximated by {TPRs}; at the same time, existing training tasks for sentence representation learning may not be sufﬁcient for inducing robust structural representations.},
	journaltitle = {{arXiv}:1812.08718 [cs]},
	author = {{McCoy}, R. Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
	urldate = {2020-07-28},
	date = {2019-03-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1812.08718},
	keywords = {Computer Science - Computation and Language},
}

@article{mccoy_does_2020,
	title = {Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks},
	url = {http://arxiv.org/abs/2001.03632},
	shorttitle = {Does syntax need to grow on trees?},
	abstract = {Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, {LSTMs} and {GRUs} displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure.},
	journaltitle = {{arXiv}:2001.03632 [cs]},
	author = {{McCoy}, R. Thomas and Frank, Robert and Linzen, Tal},
	urldate = {2020-07-27},
	date = {2020-01-10},
	eprinttype = {arxiv},
	eprint = {2001.03632},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{ramasesh_anatomy_2020,
	title = {Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics},
	shorttitle = {Anatomy of Catastrophic Forgetting},
	abstract = {A central challenge in developing versatile machine learning systems is catastrophic forgetting — a model trained on tasks in sequence will suffer significant performance drops on earlier tasks. Despite the ubiquity of catastrophic forgetting, there is limited understanding of the underlying process and its causes. In this paper, we address this important knowledge gap, investigating how forgetting affects representations in neural network models. Through representational analysis techniques, we find that deeper layers are disproportionately the source of forgetting. Supporting this, a study of methods to mitigate forgetting illustrates that they act to stabilize deeper layers. These insights enable the development of an analytic argument and empirical picture relating the degree of forgetting to representational similarity between tasks. Consistent with this picture, we observe maximal forgetting occurs for task sequences with intermediate similarity. We perform empirical studies on the standard split {CIFAR}-10 setup and also introduce a novel {CIFAR}-100 based task approximating realistic input distribution shift.},
	author = {Ramasesh, Vinay V. and Dyer, Ethan and Raghu, Maithra},
	date = {2020},
}
@inproceedings{hupkes_compositionality_2020,
	location = {Yokohama, Japan},
	title = {Compositionality Decomposed: How do Neural Networks Generalise? (Extended Abstract)},
	isbn = {978-0-9992411-6-5},
	url = {https://www.ijcai.org/proceedings/2020/708},
	doi = {10.24963/ijcai.2020/708},
	shorttitle = {Compositionality Decomposed},
	eventtitle = {Twenty-Ninth International Joint Conference on Artificial Intelligence and Seventeenth Pacific Rim International Conference on Artificial Intelligence \{{IJCAI}-{PRICAI}-20\}},
	pages = {5065--5069},
	booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
	urldate = {2020-07-16},
	date = {2020-07},
	langid = {english},
}

@article{alain_understanding_2018,
	title = {Understanding intermediate layers using linear classifier probes},
	url = {http://arxiv.org/abs/1610.01644},
	abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
	journaltitle = {{arXiv}:1610.01644 [cs, stat]},
	author = {Alain, Guillaume and Bengio, Yoshua},
	urldate = {2020-07-15},
	date = {2018-11-22},
	eprinttype = {arxiv},
	eprint = {1610.01644},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{demeter_stolen_nodate,
	title = {Stolen Probability: A Structural Weakness of Neural Language Models},
	pages = {7},
	author = {Demeter, David and Kimmel, Gregory and Downey, Doug},
	langid = {english},
}

@article{raj_understanding_2020,
	title = {Understanding Learning Dynamics of Binary Neural Networks via Information Bottleneck},
	url = {http://arxiv.org/abs/2006.07522},
	abstract = {Compact neural networks are essential for aﬀordable and power eﬃcient deep learning solutions. Binary Neural Networks ({BNNs}) take compactiﬁcation to the extreme by constraining both weights and activations to two levels, \{+1, −1\}. However, training {BNNs} are not easy due to the discontinuity in activation functions, and the training dynamics of {BNNs} is not well understood. In this paper, we present an information-theoretic perspective of {BNN} training. We analyze {BNNs} through the Information Bottleneck principle and observe that the training dynamics of {BNNs} is considerably diﬀerent from that of Deep Neural Networks ({DNNs}). While {DNNs} have a separate empirical risk minimization and representation compression phases, our numerical experiments show that in {BNNs}, both these phases are simultaneous. Since {BNNs} have a less expressive capacity, they tend to ﬁnd eﬃcient hidden representations concurrently with label ﬁtting. Experiments in multiple datasets support these observations, and we see a consistent behavior across diﬀerent activation functions in {BNNs}.},
	journaltitle = {{arXiv}:2006.07522 [cs, stat]},
	author = {Raj, Vishnu and Nayak, Nancy and Kalyani, Sheetal},
	urldate = {2020-07-13},
	date = {2020-06-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.07522},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{pellegrini_analytic_2020,
	title = {An analytic theory of shallow networks dynamics for hinge loss classification},
	url = {http://arxiv.org/abs/2006.11209},
	abstract = {Neural networks have been shown to perform incredibly well in classiﬁcation tasks over structured high-dimensional datasets. However, the learning dynamics of such networks is still poorly understood. In this paper we study in detail the training dynamics of a simple type of neural network: a single hidden layer trained to perform a classiﬁcation task. We show that in a suitable mean-ﬁeld limit this case maps to a single-node learning problem with a time-dependent dataset determined self-consistently from the average nodes population. We specialize our theory to the prototypical case of a linearly separable dataset and a linear hinge loss, for which the dynamics can be explicitly solved. This allow us to address in a simple setting several phenomena appearing in modern networks such as slowing down of training dynamics, crossover between rich and lazy learning, and overﬁtting. Finally, we asses the limitations of mean-ﬁeld theory by studying the case of large but ﬁnite number of nodes and of training samples.},
	journaltitle = {{arXiv}:2006.11209 [cond-mat, stat]},
	author = {Pellegrini, Franco and Biroli, Giulio},
	urldate = {2020-07-13},
	date = {2020-06-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.11209},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
}

@article{sommerauer_why_nodate,
	title = {Why is penguin more similar to polar bear than to sea gull? Analyzing conceptual knowledge in distributional models},
	abstract = {What do powerful models of word meaning created from distributional data (e.g. Word2vec (Mikolov et al., 2013) {BERT} (Devlin et al., 2019) and {ELMO} (Peters et al., 2018)) represent? What causes words to be similar in the semantic space? What type of information is lacking? This thesis proposal presents a framework for investigating the information encoded in distributional semantic models. Several analysis methods have been suggested, but they have been shown to be limited and are not well understood. This approach pairs observations made on actual corpora with insights obtained from data manipulation experiments. The expected outcome is a better understanding of (1) the semantic information we can infer purely based on linguistic co-occurrence patterns and (2) the potential of distributional semantic models to pick up linguistic evidence.},
	pages = {9},
	author = {Sommerauer, Pia},
	langid = {english},
}

@article{zhao_how_nodate,
	title = {How does {BERT}'s attention change when you fine-tune? An analysis methodology and a case study in negation scope},
	abstract = {Large pretrained language models like {BERT}, after ﬁne-tuning to a downstream task, have achieved high performance on a variety of {NLP} problems. Yet explaining their decisions is difﬁcult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. We apply this methodology to test {BERT} and {RoBERTa} on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. We ﬁnd that after ﬁne-tuning {BERT} and {RoBERTa} on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.},
	pages = {19},
	author = {Zhao, Yiyun and Bethard, Steven},
	langid = {english},
}

@article{jo_roles_nodate,
	title = {Roles and Utilization of Attention Heads in Transformer-based Neural Language Models},
	abstract = {Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture. However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformerbased model. For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families ({GPT}, {GPT}2, {BERT}, and {ELECTRA}). Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most inﬂuential attention heads, resulting in additional performance improvements on the downstream tasks.},
	pages = {14},
	author = {Jo, Jae-young and Myaeng, Sung-Hyon},
	langid = {english},
}

@article{zhang_inducing_nodate,
	title = {Inducing Grammar from Long Short-Term Memory Networks by Shapley Decomposition},
	pages = {7},
	author = {Zhang, Yuhui and Nie, Allen},
	langid = {english},
}

@article{nguyen_transformers_2019,
	title = {Transformers without Tears: Improving the Normalization of Self-Attention},
	url = {http://arxiv.org/abs/1910.05895},
	doi = {10.5281/zenodo.3525484},
	shorttitle = {Transformers without Tears},
	abstract = {We evaluate three simple, normalizationcentric changes to improve Transformer training. First, we show that pre-norm residual connections ({PRENORM}) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose 2 normalization with a single scale parameter ({SCALENORM}) for faster training and better performance. Finally, we reafﬁrm the effectiveness of normalizing word embeddings to a ﬁxed length ({FIXNORM}). On ﬁve low-resource translation pairs from {TED} Talks-based corpora, these changes always converge, giving an average +1.1 {BLEU} over state-of-the-art bilingual baselines and a new 32.8 {BLEU} on {IWSLT} '15 {EnglishVietnamese}. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the highresource setting ({WMT} '14 English-German), {SCALENORM} and {FIXNORM} remain competitive but {PRENORM} degrades performance. Preprocessing scripts and code are released at https://github.com/tnq177/ transformers\_without\_tears.},
	journaltitle = {{arXiv}:1910.05895 [cs, stat]},
	author = {Nguyen, Toan Q. and Salazar, Julian},
	urldate = {2020-07-13},
	date = {2019-11-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.05895},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inbook{liu_representation_2020,
	location = {Singapore},
	title = {Representation Learning and {NLP}},
	isbn = {9789811555725 9789811555732},
	url = {http://link.springer.com/10.1007/978-981-15-5573-2_1},
	abstract = {Natural languages are typical unstructured information. Conventional Natural Language Processing ({NLP}) heavily relies on feature engineering, which requires careful design and considerable expertise. Representation learning aims to learn representations of raw data as useful information for further classiﬁcation or prediction. This chapter presents a brief introduction to representation learning, including its motivation and basic idea, and also reviews its history and recent advances in both machine learning and {NLP}.},
	pages = {1--11},
	booktitle = {Representation Learning for Natural Language Processing},
	publisher = {Springer Singapore},
	author = {Liu, Zhiyuan and Lin, Yankai and Sun, Maosong},
	bookauthor = {Liu, Zhiyuan and Lin, Yankai and Sun, Maosong},
	urldate = {2020-07-12},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-981-15-5573-2_1},
}

@inproceedings{cotterell_algerian_2014,
	title = {An algerian arabic-french code-switched corpus},
	pages = {34},
	booktitle = {Workshop on free/open-source Arabic corpora and corpora processing tools workshop programme},
	author = {Cotterell, Ryan and Renduchintala, Adithya and Saphra, Naomi and Callison-Burch, Chris},
	date = {2014},
}

@article{saphra_word_2020,
	title = {Word Interdependence Exposes How {LSTMs} Compose Representations},
	url = {http://arxiv.org/abs/2004.13195},
	abstract = {Recent work in {NLP} shows that {LSTM} language models capture compositional structure in language data. For a closer look at how these representations are composed hierarchically, we present a novel measure of interdependence between word meanings in an {LSTM}, based on their interactions at the internal gates. To explore how compositional representations arise over training, we conduct simple experiments on synthetic data, which illustrate our measure by showing how high interdependence can hurt generalization. These synthetic experiments also illustrate a specific hypothesis about how hierarchical structures are discovered over the course of training: that parent constituents rely on effective representations of their children, rather than on learning long-range relations independently. We further support this measure with experiments on English language data, where interdependence is higher for more closely syntactically linked word pairs.},
	journaltitle = {{arXiv}:2004.13195 [cs, stat]},
	author = {Saphra, Naomi and Lopez, Adam},
	urldate = {2020-07-10},
	date = {2020-04-27},
	eprinttype = {arxiv},
	eprint = {2004.13195},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{tahaei_understanding_2020,
	location = {Honolulu, {HI}, {USA}},
	title = {Understanding Privacy-Related Questions on Stack Overflow},
	isbn = {978-1-4503-6708-0},
	url = {https://doi.org/10.1145/3313831.3376768},
	doi = {10.1145/3313831.3376768},
	series = {{CHI} '20},
	abstract = {We analyse Stack Overflow ({SO}) to understand challenges and confusions developers face while dealing with privacy-related topics. We apply topic modelling techniques to 1,733 privacy-related questions to identify topics and then qualitatively analyse a random sample of 315 privacy-related questions. Identified topics include privacy policies, privacy concerns, access control, and version changes. Results show that developers do ask {SO} for support on privacy-related issues. We also find that platforms such as Apple and Google are defining privacy requirements for developers by specifying what "sensitive" information is and what types of information developers need to communicate to users (e.g. privacy policies). We also examine the accepted answers in our sample and find that 28\% of them link to official documentation and more than half are answered by {SO} users without references to any external resources.},
	pages = {1--14},
	booktitle = {Proceedings of the 2020 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Tahaei, Mohammad and Vaniea, Kami and Saphra, Naomi},
	urldate = {2020-07-10},
	date = {2020-04-21},
	keywords = {software developers, stack overflow, usable privacy},
}

@article{katharopoulos_transformers_2020,
	title = {Transformers are {RNNs}: Fast Autoregressive Transformers with Linear Attention},
	url = {http://arxiv.org/abs/2006.16236},
	shorttitle = {Transformers are {RNNs}},
	abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textasciicircum}2{\textbackslash}right)\$ to \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textbackslash}right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
	journaltitle = {{arXiv}:2006.16236 [cs, stat]},
	author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
	urldate = {2020-07-08},
	date = {2020-06-30},
	eprinttype = {arxiv},
	eprint = {2006.16236},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lundberg_consistent_2019,
	title = {Consistent Individualized Feature Attribution for Tree Ensembles},
	url = {http://arxiv.org/abs/1802.03888},
	abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for {SHAP} ({SHapley} Additive {exPlanation}) values, which are the unique consistent and locally accurate attribution values. We then extend {SHAP} values to interaction effects and define {SHAP} interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into {XGBoost} and {LightGBM}, see http://github.com/slundberg/shap for details.},
	journaltitle = {{arXiv}:1802.03888 [cs, stat]},
	author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
	urldate = {2020-07-07},
	date = {2019-03-06},
	eprinttype = {arxiv},
	eprint = {1802.03888},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{vu_exploring_2020,
	title = {Exploring and Predicting Transferability across {NLP} Tasks},
	url = {http://arxiv.org/abs/2005.00770},
	abstract = {Recent advances in {NLP} demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 {NLP} tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even when the source task is small or differs substantially from the target task (e.g., part-of-speech tagging transfers well to the {DROP} {QA} dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as source data size, task and domain similarity, and task complexity all play a role in determining transferability.},
	journaltitle = {{arXiv}:2005.00770 [cs]},
	author = {Vu, Tu and Wang, Tong and Munkhdalai, Tsendsuren and Sordoni, Alessandro and Trischler, Adam and Mattarella-Micke, Andrew and Maji, Subhransu and Iyyer, Mohit},
	urldate = {2020-07-07},
	date = {2020-05-02},
	eprinttype = {arxiv},
	eprint = {2005.00770},
	keywords = {Computer Science - Computation and Language},
}

@article{davis_recurrent_nodate,
	title = {Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment},
	abstract = {A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in {RNN} {LMs} advantageously overlap with syntactic structure in English but not Spanish. Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences. We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.},
	pages = {12},
	author = {Davis, Forrest},
	langid = {english},
}

@article{wu_perturbed_nodate,
	title = {Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting {BERT}},
	abstract = {By introducing a small set of additional parameters, a probe learns to solve speciﬁc linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., {BERT}). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on {BERT} show that syntactic trees recovered from {BERT} using our method are signiﬁcantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classiﬁcation task and ﬁnd its improvement compatible with or even superior to a human-designed dependency schema.},
	pages = {11},
	author = {Wu, Zhiyong and Chen, Yun and Kao, Ben and Liu, Qun},
	langid = {english},
}

@article{lan_spontaneous_nodate,
	title = {On the Spontaneous Emergence of Discrete and Compositional Signals},
	abstract = {We propose a general framework to study language emergence through signaling games with neural agents. Using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge. We explore whether categorical perception effects follow and show that the messages are not compositional.},
	pages = {7},
	author = {Lan, Nur Geffen and Chemla, Emmanuel and Steinert-Threlkeld, Shane},
	langid = {english},
}

@article{vazquez_systematic_2020,
	title = {A Systematic Study of Inner-Attention-Based Sentence Representations in Multilingual Neural Machine Translation},
	volume = {46},
	issn = {0891-2017, 1530-9312},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/coli_a_00377},
	doi = {10.1162/coli_a_00377},
	pages = {387--424},
	number = {2},
	journaltitle = {Computational Linguistics},
	author = {Vázquez, Raúl and Raganato, Alessandro and Creutz, Mathias and Tiedemann, Jörg},
	urldate = {2020-07-04},
	date = {2020-06},
	langid = {english},
}

@article{mohankumar_towards_nodate,
	title = {Towards Transparent and Explainable Attention Models},
	pages = {11},
	author = {Mohankumar, Akash Kumar and Nema, Preksha and Narasimhan, Sharan and Khapra, Mitesh M and Srinivasan, Balaji Vasan and Ravindran, Balaraman},
	langid = {english},
}

@article{jo_roles_nodate-1,
	title = {Roles and Utilization of Attention Heads in Transformer-based Neural Language Models},
	abstract = {Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture. However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformerbased model. For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families ({GPT}, {GPT}2, {BERT}, and {ELECTRA}). Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most inﬂuential attention heads, resulting in additional performance improvements on the downstream tasks.},
	pages = {14},
	author = {Jo, Jae-young and Myaeng, Sung-Hyon},
	langid = {english},
}

@online{abnar_quantifying_nodate,
	title = {Quantifying Attention Flow in Transformers {\textbar} Samira Abnar},
	url = {https://samiraabnar.github.io/articles/2020-04/attention_flow},
	abstract = {In this post, I explain two techniques for visualising attention that address the problem of lack of token identifiability in higher layers of Transformers w...},
	author = {Abnar, Quantifying Attention Flow in Transformers \{{\textbackslash}textbar\} Samira},
	urldate = {2020-07-04},
	langid = {english},
	note = {Library Catalog: samiraabnar.github.io},
}

@article{abnar_quantifying_2020,
	title = {Quantifying Attention Flow in Transformers},
	url = {http://arxiv.org/abs/2005.00928},
	abstract = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
	journaltitle = {{arXiv}:2005.00928 [cs]},
	author = {Abnar, Samira and Zuidema, Willem},
	urldate = {2020-07-04},
	date = {2020-05-31},
	eprinttype = {arxiv},
	eprint = {2005.00928},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{giulianelli_analysing_2020,
	title = {Analysing Lexical Semantic Change with Contextualised Word Representations},
	url = {http://arxiv.org/abs/2004.14118},
	abstract = {This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the {BERT} neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.},
	journaltitle = {{arXiv}:2004.14118 [cs]},
	author = {Giulianelli, Mario and Del Tredici, Marco and Fernández, Raquel},
	urldate = {2020-07-04},
	date = {2020-04-29},
	eprinttype = {arxiv},
	eprint = {2004.14118},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@article{dubois_location_2020,
	title = {Location Attention for Extrapolation to Longer Sequences},
	url = {http://arxiv.org/abs/1911.03872},
	abstract = {Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we ﬁrst review the notion of extrapolation, why it is important, and how one could hope to tackle it. We then focus on a speciﬁc type of extrapolation, which is especially useful for natural language processing: generalization to sequences longer than those seen during training. We hypothesize that models with a separate contentand location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.},
	journaltitle = {{arXiv}:1911.03872 [cs, stat]},
	author = {Dubois, Yann and Dagan, Gautier and Hupkes, Dieuwke and Bruni, Elia},
	urldate = {2020-07-04},
	date = {2020-04-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1911.03872},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{merchant_what_2020,
	title = {What Happens To {BERT} Embeddings During Fine-tuning?},
	url = {http://arxiv.org/abs/2004.14448},
	abstract = {While there has been much recent work studying how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis techniques (probing classiﬁers, Representational Similarity Analysis, and model ablations), we investigate how ﬁne-tuning affects the representations of the {BERT} model. We ﬁnd that while ﬁne-tuning necessarily makes signiﬁcant changes, it does not lead to catastrophic forgetting of linguistic phenomena. We instead ﬁnd that ﬁne-tuning primarily affects the top layers of {BERT}, but with noteworthy variation across tasks. In particular, dependency parsing reconﬁgures most of the model, whereas {SQuAD} and {MNLI} appear to involve much shallower processing. Finally, we also ﬁnd that ﬁne-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.},
	journaltitle = {{arXiv}:2004.14448 [cs]},
	author = {Merchant, Amil and Rahimtoroghi, Elahe and Pavlick, Ellie and Tenney, Ian},
	urldate = {2020-07-04},
	date = {2020-04-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.14448},
	keywords = {Computer Science - Computation and Language},
}

@article{chi_finding_nodate,
	title = {Finding Universal Grammatical Relations in Multilingual {BERT}},
	abstract = {Recent work has found evidence that Multilingual {BERT} ({mBERT}), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on ﬁnding syntactic trees in neural networks’ internal representations to the multilingual setting. We show that subspaces of {mBERT} representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence {mBERT} learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.},
	pages = {14},
	author = {Chi, Ethan A and Hewitt, John and Manning, Christopher D},
	langid = {english},
}

@article{kulmizev_neural_nodate,
	title = {Do Neural Language Models Show Preferences for Syntactic Formalisms?},
	abstract = {Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages. We apply a probe for extracting directed dependency trees to {BERT} and {ELMo} models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies ({UD}), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies ({SUD}), focusing on surface structure. We ﬁnd that both models exhibit a preference for {UD} over {SUD} — with interesting variations across languages and layers — and that the strength of this preference is correlated with differences in tree shape.},
	pages = {15},
	author = {Kulmizev, Artur and Ravishankar, Vinit and Abdou, Mostafa and Nivre, Joakim},
	langid = {english},
}

@inproceedings{vazquez_systematic_2020-1,
	title = {A Systematic Study of Inner-Attention-Based Sentence Representations in Multilingual Neural Machine Translation},
	url = {https://virtual.acl2020.org/paper_cl.1552.html},
	abstract = {Neural machine translation has considerably improved the quality of automatic translations by learning good representations of input sentences. In this article, we explore a multilingual translation model capable of producing fixed-size sentence representations by incorporating an intermediate crosslingual shared layer, which we refer to as attention bridge. This layer exploits the semantics from each language and develops into a language-agnostic meaning representation that can be efficiently used for transfer learning.We systematically study the impact of the size of the attention bridge and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that there is no conflict between translation performance and the use of sentence representations in downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. Nevertheless, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. Similarly, we show that trainable downstream tasks benefit from multilingual models, whereas additional language signals do not improve performance in non-trainable benchmarks. This is an important insight that helps to properly design models for specific applications. Finally, we also include an in-depth analysis of the proposed attention bridge and its ability of encoding linguistic properties. We carefully analyze the information that is captured by individual attention heads and identify interesting patterns that explain the performance of specific settings in linguistic probing tasks.},
	eventtitle = {The 58th Annual Meeting Of The Association For Computational Linguistics},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	author = {Vázquez, Raúl and Raganato, Alessandro and Creutz, Mathias and Tiedemann, Jörg},
	urldate = {2020-07-04},
	date = {2020-07},
	langid = {english},
}

@inproceedings{bommasani_interpreting_2020,
	title = {Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings},
	url = {https://virtual.acl2020.org/paper_main.431.html},
	abstract = {Contextualized representations (e.g. {ELMo}, {BERT}) have become the default pretrained representations for downstream {NLP} applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, {GloVe}) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings --- while more diverse and mature than those available for their dynamic counterparts --- are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.},
	eventtitle = {The 58th Annual Meeting Of The Association For Computational Linguistics},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	author = {Bommasani, Rishi and Davis, Kelly and Cardie, Claire},
	urldate = {2020-07-04},
	date = {2020-07},
	langid = {english},
}

@inproceedings{belinkov_linguistic_2020,
	title = {On the Linguistic Representational Power of Neural Machine Translation Models},
	url = {https://virtual.acl2020.org/paper_cl.1482.html},
	abstract = {Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation ({NMT}) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual {NMT} models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in {NMT} models and their ability to capture various linguistic phenomena. We show that deep {NMT} models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.},
	eventtitle = {The 58th Annual Meeting Of The Association For Computational Linguistics},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	author = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
	urldate = {2020-07-04},
	date = {2020-07},
	langid = {english},
}

@article{belinkov_interpretability_nodate,
	title = {Interpretability and Analysis in Neural {NLP}},
	abstract = {While deep learning has transformed the natural language processing ({NLP}) ﬁeld and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of neural network models in {NLP}.},
	pages = {5},
	author = {Belinkov, Yonatan and Gehrmann, Sebastian and Pavlick, Ellie},
	langid = {english},
}

@article{htut_attention_2019,
	title = {Do Attention Heads in {BERT} Track Syntactic Dependencies?},
	url = {http://arxiv.org/abs/1911.12246},
	abstract = {We investigate the extent to which individual attention heads in pretrained transformer language models, such as {BERT} and {RoBERTa}, implicitly capture syntactic dependency relations. We employ two methods—taking the maximum attention weight and computing the maximum spanning tree—to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency ({UD}) trees. We show that, for some {UD} relation types, there exist heads that can recover the dependency type signiﬁcantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze {BERT} ﬁne-tuned on two datasets—the syntaxoriented {CoLA} and the semantics-oriented {MNLI}—to investigate whether ﬁne-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing signiﬁcantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that {BERT}-style models are known to learn.},
	journaltitle = {{arXiv}:1911.12246 [cs]},
	author = {Htut, Phu Mon and Phang, Jason and Bordia, Shikha and Bowman, Samuel R.},
	urldate = {2020-06-30},
	date = {2019-11-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1911.12246},
	keywords = {Computer Science - Computation and Language},
}

@article{linzen_syntactic_nodate,
	title = {Syntactic Structure from Deep Learning},
	abstract = {Modern deep neural networks achieve impressive performance in engineering applications that require extensive linguistic skills, such as machine translation. This success has sparked interest in probing whether these models are inducing human-like grammatical knowledge from the raw data they are exposed to, and, consequently, whether they can shed new light on long-standing debates concerning the innate structure necessary for language acquisition. In this article, we survey representative studies of the syntactic abilities of deep networks, and discuss the broader implications that this work has for theoretical linguistics.},
	pages = {19},
	author = {Linzen, Tal and Baroni, Marco},
	langid = {english},
}

@inproceedings{gain_relating_2019,
	title = {Relating information complexity and training in deep neural networks},
	volume = {10982},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10982/109822H/Relating-information-complexity-and-training-in-deep-neural-networks/10.1117/12.2520172.short},
	doi = {10.1117/12.2520172},
	abstract = {Deep Neural Networks may be costly to train, and if testing error is too large, retraining may be required, unless lifelong learning methods are applied. Crucial to addressing learning at the edge, without access to powerful cloud computing, is the notion of problem difficulty for non-standard data domains. While it is known that training is harder for classes that are more entangled, the complexity of data points was not previously studied as an important contributor to training dynamics. We analyze data points by their information complexity and relate the complexity of the data to the test error. We elucidate training dynamics of {DNNs}, demonstrating that high complexity datapoints contribute to the error of the network, and that training {DNNs} consist of two important aspects - (1) Minimization of error due to high complexity datapoints, and (2) Margin decrease where entanglement of classes occurs. Whereas data complexity may be ignored when training in a cloud, it must be considered as part of the setting when training at the edge.},
	eventtitle = {Micro- and Nanotechnology Sensors, Systems, and Applications {XI}},
	pages = {109822H},
	booktitle = {Micro- and Nanotechnology Sensors, Systems, and Applications {XI}},
	publisher = {International Society for Optics and Photonics},
	author = {Gain, Alex and Siegelmann, Hava},
	urldate = {2020-06-30},
	date = {2019-05-13},
}

@article{xia_predicting_nodate,
	title = {Predicting Performance for Natural Language Processing Tasks},
	pages = {22},
	author = {Xia, Mengzhou and Anastasopoulos, Antonios and Xu, Ruochen and Yang, Yiming and Neubig, Graham},
	langid = {english},
}

@article{henderson_unstoppable_2020,
	title = {The Unstoppable Rise of Computational Linguistics in Deep Learning},
	url = {http://arxiv.org/abs/2005.06420},
	abstract = {In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures. We focus on the importance of variable binding and its instantiation in attention-based models, and argue that Transformer is not a sequence model but an induced-structure model. This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language understanding.},
	journaltitle = {{arXiv}:2005.06420 [cs]},
	author = {Henderson, James},
	urldate = {2020-06-26},
	date = {2020-06-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.06420},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{rogers_primer_2020,
	title = {A Primer in {BERTology}: What we know about how {BERT} works},
	url = {http://arxiv.org/abs/2002.12327},
	shorttitle = {A Primer in {BERTology}},
	abstract = {Transformer-based models are now widely used in {NLP}, but we still do not understand a lot about their inner workings. This paper describes what is known to date about the famous {BERT} model (Devlin et al. 2019), synthesizing over 40 analysis studies. We also provide an overview of the proposed modifications to the model and its training regime. We then outline the directions for further research.},
	journaltitle = {{arXiv}:2002.12327 [cs]},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	urldate = {2020-06-24},
	date = {2020-02-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.12327},
	keywords = {Computer Science - Computation and Language},
}

@article{zoph_rethinking_2020,
	title = {Rethinking Pre-training and Self-training},
	url = {http://arxiv.org/abs/2006.06882},
	abstract = {Pre-training is a dominant paradigm in computer vision. For example, supervised {ImageNet} pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a surprising result that {ImageNet} pre-training has limited impact on {COCO} object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against {ImageNet} pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the {COCO} object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using {ImageNet} to help {COCO}). On the {PASCAL} segmentation dataset, which is a much smaller dataset than {COCO}, though pre-training does help significantly, self-training improves upon the pre-trained model. On {COCO} object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest {SpineNet} model. On {PASCAL} segmentation, we achieve 90.5 {mIOU}, an improvement of +1.5\% {mIOU} over the previous state-of-the-art result by {DeepLabv}3+.},
	journaltitle = {{arXiv}:2006.06882 [cs, stat]},
	author = {Zoph, Barret and Ghiasi, Golnaz and Lin, Tsung-Yi and Cui, Yin and Liu, Hanxiao and Cubuk, Ekin D. and Le, Quoc V.},
	urldate = {2020-06-21},
	date = {2020-06-11},
	eprinttype = {arxiv},
	eprint = {2006.06882},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jastrzebski_break-even_2020,
	title = {The Break-Even Point on Optimization Trajectories of Deep Neural Networks},
	url = {http://arxiv.org/abs/2002.09572},
	abstract = {The early phase of training of deep neural networks is critical for their ﬁnal performance. In this work, we study how the hyperparameters of stochastic gradient descent ({SGD}) used in the early phase of training affect the rest of the optimization trajectory. We argue for the existence of the “break-even" point on this trajectory, beyond which the curvature of the loss surface and noise in the gradient are implicitly regularized by {SGD}. In particular, we demonstrate on multiple classiﬁcation tasks that using a large learning rate in the initial phase of training reduces the variance of the gradient, and improves the conditioning of the covariance of gradients. These effects are beneﬁcial from the optimization perspective and become visible after the break-even point. Complementing prior work, we also show that using a low learning rate results in bad conditioning of the loss surface even for a neural network with batch normalization layers. In short, our work shows that key properties of the loss surface are strongly inﬂuenced by {SGD} in the early phase of training. We argue that studying the impact of the identiﬁed effects on generalization is a promising future direction.},
	journaltitle = {{arXiv}:2002.09572 [cs, stat]},
	author = {Jastrzebski, Stanislaw and Szymczak, Maciej and Fort, Stanislav and Arpit, Devansh and Tabor, Jacek and Cho, Kyunghyun and Geras, Krzysztof},
	urldate = {2020-06-19},
	date = {2020-02-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.09572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{resnick_capacity_2020,
	title = {Capacity, Bandwidth, and Compositionality in Emergent Language Learning},
	url = {http://arxiv.org/abs/1910.11424},
	abstract = {Many recent works have discussed the propensity, or lack thereof, for emergent languages to exhibit properties of natural languages. A favorite in the literature is learning compositionality. We note that most of those works have focused on communicative bandwidth as being of primary importance. While important, it is not the only contributing factor. In this paper, we investigate the learning biases that affect the efficacy and compositionality of emergent languages. Our foremost contribution is to explore how capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.},
	journaltitle = {{arXiv}:1910.11424 [cs, stat]},
	author = {Resnick, Cinjon and Gupta, Abhinav and Foerster, Jakob and Dai, Andrew M. and Cho, Kyunghyun},
	urldate = {2020-06-19},
	date = {2020-04-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.11424},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}
@article{htut_inducing_2019,
	title = {Inducing Constituency Trees through Neural Machine Translation},
	abstract = {Latent tree learning({LTL}) methods learn to parse sentences using only indirect supervision from a downstream task. Recent advances in latent tree learning have made it possible to recover moderately high quality tree structures by training with language modeling or auto-encoding objectives. In this work, we explore the hypothesis that decoding in machine translation, as a conditional language modeling task, will produce better tree structures since it offers a similar training signal as language modeling, but with more semantic signal. We adapt two existing latent-tree language models--{PRPN} {andON}-{LSTM}--for use in translation. We find that they indeed recover trees that are better in F1 score than those seen in language modeling on {WSJ} test set, while maintaining strong translation quality. We observe that translation is a better objective than language modeling for inducing trees, marking the first success at latent tree learning using a machine translation objective. Additionally, our findings suggest that, although translation provides better signal for inducing trees than language modeling, translation models can perform well without exploiting the latent tree structure.},
	journaltitle = {{ArXiv}},
	author = {Htut, Phu Mon and Cho, Kyunghyun and Bowman, Samuel R.},
	date = {2019},
}

@article{tanaka_pruning_2020,
	title = {Pruning neural networks without any data by iteratively conserving synaptic flow},
	url = {http://arxiv.org/abs/2006.05467},
	abstract = {Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning ({SynFlow}). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently outperforms existing state-of-the-art pruning algorithms at initialization over a range of models ({VGG} and {ResNet}), datasets ({CIFAR}-10/100 and Tiny {ImageNet}), and sparsity constraints (up to 99.9 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that data must be used to quantify which synapses are important.},
	journaltitle = {{arXiv}:2006.05467 [cond-mat, q-bio, stat]},
	author = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L. K. and Ganguli, Surya},
	urldate = {2020-06-13},
	date = {2020-06-09},
	eprinttype = {arxiv},
	eprint = {2006.05467},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{jacovi_aligning_2020,
	title = {Aligning Faithful Interpretations with their Social Attribution},
	url = {http://arxiv.org/abs/2006.01067},
	abstract = {We find that the requirement of model interpretations to be faithful is vague and incomplete. Indeed, recent work refers to interpretations as unfaithful despite adhering to the available definition. Similarly, we identify several critical failures with the notion of textual highlights as faithful interpretations, although they adhere to the faithfulness definition. With textual highlights as a case-study, and borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and social attribution of human behavior to the interpretation. We re-formulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of "aligned faithfulness": faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution *together* complete the process of explaining behavior, making the alignment of faithful interpretations a requirement. With this formalization, we characterize the observed failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we the implement highlight explanations of proposed causal format using contrastive explanations.},
	journaltitle = {{arXiv}:2006.01067 [cs]},
	author = {Jacovi, Alon and Goldberg, Yoav},
	urldate = {2020-06-11},
	date = {2020-06-01},
	eprinttype = {arxiv},
	eprint = {2006.01067},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{chattopadhyay_neural_2019,
	title = {Neural Network Attributions: A Causal Perspective},
	url = {http://arxiv.org/abs/1902.02302},
	shorttitle = {Neural Network Attributions},
	abstract = {We propose a new attribution method for neural networks developed using ﬁrst principles of causality (to the best of our knowledge, the ﬁrst such). The neural network architecture is viewed as a Structural Causal Model, and a methodology to compute the causal effect of each feature on the output is presented. With reasonable assumptions on the causal structure of the input data, we propose algorithms to efﬁciently compute the causal effects, as well as scale the approach to data with large dimensionality. We also show how this method can be used for recurrent neural networks. We report experimental results on both simulated and real datasets showcasing the promise and usefulness of the proposed algorithm.},
	journaltitle = {{arXiv}:1902.02302 [cs, stat]},
	author = {Chattopadhyay, Aditya and Manupriya, Piyushi and Sarkar, Anirban and Balasubramanian, Vineeth N.},
	urldate = {2020-06-10},
	date = {2019-07-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1902.02302},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{subramani_can_2019,
	title = {Can Unconditional Language Models Recover Arbitrary Sentences?},
	abstract = {Neural network-based generative language models like {ELMo} and {BERT} can work effectively as general purpose sentence encoders in text classification without further fine-tuning. Is it possible to adapt them in a similar way for use as general-purpose decoders? For this to be possible, it would need to be the case that for any target sentence of interest, there is some continuous representation that can be passed to the language model to cause it to reproduce that sentence. We set aside the difficult problem of designing an encoder that can produce such representations and instead ask directly whether such representations exist at all. To do this, we introduce a pair of effective complementary methods for feeding representations into pretrained unconditional language models and a corresponding set of methods to map sentences into and out of this representation space, the {\textbackslash}textit\{reparametrized sentence space\}. We then investigate the conditions under which a language model can be made to generate a sentence through the identification of a point in such a space and find that it is possible to recover arbitrary sentences nearly perfectly with language models and representations of moderate size.},
	journaltitle = {{NeurIPS}},
	author = {Subramani, Nishant and Bowman, Samuel R. and Cho, Kyunghyun},
	date = {2019},
}

@article{domingos_few_2012,
	title = {A few useful things to know about machine learning},
	volume = {55},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2347736.2347755},
	doi = {10.1145/2347736.2347755},
	abstract = {Machine learning algorithms can ﬁgure out how to perform important tasks by generalizing from examples. This is often feasible and cost-eﬀective where manual programming is not. As more data becomes available, more ambitious problems can be tackled. As a result, machine learning is widely used in computer science and other ﬁelds. However, developing successful machine learning applications requires a substantial amount of “black art” that is hard to ﬁnd in textbooks. This article summarizes twelve key lessons that machine learning researchers and practitioners have learned. These include pitfalls to avoid, important issues to focus on, and answers to common questions.},
	pages = {78--87},
	number = {10},
	journaltitle = {Communications of the {ACM}},
	author = {Domingos, Pedro},
	urldate = {2020-06-09},
	date = {2012-10},
	langid = {english},
}

@article{murdoch_interpretable_2019,
	title = {Interpretable machine learning: definitions, methods, and applications},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1901.04592},
	doi = {10.1073/pnas.1900654116},
	shorttitle = {Interpretable machine learning},
	abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related, and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the Predictive, Descriptive, Relevant ({PDR}) framework for discussing interpretations. The {PDR} framework provides three overarching desiderata for evaluation: predictive accuracy, descriptive accuracy and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post-hoc categories, with sub-groups including sparsity, modularity and simulatability. To demonstrate how practitioners can use the {PDR} framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often under-appreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
	pages = {22071--22080},
	number = {44},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc Natl Acad Sci {USA}},
	author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
	urldate = {2020-06-09},
	date = {2019-10-29},
	eprinttype = {arxiv},
	eprint = {1901.04592},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Applications, Statistics - Machine Learning},
}

@article{rush_tutorial_2012,
	title = {A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference in Natural Language Processing},
	volume = {45},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1405.5208},
	doi = {10.1613/jair.3680},
	abstract = {Dual decomposition, and more generally Lagrangian relaxation, is a classical method for combinatorial optimization; it has recently been applied to several inference problems in natural language processing ({NLP}). This tutorial gives an overview of the technique. We describe example algorithms, describe formal guarantees for the method, and describe practical issues in implementing the algorithms. While our examples are predominantly drawn from the {NLP} literature, the material should be of general relevance to inference problems in machine learning. A central theme of this tutorial is that Lagrangian relaxation is naturally applied in conjunction with a broad class of combinatorial algorithms, allowing inference in models that go significantly beyond previous work on Lagrangian relaxation for inference in graphical models.},
	pages = {305--362},
	journaltitle = {Journal of Artificial Intelligence Research},
	shortjournal = {jair},
	author = {Rush, Alexander M. and Collins, Michael},
	urldate = {2020-06-09},
	date = {2012-10-30},
	eprinttype = {arxiv},
	eprint = {1405.5208},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@online{allen_analogies_2019,
	title = {'’Analogies Explained’’ … Explained},
	url = {https://carl-allen.github.io/blog/nlp/2019/07/01/explaining-analogies-explained.html},
	abstract = {This post provides a `less maths, more intuition’ overview of Analogies Explained: Towards Understanding Word Embeddings ({ICML}, 2019, Best Paper Honourable Mention). The outline follows that of the conference presentation. Target audience: general machine learning, {NLP}, computational linguistics.},
	titleaddon = {Carl Allen: Machine Learning Blog},
	author = {Allen, Carl},
	urldate = {2020-06-05},
	date = {2019-07-01},
	langid = {english},
	note = {Library Catalog: carl-allen.github.io},
}

@article{allen_analogies_2019-1,
	title = {Analogies Explained: Towards Understanding Word Embeddings},
	url = {http://arxiv.org/abs/1901.09813},
	shorttitle = {Analogies Explained},
	abstract = {Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy "woman is to queen as man is to king" approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of "\$w\_x\$ is to \$w\_y\$". From these concepts we prove existence of linear relationships between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.},
	journaltitle = {{arXiv}:1901.09813 [cs, stat]},
	author = {Allen, Carl and Hospedales, Timothy},
	urldate = {2020-06-05},
	date = {2019-05-11},
	eprinttype = {arxiv},
	eprint = {1901.09813},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{huang_dynamics_2019,
	title = {Dynamics of Deep Neural Networks and Neural Tangent Hierarchy},
	url = {http://arxiv.org/abs/1909.08156},
	abstract = {The evolution of a deep neural network trained by the gradient descent can be described by its neural tangent kernel ({NTK}) as introduced in [20], where it was proven that in the infinite width limit the {NTK} converges to an explicit limiting kernel and it stays constant during training. The {NTK} was also implicit in some other recent papers [6,13,14]. In the overparametrization regime, a fully-trained deep neural network is indeed equivalent to the kernel regression predictor using the limiting {NTK}. And the gradient descent achieves zero training loss for a deep overparameterized neural network. However, it was observed in [5] that there is a performance gap between the kernel regression using the limiting {NTK} and the deep neural networks. This performance gap is likely to originate from the change of the {NTK} along training due to the finite width effect. The change of the {NTK} along the training is central to describe the generalization features of deep neural networks. In the current paper, we study the dynamic of the {NTK} for finite width deep fully-connected neural networks. We derive an infinite hierarchy of ordinary differential equations, the neural tangent hierarchy ({NTH}) which captures the gradient descent dynamic of the deep neural network. Moreover, under certain conditions on the neural network width and the data set dimension, we prove that the truncated hierarchy of {NTH} approximates the dynamic of the {NTK} up to arbitrary precision. This description makes it possible to directly study the change of the {NTK} for deep neural networks, and sheds light on the observation that deep neural networks outperform kernel regressions using the corresponding limiting {NTK}.},
	journaltitle = {{arXiv}:1909.08156 [cs, stat]},
	author = {Huang, Jiaoyang and Yau, Horng-Tzer},
	urldate = {2020-06-05},
	date = {2019-09-17},
	eprinttype = {arxiv},
	eprint = {1909.08156},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{achille_critical_2019,
	title = {Critical Learning Periods in Deep Neural Networks},
	url = {http://arxiv.org/abs/1711.08856},
	abstract = {Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training. Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of "Information Plasticity". Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.},
	journaltitle = {{arXiv}:1711.08856 [cs, q-bio, stat]},
	author = {Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
	urldate = {2020-06-05},
	date = {2019-02-25},
	eprinttype = {arxiv},
	eprint = {1711.08856},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@book{mackay_information_nodate,
	title = {Information Theory, Inference, and Learning Algorithms},
	author = {{MacKay}, David J C},
	langid = {english},
}

@article{saxe_exact_2014,
	title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
	url = {http://arxiv.org/abs/1312.6120},
	abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by ﬁnding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising ﬁnding that as the depth of a network approaches inﬁnity, learning speed can nevertheless remain ﬁnite: for a special class of initial conditions on the weights, very deep networks incur only a ﬁnite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can ﬁnd this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
	journaltitle = {{arXiv}:1312.6120 [cond-mat, q-bio, stat]},
	author = {Saxe, Andrew M. and {McClelland}, James L. and Ganguli, Surya},
	urldate = {2020-06-05},
	date = {2014-02-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1312.6120},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@inproceedings{he_revisiting_2020,
	title = {Revisiting Self-Training for Neural Sequence Generation},
	url = {https://iclr.cc/virtual_2020/poster_SJgdnAVKDH.html},
	abstract = {Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. the pseudo-parallel data). While self-training has been extensively studied on classification problems, in complex sequence generation tasks (e.g. machine translation) it is still unclear how self-training works due to the compositionality of the target space. In this work, we first empirically show that self-training is able to decently improve the supervised baseline on neural sequence generation tasks. Through careful examination of the performance gains, we find that the perturbation on the hidden states (i.e. dropout) is critical for self-training to benefit from the pseudo-parallel data, which acts as a regularizer and forces the model to yield close predictions for similar unlabeled inputs. Such effect helps the model correct some incorrect predictions on unlabeled data. To further encourage this mechanism, we propose to inject noise to the input space, resulting in a noisy version of self-training. Empirical study on standard machine translation and text summarization benchmarks shows that noisy self-training is able to effectively utilize unlabeled data and improve the performance of the supervised baseline by a large margin.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {He, Junxian and Gu, Jiatao and Shen, Jiajun and Ranzato, Marc'Aurelio},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{chatterjee_coherent_2020,
	title = {Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization},
	url = {https://iclr.cc/virtual_2020/poster_ryeFY0EFwS.html},
	shorttitle = {Coherent Gradients},
	abstract = {An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Chatterjee, Satrajit},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{chatterji_intriguing_2020,
	title = {The intriguing role of module criticality in the generalization of deep networks},
	url = {https://iclr.cc/virtual_2020/poster_S1e4jkSKvB.html},
	abstract = {We study the phenomenon that some modules of deep neural networks ({DNNs}) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Chatterji, Niladri and Neyshabur, Behnam and Sedghi, Hanie},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{li_exponential_2020,
	title = {An Exponential Learning Rate Schedule for Deep Learning},
	url = {https://iclr.cc/virtual_2020/poster_rJg8TeSFDH.html},
	abstract = {Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or {BN}(Ioffe \& Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about {BN} with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone {BN} (Ioffe \& Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018) • Training can be done using {SGD} with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + α) factor in every epoch for some α {\textgreater} 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization. • Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of {BN} + {SGD} + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu \& He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc. • A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or {BN} alone reaches global minimum, but convergence fails when both are used.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Li, Zhiyuan and Arora, Sanjeev},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{ren_compositional_2020,
	title = {Compositional languages emerge in a neural iterated learning model},
	url = {https://iclr.cc/virtual_2020/poster_HkePNpVKPB.html},
	abstract = {The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents via grounded language learning. Inspired by the iterated learning framework, which simulates the process of language evolution, we propose an effective neural iterated learning algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide specific advantages to neural agents during training, which translates as a larger posterior probability, which is then incrementally amplified via the iterated learning procedure. Our experiments confirm our analysis, and also demonstrate that the emerged languages largely improve the generalization of the neural agent communication.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Ren, Yi and Guo, Shangmin and Labeau, Matthieu and Cohen, Shay B. and Kirby, Simon},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{lyu_curriculum_2020,
	title = {Curriculum Loss: Robust Learning and Generalization against Label Corruption},
	url = {https://iclr.cc/virtual_2020/poster_rkgt0REKwS.html},
	shorttitle = {Curriculum Loss},
	abstract = {Deep neural networks ({DNNs}) have great expressive power, which can even memorize samples with wrong labels. It is vitally important to reiterate robustness and generalization in {DNNs} against label corruption. To this end, this paper studies the 0-1 loss, which has a monotonic relationship between empirical adversary (reweighted) risk (Hu et al. 2018). Although the 0-1 loss is robust to outliers, it is also difficult to optimize. To efficiently optimize the 0-1 loss while keeping its robust properties, we propose a very simple and efficient loss, i.e. curriculum loss ({CL}). Our {CL} is a tighter upper bound of the 0-1 loss compared with conventional summation based surrogate losses. Moreover, {CL} can adaptively select samples for stagewise training. As a result, our loss can be deemed as a novel perspective of curriculum sample selection strategy, which bridges a connection between curriculum learning and robust learning. Experimental results on noisy {MNIST}, {CIFAR}10 and {CIFAR}100 dataset validate the robustness of the proposed loss.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Lyu, Yueming and Tsang, Ivor W.},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{ba_generalization_2020,
	title = {Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint},
	url = {https://iclr.cc/virtual_2020/poster_H1gBsgBYwH.html},
	shorttitle = {Generalization of Two-layer Neural Networks},
	abstract = {This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples \$n\$, features \$d\$, and neurons \$h\$ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups. When only the second layer coefficients are optimized, we recover the {\textbackslash}textit\{double descent\} phenomenon: a cusp in the population risk appears at \$h{\textbackslash}approx n\$ and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is {\textbackslash}textit\{independent\} of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to {\textbackslash}textit\{double descent\} might not translate to optimizing two-layer neural networks.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Ba, Jimmy and Erdogdu, Murat and Suzuki, Taiji and Wu, Denny and Zhang, Tianzong},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{wang_picking_2020,
	title = {Picking Winning Tickets Before Training by Preserving Gradient Flow},
	url = {https://iclr.cc/virtual_2020/poster_SkgsACVKPH.html},
	abstract = {Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation ({GraSP}). We empirically investigate the effectiveness of the proposed method with extensive experiments on {CIFAR}-10, {CIFAR}-100, Tiny-{ImageNet} and {ImageNet}, using {VGGNet} and {ResNet} architectures. Our method can prune 80\% of the weights of a {VGG}-16 network on {ImageNet} at initialization, with only a 1.6\% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public at: https://github.com/alecwangcq/{GraSP}.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{liu_variance_2020,
	title = {On the Variance of the Adaptive Learning Rate and Beyond},
	url = {https://iclr.cc/virtual_2020/poster_rkgz2aEKDr.html},
	abstract = {The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like {RMSprop} and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam ({RAdam}), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of {RAdam}.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{gissin_implicit_2020,
	title = {The Implicit Bias of Depth: How Incremental Learning Drives Generalization},
	url = {https://iclr.cc/virtual_2020/poster_H1lj0nNFwB.html},
	shorttitle = {The Implicit Bias of Depth},
	abstract = {A leading hypothesis for the surprising generalization of neural networks is that the dynamics of gradient descent bias the model towards simple solutions, by searching through the solution space in an incremental order of complexity. We formally define the notion of incremental learning dynamics and derive the conditions on depth and initialization for which this phenomenon arises in deep linear models. Our main theoretical contribution is a dynamical depth separation result, proving that while shallow models can exhibit incremental learning dynamics, they require the initialization to be exponentially small for these dynamics to present themselves. However, once the model becomes deeper, the dependence becomes polynomial and incremental learning can arise in more natural settings. We complement our theoretical findings by experimenting with deep matrix sensing, quadratic neural networks and with binary classification using diagonal and convolutional linear networks, showing all of these models exhibit incremental learning.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Gissin, Daniel and Shalev-Shwartz, Shai and Daniely, Amit},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{melis_mogrifier_2020,
	title = {Mogrifier {LSTM}},
	url = {https://iclr.cc/virtual_2020/poster_SJe5P6EYvS.html},
	abstract = {Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the {LSTM} context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3–4 perplexity points on Penn Treebank and Wikitext-2, and 0.01–0.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the {LSTM} and Transformer models.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Melis, Gábor and Kočiský, Tomáš and Blunsom, Phil},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{jiang_fantastic_2020,
	title = {Fantastic Generalization Measures and Where to Find Them},
	url = {https://iclr.cc/virtual_2020/poster_SJgIPJBFvH.html},
	abstract = {Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand {CIFAR}-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@inproceedings{tu_understanding_2020,
	title = {Understanding Generalization in Recurrent Neural Networks},
	url = {https://iclr.cc/virtual_2020/poster_rkgg6xBYDH.html},
	abstract = {In this work, we develop the theory for analyzing the generalization performance of recurrent neural networks. We first present a new generalization bound for recurrent neural networks based on matrix 1-norm and Fisher-Rao norm. The definition of Fisher-Rao norm relies on a structural lemma about the gradient of {RNNs}. This new generalization bound assumes that the covariance matrix of the input data is positive definite, which might limit its use in practice. To address this issue, we propose to add random noise to the input data and prove a generalization bound for training with random noise, which is an extension of the former one. Compared with existing results, our generalization bounds have no explicit dependency on the size of networks. We also discover that Fisher-Rao norm for {RNNs} can be interpreted as a measure of gradient, and incorporating this gradient measure not only can tighten the bound, but allows us to build a relationship between generalization and trainability. Based on the bound, we theoretically analyze the effect of covariance of features on generalization of {RNNs} and discuss how weight decay and gradient clipping in the training can help improve generalization.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Tu, Zhuozhuo and He, Fengxiang and Tao, Dacheng},
	urldate = {2020-06-05},
	date = {2020-04},
	langid = {english},
}

@article{frankle_early_2020,
	title = {The Early Phase of Neural Network Training},
	url = {http://arxiv.org/abs/2002.10365},
	abstract = {Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here, we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.},
	journaltitle = {{arXiv}:2002.10365 [cs, stat]},
	author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
	urldate = {2020-06-05},
	date = {2020-02-24},
	eprinttype = {arxiv},
	eprint = {2002.10365},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{blier_description_nodate,
	title = {The Description Length of Deep Learning models},
	abstract = {Solomonoff’s general theory of inference (Solomonoff, 1964) and the Minimum Description Length principle (Grünwald, 2007; Rissanen, 2007) formalize Occam’s razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded.},
	pages = {11},
	author = {Blier, Léonard and Ollivier, Yann},
	langid = {english},
}

@article{yu_playing_2020,
	title = {Playing the lottery with rewards and multiple languages: lottery tickets in {RL} and {NLP}},
	url = {http://arxiv.org/abs/1906.02768},
	shorttitle = {Playing the lottery with rewards and multiple languages},
	abstract = {The lottery ticket hypothesis proposes that over-parameterization of deep neural networks ({DNNs}) aids training by increasing the probability of a "lucky" sub-network initialization being present rather than by helping the optimization process (Frankle \& Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for {DNNs} can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether "winning ticket" initializations exist in two different domains: natural language processing ({NLP}) and reinforcement learning ({RL}).For {NLP}, we examined both recurrent {LSTM} models and large-scale Transformer models (Vaswani et al., 2017). For {RL}, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with workin supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both {NLP} and {RL}. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in {DNNs}.},
	journaltitle = {{arXiv}:1906.02768 [cs, stat]},
	author = {Yu, Haonan and Edunov, Sergey and Tian, Yuandong and Morcos, Ari S.},
	urldate = {2020-06-03},
	date = {2020-02-25},
	eprinttype = {arxiv},
	eprint = {1906.02768},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{kuncoro_syntactic_2020,
	title = {Syntactic Structure Distillation Pretraining For Bidirectional Encoders},
	url = {http://arxiv.org/abs/2005.13482},
	abstract = {Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence. Given this success, it remains an open question whether scalable learners like {BERT} can become fully proﬁcient in the syntax of natural language by virtue of data scale alone, or whether they still beneﬁt from more explicit syntactic biases. To answer this question, we introduce a knowledge distillation strategy for injecting syntactic biases into {BERT} pretraining, by distilling the syntactically informative predictions of a hierarchical—albeit harder to scale—syntactic language model. Since {BERT} models masked words in bidirectional context, we propose to distill the approximate marginal distribution over words in context from the syntactic {LM}. Our approach reduces relative error by 2-21\% on a diverse set of structured prediction tasks, although we obtain mixed results on the {GLUE} benchmark. Our ﬁndings demonstrate the beneﬁts of syntactic biases, even in representation learners that exploit large amounts of data, and contribute to a better understanding of where syntactic biases are most helpful in benchmarks of natural language understanding.},
	journaltitle = {{arXiv}:2005.13482 [cs]},
	author = {Kuncoro, Adhiguna and Kong, Lingpeng and Fried, Daniel and Yogatama, Dani and Rimell, Laura and Dyer, Chris and Blunsom, Phil},
	urldate = {2020-06-03},
	date = {2020-05-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.13482},
	keywords = {Computer Science - Computation and Language},
}

@article{qiu_pre-trained_2020,
	title = {Pre-trained Models for Natural Language Processing: A Survey},
	url = {http://arxiv.org/abs/2003.08271},
	shorttitle = {Pre-trained Models for Natural Language Processing},
	abstract = {Recently, the emergence of pre-trained models ({PTMs}) has brought natural language processing ({NLP}) to a new era. In this survey, we provide a comprehensive review of {PTMs} for {NLP}. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing {PTMs} based on a taxonomy with four perspectives. Next, we describe how to adapt the knowledge of {PTMs} to the downstream tasks. Finally, we outline some potential directions of {PTMs} for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing {PTMs} for various {NLP} tasks.},
	journaltitle = {{arXiv}:2003.08271 [cs]},
	author = {Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
	urldate = {2020-05-30},
	date = {2020-04-24},
	eprinttype = {arxiv},
	eprint = {2003.08271},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{yogatama_learning_2019,
	title = {Learning and Evaluating General Linguistic Intelligence},
	url = {http://arxiv.org/abs/1901.11373},
	abstract = {We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., {SQuAD}). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.},
	journaltitle = {{arXiv}:1901.11373 [cs, stat]},
	author = {Yogatama, Dani and d'Autume, Cyprien de Masson and Connor, Jerome and Kocisky, Tomas and Chrzanowski, Mike and Kong, Lingpeng and Lazaridou, Angeliki and Ling, Wang and Yu, Lei and Dyer, Chris and Blunsom, Phil},
	urldate = {2020-05-30},
	date = {2019-01-31},
	eprinttype = {arxiv},
	eprint = {1901.11373},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{merity_regularizing_2017,
	title = {Regularizing and Optimizing {LSTM} Language Models},
	url = {http://arxiv.org/abs/1708.02182},
	abstract = {Recurrent neural networks ({RNNs}), such as long short-term memory networks ({LSTMs}), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the speciﬁc problem of word-level language modeling and investigate strategies for regularizing and optimizing {LSTMbased} models. We propose the weight-dropped {LSTM} which uses {DropConnect} on hidden-tohidden weights as a form of recurrent regularization. Further, we introduce {NT}-{ASGD}, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on {WikiText}-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on {WikiText}-2.},
	journaltitle = {{arXiv}:1708.02182 [cs]},
	author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	urldate = {2020-05-30},
	date = {2017-08-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1708.02182},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{wang_transformer_2020,
	title = {Transformer on a Diet},
	url = {http://arxiv.org/abs/2002.06170},
	abstract = {Transformer has been widely used thanks to its ability to capture sequence information in an efﬁcient way. However, recent developments, such as {BERT} and {GPT}-2, deliver only heavy architectures with a focus on effectiveness. In this paper, we explore three carefullydesigned light Transformer architectures to ﬁgure out whether the Transformer with less computations could produce competitive results. Experimental results on language model benchmark datasets hint that such trade-off is promising, and the light Transformer reduces 70\% parameters at best, while obtains competitive perplexity compared to standard Transformer. The source code is publicly available 1.},
	journaltitle = {{arXiv}:2002.06170 [cs]},
	author = {Wang, Chenguang and Ye, Zihao and Zhang, Aston and Zhang, Zheng and Smola, Alexander J.},
	urldate = {2020-05-30},
	date = {2020-02-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.06170},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{bowman_fast_2016,
	title = {A Fast Unified Model for Parsing and Sentence Understanding},
	url = {http://arxiv.org/abs/1603.06021},
	abstract = {Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suﬀer from two key technical problems that make them slow and unwieldy for large-scale {NLP} tasks: they usually operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stackaugmented Parser-Interpreter Neural Network ({SPINN}), which combines parsing and interpretation within a single treesequence hybrid model by integrating treestructured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25× over other tree-structured models, and its integrated parser can operate on unparsed data with little loss in accuracy. We evaluate it on the Stanford {NLI} entailment task and show that it signiﬁcantly outperforms other sentence-encoding models.},
	journaltitle = {{arXiv}:1603.06021 [cs]},
	author = {Bowman, Samuel R. and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christopher D. and Potts, Christopher},
	urldate = {2020-05-30},
	date = {2016-07-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1603.06021},
	keywords = {Computer Science - Computation and Language},
}
@article{recht_imagenet_nodate,
	title = {Do {ImageNet} Classiﬁers Generalize to {ImageNet}?},
	abstract = {We build new test sets for the {CIFAR}-10 and {ImageNet} datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overﬁtting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classiﬁcation models generalize to new data. We evaluate a broad range of models and ﬁnd accuracy drops of 3\% – 15\% on {CIFAR}-10 and 11\% – 14\% on {ImageNet}. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models’ inability to generalize to slightly “harder” images than those found in the original test sets.},
	pages = {12},
	author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
	langid = {english},
}

@article{langeberg_effect_2019,
	title = {On the Effect of Low-Rank Weights on Adversarial Robustness of Neural Networks},
	url = {http://arxiv.org/abs/1901.10371},
	abstract = {Recently, there has been an abundance of works on designing Deep Neural Networks ({DNNs}) that are robust to adversarial examples. In particular, a central question is which features of {DNNs} influence adversarial robustness and, therefore, can be to used to design robust {DNNs}. In this work, this problem is studied through the lens of compression which is captured by the low-rank structure of weight matrices. It is first shown that adversarial training tends to promote simultaneously low-rank and sparse structure in the weight matrices of neural networks. This is measured through the notions of effective rank and effective sparsity. In the reverse direction, when the low rank structure is promoted by nuclear norm regularization and combined with sparsity inducing regularizations, neural networks show significantly improved adversarial robustness. The effect of nuclear norm regularization on adversarial robustness is paramount when it is applied to convolutional neural networks. Although still not competing with adversarial training, this result contributes to understanding the key properties of robust classifiers.},
	journaltitle = {{arXiv}:1901.10371 [cs, stat]},
	author = {Langeberg, Peter and Balda, Emilio Rafael and Behboodi, Arash and Mathar, Rudolf},
	urldate = {2020-05-28},
	date = {2019-01-29},
	eprinttype = {arxiv},
	eprint = {1901.10371},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{goldblum_truth_2019,
	title = {Truth or backpropaganda? An empirical investigation of deep learning theory},
	url = {https://openreview.net/forum?id=HyxyIgHFvr},
	shorttitle = {Truth or backpropaganda?},
	abstract = {We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  In this work, we: (1) prove the widespread existence of suboptimal local...},
	eventtitle = {International Conference on Learning Representations},
	author = {Goldblum, Micah and Geiping, Jonas and Schwarzschild, Avi and Moeller, Michael and Goldstein, Tom},
	urldate = {2020-05-28},
	date = {2019-09-25},
}

@article{jetka_information-theoretic_2019,
	title = {Information-theoretic analysis of multivariate single-cell signaling responses},
	volume = {15},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007132},
	doi = {10.1371/journal.pcbi.1007132},
	abstract = {Mathematical methods of information theory appear to provide a useful language to describe how stimuli are encoded in activities of signaling effectors. Exploring the information-theoretic perspective, however, remains conceptually, experimentally and computationally challenging. Specifically, existing computational tools enable efficient analysis of relatively simple systems, usually with one input and output only. Moreover, their robust and readily applicable implementations are missing. Here, we propose a novel algorithm, {SLEMI}—statistical learning based estimation of mutual information, to analyze signaling systems with high-dimensional outputs and a large number of input values. Our approach is efficient in terms of computational time as well as sample size needed for accurate estimation. Analysis of the {NF}-κB single—cell signaling responses to {TNF}-α reveals that {NF}-κB signaling dynamics improves discrimination of high concentrations of {TNF}-α with a relatively modest impact on discrimination of low concentrations. Provided R-package allows the approach to be used by computational biologists with only elementary knowledge of information theory.},
	pages = {e1007132},
	number = {7},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Jetka, Tomasz and Nienałtowski, Karol and Winarski, Tomasz and Błoński, Sławomir and Komorowski, Michał},
	urldate = {2020-05-28},
	date = {2019-07-12},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Biologists, Cell processes, Cell signaling, Information theory, Optimization, Probability distribution, Transcription factors},
}

@inproceedings{gxe1_state_2018,
	title = {On the State of the Art of Evaluation in Neural Language Models},
	url = {https://openreview.net/forum?id=ByJHuTgA-},
	abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been...},
	eventtitle = {International Conference on Learning Representations},
	author = {G{\textbackslash}\&{\textbackslash}\#{xE}1 and Melis, Bor and Dyer, Chris and Blunsom, Phil},
	urldate = {2020-05-27},
	date = {2018-02-15},
}

@inproceedings{kuncoro_lstms_2018,
	title = {{LSTMs} Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better},
	url = {https://www.aclweb.org/anthology/P18-1132},
	doi = {10.18653/v1/P18-1132},
	abstract = {Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yogatama, Stephen Clark, Phil Blunsom. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018.},
	eventtitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages = {1426--1436},
	author = {Kuncoro, Adhiguna and Dyer, Chris and Hale, John and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
	urldate = {2020-05-27},
	date = {2018-07},
	langid = {english},
}

@article{tapia_information_2020,
	title = {On the Information Plane of Autoencoders},
	url = {http://arxiv.org/abs/2005.07783},
	abstract = {The training dynamics of hidden layers in deep learning are poorly understood in theory. Recently, the Information Plane ({IP}) was proposed to analyze them, which is based on the information-theoretic concept of mutual information ({MI}). The Information Bottleneck ({IB}) theory predicts that layers maximize relevant information and compress irrelevant information. Due to the limitations in {MI} estimation from samples, there is an ongoing debate about the properties of the {IP} for the supervised learning case. In this work, we derive a theoretical convergence for the {IP} of autoencoders. The theory predicts that ideal autoencoders with a large bottleneck layer size do not compress input information, whereas a small size causes compression only in the encoder layers. For the experiments, we use a Gram-matrix based {MI} estimator recently proposed in the literature. We propose a new rule to adjust its parameters that compensates scale and dimensionality effects. Using our proposed rule, we obtain experimental {IPs} closer to the theory. Our theoretical {IP} for autoencoders could be used as a benchmark to validate new methods to estimate {MI} in neural networks. In this way, experimental limitations could be recognized and corrected, helping with the ongoing debate on the supervised learning case.},
	journaltitle = {{arXiv}:2005.07783 [cs, math, stat]},
	author = {Tapia, Nicolás I. and Estévez, Pablo A.},
	urldate = {2020-05-27},
	date = {2020-05-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.07783},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wickstrom_information_2019,
	title = {Information Plane Analysis of Deep Neural Networks via Matrix-Based Renyi's Entropy and Tensor Kernels},
	url = {http://arxiv.org/abs/1909.11396},
	abstract = {Analyzing deep neural networks ({DNNs}) via information plane ({IP}) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate mutual information ({MI}) between each hidden layer and the input/desired output, to construct the {IP}. For instance, hidden layers with many neurons require {MI} estimators with robustness towards the high dimensionality associated with such layers. {MI} estimators should also be able to naturally handle convolutional layers, while at the same time being computationally tractable to scale to large networks. None of the existing {IP} methods to date have been able to study truly deep Convolutional Neural Networks ({CNNs}), such as the e.g.{\textbackslash} {VGG}-16. In this paper, we propose an {IP} analysis using the new matrix--based R{\textbackslash}'enyi's entropy coupled with tensor kernels over convolutional layers, leveraging the power of kernel methods to represent properties of the probability distribution independently of the dimensionality of the data. The obtained results shed new light on the previous literature concerning small-scale {DNNs}, however using a completely new approach. Importantly, the new framework enables us to provide the first comprehensive {IP} analysis of contemporary large-scale {DNNs} and {CNNs}, investigating the different training phases and providing new insights into the training dynamics of large-scale neural networks.},
	journaltitle = {{arXiv}:1909.11396 [cs, stat]},
	author = {Wickstrøm, Kristoffer and Løkse, Sigurd and Kampffmeyer, Michael and Yu, Shujian and Principe, Jose and Jenssen, Robert},
	urldate = {2020-05-23},
	date = {2019-09-25},
	eprinttype = {arxiv},
	eprint = {1909.11396},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{grathwohl_your_2019,
	title = {Your classifier is secretly an energy based model and you should treat it like one},
	url = {https://openreview.net/forum?id=Hkxzx0NtDB},
	abstract = {We propose to reinterpret a standard discriminative classifier of p(y{\textbar}x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily...},
	eventtitle = {International Conference on Learning Representations},
	author = {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, Joern-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin},
	urldate = {2020-05-21},
	date = {2019-09-25},
}

@inproceedings{reif_visualizing_2019,
	title = {Visualizing and Measuring the Geometry of {BERT}},
	url = {http://papers.neurips.cc/paper/9065-visualizing-and-measuring-the-geometry-of-bert},
	abstract = {Electronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {8594--8603},
	author = {Reif, Emily and Yuan, Ann and Wattenberg, Martin and Viegas, Fernanda B. and Coenen, Andy and Pearce, Adam and Kim, Been},
	urldate = {2020-05-21},
	date = {2019},
}

@article{zolna_classifier-agnostic_2018,
	title = {Classifier-agnostic saliency map extraction},
	url = {http://arxiv.org/abs/1805.08249},
	abstract = {Extracting saliency maps, which indicate parts of the image important to classiﬁcation, requires many tricks to achieve satisfactory performance when using classiﬁer-dependent methods. Instead, we propose classiﬁer-agnostic saliency map extraction, which ﬁnds all parts of the image that any classiﬁer could use, not just one given in advance. We observe that the proposed approach extracts higher quality saliency maps and outperforms existing weakly-supervised localization techniques, setting the new state of the art result on the {ImageNet} dataset. We made our code publicly available at https://github.com/kondiz/casme.},
	journaltitle = {{arXiv}:1805.08249 [cs, stat]},
	author = {Zolna, Konrad and Geras, Krzysztof J. and Cho, Kyunghyun},
	urldate = {2020-05-20},
	date = {2018-10-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.08249},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-based Localization},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	shorttitle = {Grad-{CAM}},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of {CNN}-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-{CAM}), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-{CAM} is applicable to a wide variety of {CNN} model-families: (1) {CNNs} with fully-connected layers, (2) {CNNs} used for structured outputs, (3) {CNNs} used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-{CAM} with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering ({VQA}) models, including {ResNet}-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and {VQA}, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-{CAM} and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-{CAM} helps users establish appropriate trust in predictions from models and show that Grad-{CAM} helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/{COjUB}9Izk6E.},
	pages = {336--359},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	urldate = {2020-05-20},
	date = {2020-02},
	eprinttype = {arxiv},
	eprint = {1610.02391},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{voita_information-theoretic_2020,
	title = {Information-Theoretic Probing with Minimum Description Length},
	url = {http://arxiv.org/abs/2003.12298},
	abstract = {To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length ({MDL}). With {MDL} probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates "the amount of effort" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating {MDL} which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.},
	journaltitle = {{arXiv}:2003.12298 [cs]},
	author = {Voita, Elena and Titov, Ivan},
	urldate = {2020-05-19},
	date = {2020-03-27},
	eprinttype = {arxiv},
	eprint = {2003.12298},
	keywords = {Computer Science - Computation and Language},
}

@article{peng_mixture_2020,
	title = {A Mixture of \$h-1\$ Heads is Better than \$h\$ Heads},
	url = {http://arxiv.org/abs/2005.06537},
	abstract = {Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss. In this work, we instead "reallocate" them -- the model learns to activate different heads on different inputs. Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model ({MAE}). {MAE} is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters. Experiments on machine translation and language modeling show that {MAE} outperforms strong baselines on both tasks. Particularly, on the {WMT}14 English to German translation dataset, {MAE} improves over "transformer-base" by 0.8 {BLEU}, with a comparable number of parameters. Our analysis shows that our model learns to specialize different experts to different inputs.},
	journaltitle = {{arXiv}:2005.06537 [cs]},
	author = {Peng, Hao and Schwartz, Roy and Li, Dianqi and Smith, Noah A.},
	urldate = {2020-05-19},
	date = {2020-05-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.06537},
	keywords = {Computer Science - Computation and Language},
}

@article{hu_systematic_2020,
	title = {A Systematic Assessment of Syntactic Generalization in Neural Language Models},
	url = {http://arxiv.org/abs/2005.03692},
	abstract = {State-of-the-art neural network models have achieved dizzyingly low perplexity scores on major language modeling benchmarks, but it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 syntactic test suites. We ﬁnd that model architecture clearly inﬂuences syntactic generalization performance: Transformer models and models with explicit hierarchical structure reliably outperform pure sequence models in their predictions. In contrast, we ﬁnd no clear inﬂuence of the scale of training data on these syntactic generalization tests. We also ﬁnd no clear relation between a model’s perplexity and its syntactic generalization performance.},
	journaltitle = {{arXiv}:2005.03692 [cs]},
	author = {Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger P.},
	urldate = {2020-05-19},
	date = {2020-05-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.03692},
	keywords = {Computer Science - Computation and Language},
}

@article{gidel_implicit_2019,
	title = {Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks},
	url = {http://arxiv.org/abs/1904.13262},
	abstract = {When optimizing over-parameterized models, such as deep neural networks, a large set of parameters can achieve zero training error. In such cases, the choice of the optimization algorithm and its respective hyper-parameters introduces biases that will lead to convergence to specific minimizers of the objective. Consequently, this choice can be considered as an implicit regularization for the training of over-parametrized models. In this work, we push this idea further by studying the discrete gradient dynamics of the training of a two-layer linear network with the least-squares loss. Using a time rescaling, we show that, with a vanishing initialization and a small enough step size, this dynamics sequentially learns the solutions of a reduced-rank regression with a gradually increasing rank.},
	journaltitle = {{arXiv}:1904.13262 [cs, math, stat]},
	author = {Gidel, Gauthier and Bach, Francis and Lacoste-Julien, Simon},
	urldate = {2020-05-14},
	date = {2019-12-05},
	eprinttype = {arxiv},
	eprint = {1904.13262},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{frankle_mode_2019,
	title = {Mode Connectivity and Sparse Neural Networks},
	url = {https://openreview.net/forum?id=rkeO-lrYwr},
	abstract = {We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, {SGD}...},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
	urldate = {2020-05-14},
	date = {2019-09-25},
}

@article{prasanna_when_2020,
	title = {When {BERT} Plays the Lottery, All Tickets Are Winning},
	url = {http://arxiv.org/abs/2005.00561},
	abstract = {Much of the recent success in {NLP} is due to the large Transformer-based models such as {BERT} (Devlin et al, 2019). However, these models have been shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis. For fine-tuned {BERT}, we show that (a) it is possible to find a subnetwork of elements that achieves performance comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. However, the "bad" subnetworks can be fine-tuned separately to achieve only slightly worse performance than the "good" ones, indicating that most weights in the pre-trained {BERT} are potentially useful. We also show that the "good" subnetworks vary considerably across {GLUE} tasks, opening up the possibilities to learn what knowledge {BERT} actually uses at inference time.},
	journaltitle = {{arXiv}:2005.00561 [cs]},
	author = {Prasanna, Sai and Rogers, Anna and Rumshisky, Anna},
	urldate = {2020-05-13},
	date = {2020-05-01},
	eprinttype = {arxiv},
	eprint = {2005.00561},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{vashishth_attention_2019,
	title = {Attention Interpretability Across {NLP} Tasks},
	url = {http://arxiv.org/abs/1909.11218},
	abstract = {The attention layer in a neural network model provides insights into the model's reasoning behind its prediction, which are usually criticized for being opaque. Recently, seemingly contradictory viewpoints have emerged about the interpretability of attention weights (Jain \& Wallace, 2019; Vig \& Belinkov, 2019). Amid such confusion arises the need to understand attention mechanism more systematically. In this work, we attempt to fill this gap by giving a comprehensive explanation which justifies both kinds of observations (i.e., when is attention interpretable and when it is not). Through a series of experiments on diverse {NLP} tasks, we validate our observations and reinforce our claim of interpretability of attention through manual evaluation.},
	journaltitle = {{arXiv}:1909.11218 [cs]},
	author = {Vashishth, Shikhar and Upadhyay, Shyam and Tomar, Gaurav Singh and Faruqui, Manaal},
	urldate = {2020-05-13},
	date = {2019-09-24},
	eprinttype = {arxiv},
	eprint = {1909.11218},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{sabatelli_transferability_2020,
	title = {On the Transferability of Winning Tickets in Non-Natural Image Datasets},
	url = {http://arxiv.org/abs/2005.05232},
	abstract = {We study the generalization properties of pruned neural networks that are the winners of the lottery ticket hypothesis on datasets of natural images. We analyse their potential under conditions in which training data is scarce and comes from a non-natural domain. Speciﬁcally, we investigate whether pruned models that are found on the popular {CIFAR}-10/100 and Fashion-{MNIST} datasets, generalize to seven different datasets that come from the ﬁelds of digital pathology and digital heritage. Our results show that there are signiﬁcant beneﬁts in transferring and training sparse architectures over larger parametrized models, since in all of our experiments pruned networks, winners of the lottery ticket hypothesis, signiﬁcantly outperform their larger unpruned counterparts. These results suggest that winning initializations do contain inductive biases that are generic to some extent, although, as reported by our experiments on the biomedical datasets, their generalization properties can be more limiting than what has been so far observed in the literature.},
	journaltitle = {{arXiv}:2005.05232 [cs]},
	author = {Sabatelli, Matthia and Kestemont, Mike and Geurts, Pierre},
	urldate = {2020-05-12},
	date = {2020-05-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.05232},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@book{axler_linear_2015,
	location = {Cham},
	title = {Linear Algebra Done Right},
	isbn = {978-3-319-11079-0 978-3-319-11080-6},
	url = {http://link.springer.com/10.1007/978-3-319-11080-6},
	series = {Undergraduate Texts in Mathematics},
	publisher = {Springer International Publishing},
	author = {Axler, Sheldon},
	urldate = {2020-05-11},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-11080-6},
}

@book{liesen_linear_2015,
	location = {Cham},
	title = {Linear Algebra},
	isbn = {978-3-319-24344-3 978-3-319-24346-7},
	url = {http://link.springer.com/10.1007/978-3-319-24346-7},
	series = {Springer Undergraduate Mathematics Series},
	publisher = {Springer International Publishing},
	author = {Liesen, Jörg and Mehrmann, Volker},
	urldate = {2020-05-11},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-24346-7},
}

@article{maudslay_tale_2020,
	title = {A Tale of a Probe and a Parser},
	url = {http://arxiv.org/abs/2005.01641},
	abstract = {Measuring what linguistic information is encoded in neural models of language has become popular in {NLP}. Researchers approach this enterprise by training "probes" - supervised models designed to extract linguistic structure from another model's output. One such probe is the structural probe (Hewitt and Manning, 2019), designed to quantify the extent to which syntactic information is encoded in contextualised word representations. The structural probe has a novel design, unattested in the parsing literature, the precise benefit of which is not immediately obvious. To explore whether syntactic probes would do better to make use of existing techniques, we compare the structural probe to a more traditional parser with an identical lightweight parameterisation. The parser outperforms structural probe on {UUAS} in seven of nine analysed languages, often by a substantial amount (e.g. by 11.1 points in English). Under a second less common metric, however, there is the opposite trend - the structural probe outperforms the parser. This begs the question: which metric should we prefer?},
	journaltitle = {{arXiv}:2005.01641 [cs]},
	author = {Maudslay, Rowan Hall and Valvoda, Josef and Pimentel, Tiago and Williams, Adina and Cotterell, Ryan},
	urldate = {2020-05-11},
	date = {2020-05-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.01641},
	keywords = {Computer Science - Computation and Language},
}

@article{bailly_emergence_2020,
	title = {Emergence of Syntax Needs Minimal Supervision},
	url = {http://arxiv.org/abs/2005.01119},
	abstract = {This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-speciﬁc guidance. Our approach originates in the observable structure of a corpus, which we use to deﬁne and isolate grammaticality (syntactic information) and meaning/pragmatics information. We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model.},
	journaltitle = {{arXiv}:2005.01119 [cs]},
	author = {Bailly, Raphaël and Gábor, Kata},
	urldate = {2020-05-11},
	date = {2020-05-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.01119},
	keywords = {Computer Science - Computation and Language},
}

@article{lu_influence_2020,
	title = {Influence Paths for Characterizing Subject-Verb Number Agreement in {LSTM} Language Models},
	url = {http://arxiv.org/abs/2005.01190},
	abstract = {{LSTM}-based recurrent neural networks are the state-of-the-art for many natural language processing ({NLP}) tasks. Despite their performance, it is unclear whether, or how, {LSTMs} learn structural features of natural languages such as subject-verb number agreement in English. Lacking this understanding, the generality of {LSTM} performance on this task and their suitability for related tasks remains uncertain. Further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other exceptional faults. We introduce *influence paths*, a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network. The approach refines the notion of influence (the subject's grammatical number has influence on the grammatical number of the subsequent verb) into a set of gate or neuron-level paths. The set localizes and segments the concept (e.g., subject-verb agreement), its constituent elements (e.g., the subject), and related or interfering elements (e.g., attractors). We exemplify the methodology on a widely-studied multi-layer {LSTM} language model, demonstrating its accounting for subject-verb number agreement. The results offer both a finer and a more complete view of an {LSTM}'s handling of this structural aspect of the English language than prior results based on diagnostic classifiers and ablation.},
	journaltitle = {{arXiv}:2005.01190 [cs]},
	author = {Lu, Kaiji and Mardziel, Piotr and Leino, Klas and Fedrikson, Matt and Datta, Anupam},
	urldate = {2020-05-11},
	date = {2020-05-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.01190},
	keywords = {Computer Science - Computation and Language},
}

@article{sap_recollection_nodate,
	title = {Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models},
	abstract = {We investigate the use of {NLP} as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events. To facilitate this, we collect and release {HIPPOCORPUS}, a dataset of 7,000 stories about imagined and recalled events.},
	pages = {9},
	author = {Sap, Maarten and Horvitz, Eric and Choi, Yejin and Smith, Noah A and Pennebaker, James W},
	langid = {english},
}

@article{meng_spherical_2019,
	title = {Spherical Text Embedding},
	url = {http://arxiv.org/abs/1911.01196},
	abstract = {Unsupervised text embedding has shown great power in a wide range of {NLP} tasks. While text embeddings are typically learned in the Euclidean space, directional similarity is often more effective in tasks such as word similarity and document clustering, which creates a gap between the training stage and usage stage of text embedding. To close this gap, we propose a spherical generative model based on which unsupervised word and paragraph embeddings are jointly learned. To learn text embeddings in the spherical space, we develop an efﬁcient optimization algorithm with convergence guarantee based on Riemannian optimization. Our model enjoys high efﬁciency and achieves state-of-the-art performances on various text embedding tasks including word similarity and document clustering2.},
	journaltitle = {{arXiv}:1911.01196 [cs, stat]},
	author = {Meng, Yu and Huang, Jiaxin and Wang, Guangyuan and Zhang, Chao and Zhuang, Honglei and Kaplan, Lance and Han, Jiawei},
	urldate = {2020-05-09},
	date = {2019-11-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1911.01196},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{nickel_poincare_nodate,
	title = {Poincaré Embeddings for Learning Hierarchical Representations},
	abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space – or more precisely into an n-dimensional Poincaré ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efﬁcient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincaré embeddings can outperform Euclidean embeddings signiﬁcantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
	pages = {10},
	author = {Nickel, Maximillian and Kiela, Douwe},
	langid = {english},
}

@online{noauthor_computer-aided_nodate,
	title = {Computer-aided analyses in optimization – Machine Learning Research Blog},
	url = {https://francisbach.com/computer-aided-analyses/},
	urldate = {2020-05-06},
	langid = {american},
}

@online{hardt_exponential_nodate,
	title = {Exponential Learning Rate Schedules for Deep Learning (Part 1)},
	url = {http://offconvex.github.io/2020/04/24/ExpLR1/},
	abstract = {Algorithms off the convex path.},
	titleaddon = {Off the convex path},
	author = {Hardt, Moritz},
	urldate = {2020-05-06},
}

@article{michael_asking_2020,
	title = {Asking without Telling: Exploring Latent Ontologies in Contextual Representations},
	shorttitle = {Asking without Telling},
	abstract = {The success of pretrained contextual encoders, such as {ELMo} and {BERT}, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning ({LSL}): a modification to existing classifier-based probing methods that induces a latent categorization (or ontology) of the probe’s inputs. Without access to finegrained gold labels, {LSL} extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in {ELMo}, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.},
	journaltitle = {{ArXiv}},
	author = {Michael, Julian and Botha, Jan A. and Tenney, Ian},
	date = {2020},
}

@article{hupkes_compositionality_2020,
	title = {Compositionality decomposed: how do neural networks generalise?},
	url = {http://arxiv.org/abs/1908.08351},
	shorttitle = {Compositionality decomposed},
	abstract = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub {PCFG} {SET} and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
	journaltitle = {{arXiv}:1908.08351 [cs, stat]},
	author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
	urldate = {2020-05-06},
	date = {2020-02-23},
	eprinttype = {arxiv},
	eprint = {1908.08351},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{linzen_syntactic_2020,
	title = {Syntactic Structure from Deep Learning},
	url = {http://arxiv.org/abs/2004.10827},
	doi = {10.1146/annurev-linguistics-032020-051035},
	abstract = {Modern deep neural networks achieve impressive performance in engineering applications that require extensive linguistic skills, such as machine translation. This success has sparked interest in probing whether these models are inducing human-like grammatical knowledge from the raw data they are exposed to, and, consequently, whether they can shed new light on long-standing debates concerning the innate structure necessary for language acquisition. In this article, we survey representative studies of the syntactic abilities of deep networks, and discuss the broader implications that this work has for theoretical linguistics.},
	journaltitle = {{arXiv}:2004.10827 [cs]},
	author = {Linzen, Tal and Baroni, Marco},
	urldate = {2020-05-06},
	date = {2020-04-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.10827},
	keywords = {Computer Science - Computation and Language},
}

@article{merrill_formal_2020,
	title = {A Formal Hierarchy of {RNN} Architectures},
	url = {http://arxiv.org/abs/2004.08500},
	abstract = {We develop a formal hierarchy of the expressive capacity of {RNN} architectures. The hierarchy is based on two formal properties: space complexity, which measures the {RNN}’s memory, and rational recurrence, deﬁned as whether the recurrent update can be described by a weighted ﬁnite-state machine. We place several {RNN} variants within this hierarchy. For example, we prove the {LSTM} is not rational, which formally separates it from the related {QRNN} (Bradbury et al., 2016). We also show how these models’ expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions. Our results build on the theory of “saturated” {RNNs} (Merrill, 2019). While formally extending these ﬁndings to unsaturated {RNNs} is left to future work, we hypothesize that the practical learnable capacity of unsaturated {RNNs} obeys a similar hierarchy. Experimental ﬁndings from training unsaturated networks on formal languages support this conjecture.},
	journaltitle = {{arXiv}:2004.08500 [cs]},
	author = {Merrill, William and Weiss, Gail and Goldberg, Yoav and Schwartz, Roy and Smith, Noah A. and Yahav, Eran},
	urldate = {2020-05-06},
	date = {2020-04-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.08500},
	keywords = {Computer Science - Computation and Language, Computer Science - Formal Languages and Automata Theory},
}

@article{kuznetsov_matter_2020,
	title = {A Matter of Framing: The Impact of Linguistic Formalism on Probing Results},
	url = {http://arxiv.org/abs/2004.14999},
	shorttitle = {A Matter of Framing},
	abstract = {Deep pre-trained contextualized encoders like {BERT} (Devlin et al., 2019) demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pretraining. While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms. Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data. Can the choice of formalism affect probing results? To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics. We ﬁnd linguistically meaningful differences in the encoding of semantic role- and proto-role information by {BERT} depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism. Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings.},
	journaltitle = {{arXiv}:2004.14999 [cs]},
	author = {Kuznetsov, Ilia and Gurevych, Iryna},
	urldate = {2020-05-06},
	date = {2020-04-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.14999},
	keywords = {Computer Science - Computation and Language},
}

@article{merchant_what_2020,
	title = {What Happens To {BERT} Embeddings During Fine-tuning?},
	url = {http://arxiv.org/abs/2004.14448},
	abstract = {While there has been much recent work studying how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis techniques (probing classiﬁers, Representational Similarity Analysis, and model ablations), we investigate how ﬁne-tuning affects the representations of the {BERT} model. We ﬁnd that while ﬁne-tuning necessarily makes signiﬁcant changes, it does not lead to catastrophic forgetting of linguistic phenomena. We instead ﬁnd that ﬁne-tuning primarily affects the top layers of {BERT}, but with noteworthy variation across tasks. In particular, dependency parsing reconﬁgures most of the model, whereas {SQuAD} and {MNLI} appear to involve much shallower processing. Finally, we also ﬁnd that ﬁne-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.},
	journaltitle = {{arXiv}:2004.14448 [cs]},
	author = {Merchant, Amil and Rahimtoroghi, Elahe and Pavlick, Ellie and Tenney, Ian},
	urldate = {2020-05-06},
	date = {2020-04-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.14448},
	keywords = {Computer Science - Computation and Language},
}

@article{papadimitriou_pretraining_2020,
	title = {Pretraining on Non-linguistic Structure as a Tool for Analyzing Learning Bias in Language Models},
	url = {http://arxiv.org/abs/2004.14601},
	abstract = {We propose a novel methodology for analyzing the encoding of grammatical structure in neural language models through transfer learning. We test how a language model can leverage its internal representations to transfer knowledge across languages and symbol systems. We train {LSTMs} on non-linguistic, structured data and test their performance on human language to assess which kinds of data induce generalizable encodings that {LSTMs} can use for natural language. We ﬁnd that models trained on structured data such as music and Java code have internal representations that help in modelling human language, and that, surprisingly, adding minimal amounts of structure to the training data makes a large difference in transfer to natural language. Further experiments on transfer between human languages show that zero-shot performance on a test language is highly correlated with syntactic similarity to the training language, even after removing any vocabulary overlap. This suggests that the internal representations induced from natural languages are typologically coherent: they encode the features and differences outlined in typological studies. Our results provide insights into how neural networks represent linguistic structure, and also about the kinds of structural biases that give learners the ability to model language.},
	journaltitle = {{arXiv}:2004.14601 [cs]},
	author = {Papadimitriou, Isabel and Jurafsky, Dan},
	urldate = {2020-05-06},
	date = {2020-04-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.14601},
	keywords = {Computer Science - Computation and Language},
}

@article{bender_climbing_nodate,
	title = {Climbing towards {NLU}: On Meaning, Form, and Understanding in the Age of Data},
	abstract = {The success of the large neural language models on many {NLP} tasks is exciting. However, we ﬁnd that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the {ACL} 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the ﬁeld towards better science around natural language understanding.},
	pages = {14},
	author = {Bender, Emily M and Koller, Alexander},
	langid = {english},
}

@article{tamkin_investigating_2020,
	title = {Investigating Transferability in Pretrained Language Models},
	url = {http://arxiv.org/abs/2004.14975},
	abstract = {While probing is a common technique for identifying knowledge in the representations of pretrained models, it is unclear whether this technique can explain the downstream success of models like {BERT} which are trained end-to-end during finetuning. To address this question, we compare probing with a different measure of transferability: the decrease in finetuning performance of a partially-reinitialized model. This technique reveals that in {BERT}, layers with high probing accuracy on downstream {GLUE} tasks are neither necessary nor sufficient for high accuracy on those tasks. In addition, dataset size impacts layer transferability: the less finetuning data one has, the more important the middle and later layers of {BERT} become. Furthermore, {BERT} does not simply find a better initializer for individual layers; instead, interactions between layers matter and reordering {BERT}'s layers prior to finetuning significantly harms evaluation metrics. These results provide a way of understanding the transferability of parameters in pretrained language models, revealing the fluidity and complexity of transfer learning in these models.},
	journaltitle = {{arXiv}:2004.14975 [cs]},
	author = {Tamkin, Alex and Singh, Trisha and Giovanardi, Davide and Goodman, Noah},
	urldate = {2020-05-02},
	date = {2020-04-30},
	eprinttype = {arxiv},
	eprint = {2004.14975},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{belinkov_synthetic_2018,
	title = {Synthetic and Natural Noise Both Break Neural Machine Translation},
	url = {http://arxiv.org/abs/1711.02173},
	abstract = {Character-based neural machine translation ({NMT}) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront {NMT} models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.},
	journaltitle = {{arXiv}:1711.02173 [cs]},
	author = {Belinkov, Yonatan and Bisk, Yonatan},
	urldate = {2020-05-02},
	date = {2018-02-24},
	eprinttype = {arxiv},
	eprint = {1711.02173},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7},
}

@article{andreas_good-enough_2020,
	title = {Good-Enough Compositional Data Augmentation},
	url = {http://arxiv.org/abs/1904.09545},
	abstract = {We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87\% on diagnostic tasks from the {SCAN} dataset and 16\% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1\% on small corpora in several languages.},
	journaltitle = {{arXiv}:1904.09545 [cs]},
	author = {Andreas, Jacob},
	urldate = {2020-05-01},
	date = {2020-04-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.09545},
	keywords = {Computer Science - Computation and Language},
}
@article{kulmizev_neural_2020,
	title = {Do Neural Language Models Show Preferences for Syntactic Formalisms?},
	url = {http://arxiv.org/abs/2004.14096},
	abstract = {Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages. We apply a probe for extracting directed dependency trees to {BERT} and {ELMo} models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies ({UD}), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies ({SUD}), focusing on surface structure. We ﬁnd that both models exhibit a preference for {UD} over {SUD} — with interesting variations across languages and layers — and that the strength of this preference is correlated with differences in tree shape.},
	journaltitle = {{arXiv}:2004.14096 [cs]},
	author = {Kulmizev, Artur and Ravishankar, Vinit and Abdou, Mostafa and Nivre, Joakim},
	urldate = {2020-04-30},
	date = {2020-04-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.14096},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@online{li_exponential_2020,
	title = {{AN} {EXPONENTIAL} {LEARNING} {RATE} {SCHEDULE} {FOR} {BATCH} {NORMALIZED} {NETWORKS}},
	url = {https://www.semanticscholar.org/paper/AN-EXPONENTIAL-LEARNING-RATE-SCHEDULE-FOR-BATCH-Li-Arora/b068e16f2a0900c2825e18862972ae5e6c23495e?utm_source=alert_email&utm_content=AuthorPaper&utm_campaign=AlertEmails_DAILY&utm_term=AuthorPaper+AuthorCitation&email_index=1-0-1&utm_medium=213713},
	abstract = {Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or {BN}(Ioffe \&amp; Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about {BN} with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone {BN} (Ioffe \&amp; Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018) • Training can be done using {SGD} with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + α) factor in every epoch for some α \&gt; 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization. • Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of {BN} + {SGD} + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu \&amp; He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc. • A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or {BN} alone reaches global minimum, but convergence fails when both are used.},
	titleaddon = {undefined},
	author = {Li, Zhongyuan and Arora, Sanjeev},
	urldate = {2020-04-30},
	date = {2020},
	langid = {english},
	note = {Library Catalog: www.semanticscholar.org},
}

@article{yu_neural_2017,
	title = {The Neural Noisy Channel},
	url = {http://arxiv.org/abs/1611.02554},
	abstract = {We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.},
	journaltitle = {{arXiv}:1611.02554 [cs]},
	author = {Yu, Lei and Blunsom, Phil and Dyer, Chris and Grefenstette, Edward and Kocisky, Tomas},
	urldate = {2020-04-30},
	date = {2017-03-06},
	eprinttype = {arxiv},
	eprint = {1611.02554},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{kong_mutual_2019,
	title = {A Mutual Information Maximization Perspective of Language Representation Learning},
	url = {https://openreview.net/forum?id=Syx79eBKwr},
	abstract = {We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a...},
	eventtitle = {International Conference on Learning Representations},
	author = {Kong, Lingpeng and D{\textbackslash}\&{\textbackslash}\#x27, Cyprien de Masson and Autume and Yu, Lei and Ling, Wang and Dai, Zihang and Yogatama, Dani},
	urldate = {2020-04-29},
	date = {2019-09-25},
}

@inproceedings{rosenfeld_constructive_2019,
	title = {A Constructive Prediction of the Generalization Error Across Scales},
	url = {https://openreview.net/forum?id=ryenvpEKDr},
	abstract = {The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless,...},
	eventtitle = {International Conference on Learning Representations},
	author = {Rosenfeld, Jonathan S. and Rosenfeld, Amir and Belinkov, Yonatan and Shavit, Nir},
	urldate = {2020-04-29},
	date = {2019-09-25},
}

@article{nagarajan_uniform_2019,
	title = {Uniform convergence may be unable to explain generalization in deep learning},
	url = {https://arxiv.org/abs/1902.04742v3},
	abstract = {Aimed at explaining the surprisingly good generalization behavior of
overparameterized deep networks, recent works have developed a variety of
generalization bounds for deep learning, all based on the fundamental
learning-theoretic technique of uniform convergence. While it is well-known
that many of these existing bounds are numerically large, through numerous
experiments, we bring to light a more concerning aspect of these bounds: in
practice, these bounds can \{{\textbackslash}em increase\} with the training dataset size.
Guided by our observations, we then present examples of overparameterized
linear classifiers and neural networks trained by gradient descent ({GD}) where
uniform convergence provably cannot ``explain generalization'' -- even if we
take into account the implicit bias of {GD} \{{\textbackslash}em to the fullest extent possible\}.
More precisely, even if we consider only the set of classifiers output by {GD},
which have test errors less than some small \$ε\$ in our settings, we show
that applying (two-sided) uniform convergence on this set of classifiers will
yield only a vacuous generalization guarantee larger than \$1-ε\$. Through
these findings, we cast doubt on the power of uniform convergence-based
generalization bounds to provide a complete picture of why overparameterized
deep networks generalize well.},
	author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
	urldate = {2020-04-28},
	date = {2019-02-13},
	langid = {english},
}

@inproceedings{hu_provable_2020,
	title = {Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks},
	url = {https://iclr.cc/virtual/poster_rkgqN1SYvr.html},
	abstract = {The selection of initial parameter values for gradient-based optimization of deep neural networks is one of the most impactful hyperparameter choices in deep learning systems, affecting both convergence times and model performance. Yet despite significant empirical and theoretical analysis, relatively little has been proved about the concrete effects of different initialization schemes. In this work, we analyze the effect of initialization in deep linear networks, and provide for the first time a rigorous proof that drawing the initial weights from the orthogonal group speeds up convergence relative to the standard Gaussian initialization with iid weights. We show that for deep networks, the width needed for efficient convergence to a global minimum with orthogonal initializations is independent of the depth, whereas the width needed for efficient convergence with Gaussian initializations scales linearly in the depth. Our results demonstrate how the benefits of a good initialization can persist throughout learning, suggesting an explanation for the recent empirical successes found by initializing very deep non-linear networks according to the principle of dynamical isometry.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Hu, Wei and Xiao, Lechao and Pennington, Jeffrey},
	urldate = {2020-04-27},
	date = {2020-04},
	langid = {english},
}

@inproceedings{zhang_why_2020,
	title = {Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
	url = {https://iclr.cc/virtual/poster_BJgnXpVYwS.html},
	shorttitle = {Why Gradient Clipping Accelerates Training},
	abstract = {We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
	urldate = {2020-04-27},
	date = {2020-04},
	langid = {english},
}

@online{hardt_exponential_nodate,
	title = {Exponential Learning Rate Schedules for Deep Learning (Part 1)},
	url = {http://offconvex.github.io/2020/04/24/ExpLR1/},
	abstract = {Algorithms off the convex path.},
	titleaddon = {Off the convex path},
	author = {Hardt, Moritz},
	urldate = {2020-04-26},
}

@online{hardt_can_nodate,
	title = {Can increasing depth serve to accelerate optimization?},
	url = {http://offconvex.github.io/2018/03/02/acceleration-overparameterization/},
	abstract = {Algorithms off the convex path.},
	titleaddon = {Off the convex path},
	author = {Hardt, Moritz},
	urldate = {2020-04-26},
}

@incollection{maheswaranathan_reverse_2019,
	title = {Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics},
	url = {http://papers.nips.cc/paper/9700-reverse-engineering-recurrent-networks-for-sentiment-classification-reveals-line-attractor-dynamics.pdf},
	pages = {15696--15705},
	booktitle = {Advances in Neural Information Processing Systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d\{{\textbackslash}textbackslash\}textquotesingle and Fox, E. and Garnett, R.},
	urldate = {2020-04-22},
	date = {2019},
}

@article{wilson_bayesian_2020,
	title = {Bayesian Deep Learning and a Probabilistic Perspective of Generalization},
	url = {http://arxiv.org/abs/2002.08791},
	abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
	journaltitle = {{arXiv}:2002.08791 [cs, stat]},
	author = {Wilson, Andrew Gordon and Izmailov, Pavel},
	urldate = {2020-04-19},
	date = {2020-03-17},
	eprinttype = {arxiv},
	eprint = {2002.08791},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{schwartz_right_2020,
	title = {The Right Tool for the Job: Matching Model and Instance Complexities},
	url = {http://arxiv.org/abs/2004.07453},
	shorttitle = {The Right Tool for the Job},
	abstract = {As {NLP} models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) "exit" from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of {BERT} and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline {BERT} model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.},
	journaltitle = {{arXiv}:2004.07453 [cs]},
	author = {Schwartz, Roy and Stanovsky, Gabi and Swayamdipta, Swabha and Dodge, Jesse and Smith, Noah A.},
	urldate = {2020-04-17},
	date = {2020-04-16},
	eprinttype = {arxiv},
	eprint = {2004.07453},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{dalvi_exploiting_2020,
	title = {Exploiting Redundancy in Pre-trained Language Models for Efficient Transfer Learning},
	url = {http://arxiv.org/abs/2004.04010},
	abstract = {Large pre-trained contextual word representations have transformed the ﬁeld of natural language processing, obtaining impressive results on a wide range of tasks. However, as models increase in size, computational limitations make them impractical for researchers and practitioners alike. We hypothesize that contextual representations have both intrinsic and task-speciﬁc redundancies. We propose a novel feature selection method, which takes advantage of these redundancies to reduce the size of the pre-trained features. In a comprehensive evaluation on two pre-trained models, {BERT} and {XLNet}, using a diverse suite of sequence labeling and sequence classiﬁcation tasks, our method reduces the feature set down to 1–7\% of the original size, while maintaining more than 97\% of the performance.},
	journaltitle = {{arXiv}:2004.04010 [cs]},
	author = {Dalvi, Fahim and Sajjad, Hassan and Durrani, Nadir and Belinkov, Yonatan},
	urldate = {2020-04-10},
	date = {2020-04-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.04010},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{renda_comparing_2020,
	title = {Comparing Rewinding and Fine-tuning in Neural Network Pruning},
	url = {http://arxiv.org/abs/2003.02389},
	abstract = {Many neural network pruning algorithms proceed in three steps: train the network to completion, remove unwanted structure to compress the network, and retrain the remaining structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned weights from their final trained values using a small fixed learning rate. In this paper, we compare fine-tuning to alternative retraining techniques. Weight rewinding (as proposed by Frankle et al., (2019)), rewinds unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate rewinding (which we propose) trains the unpruned weights from their final values using the same learning rate schedule as weight rewinding. Both rewinding techniques outperform fine-tuning, forming the basis of a network-agnostic pruning algorithm that matches the accuracy and compression ratios of several more network-specific state-of-the-art techniques.},
	journaltitle = {{arXiv}:2003.02389 [cs, stat]},
	author = {Renda, Alex and Frankle, Jonathan and Carbin, Michael},
	urldate = {2020-04-07},
	date = {2020-03-04},
	eprinttype = {arxiv},
	eprint = {2003.02389},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{davis_network_nodate,
	title = {{ON} {NETWORK} {SCIENCE} {AND} {MUTUAL} {INFORMATION} {FOR} {EXPLAINING} {DEEP} {NEURAL} {NETWORKS}},
	abstract = {In this paper, we present a new approach to interpret deep learning models. By coupling mutual information with network science, we explore how information ﬂows through feedforward networks. We show that efﬁciently approximating mutual information allows us to create an information measure that quantiﬁes how much information ﬂows between any two neurons of a deep learning model. To that end, we propose {NIF}, Neural Information Flow, a technique for codifying information ﬂow that exposes deep learning model internals and provides feature attributions.},
	pages = {5},
	author = {Davis, Brian and Bhatt, Umang and Bhardwaj, Kartikeya and Marculescu, Radu and Moura, Jose M F},
	langid = {english},
}

@article{zhang_assessing_2020,
	title = {Assessing the Memory Ability of Recurrent Neural Networks},
	url = {http://arxiv.org/abs/2002.07422},
	abstract = {It is known that Recurrent Neural Networks ({RNNs}) can remember, in their hidden layers, part of the semantic information expressed by a sequence (e.g., a sentence) that is being processed. Different types of recurrent units have been designed to enable {RNNs} to remember information over longer time spans. However, the memory abilities of different recurrent units are still theoretically and empirically unclear, thus limiting the development of more effective and explainable {RNNs}. To tackle the problem, in this paper, we identify and analyze the internal and external factors that affect the memory ability of {RNNs}, and propose a Semantic Euclidean Space to represent the semantics expressed by a sequence. Based on the Semantic Euclidean Space, a series of evaluation indicators are deﬁned to measure the memory abilities of different recurrent units and analyze their limitations. These evaluation indicators also provide a useful guidance to select suitable sequence lengths for different {RNNs} during training.},
	journaltitle = {{arXiv}:2002.07422 [cs]},
	author = {Zhang, Cheng and Li, Qiuchi and Hua, Lingyu and Song, Dawei},
	urldate = {2020-04-07},
	date = {2020-02-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.07422},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{gebhart_characterizing_2019,
	title = {Characterizing the Shape of Activation Space in Deep Neural Networks},
	doi = {10.1109/ICMLA.2019.00254},
	abstract = {The representations learned by deep neural networks are difficult to interpret in part due to their large parameter space and the complexities introduced by their multi-layer structure. We introduce a method for computing persistent homology over the graphical activation structure of neural networks, which provides access to the task-relevant substructures activated throughout the network for a given input. This topological perspective provides unique insights into the distributed representations encoded by neural networks in terms of the shape of their activation structures. We demonstrate the value of this approach by showing an alternative explanation for the existence of adversarial examples. By studying the topology of network activations across multiple architectures and datasets, we find that adversarial perturbations do not add activations that target the semantic structure of the adversarial class as previously hypothesized. Rather, adversarial examples are explainable as alterations to the dominant activation structures induced by the original image, suggesting the class representations learned by deep networks are problematically sparse on the input space.},
	eventtitle = {2019 18th {IEEE} International Conference On Machine Learning And Applications ({ICMLA})},
	pages = {1537--1542},
	booktitle = {2019 18th {IEEE} International Conference On Machine Learning And Applications ({ICMLA})},
	author = {Gebhart, Thomas and Schrater, Paul and Hylton, Alan},
	date = {2019-12},
	keywords = {Adversarial Examples, Biological neural networks, Deep Learning, Network topology, Neural Networks, Neurons, Persistent Homology, Semantics, Task analysis, Topology, activation space, adversarial perturbations, class representations, computing persistent homology, deep neural networks, distributed representations, dominant activation structures, graphical activation structure, image representation, learning (artificial intelligence), multilayer structure, network activations, neural nets, semantic structure, task-relevant substructures, topology},
}

@report{arehalli_neural_2020,
	title = {Neural Language Models Capture Some, But Not All, Agreement Attraction Effects},
	url = {https://osf.io/97qcg},
	abstract = {The number of the subject in English must match the number of the corresponding verb (dog runs but dogs run). Yet in real-time language production and comprehension, speakers often mistakenly compute agreement between the verb and a grammatically irrelevant non-subject noun phrase instead. This phenomenon, referred to as agreement attraction, is modulated by a wide range of factors; any complete computational model of grammatical planning and comprehension would be expected to derive this rich empirical picture. Recent developments in Natural Language Processing have shown that neural networks trained only on word-prediction over large corpora are capable of capturing subject-verb agreement dependencies to a significant extent, but with occasional errors. The goal of this paper is to evaluate the potential of such neural word prediction models as a foundation for a cognitive model of real-time grammatical processing. We simulate six experiments taken from the agreement attraction literature with {LSTMs}, one common type of neural language model. The {LSTMs} captured the critical human behavior in three of them, indicating that (1) some agreement attraction phenomena can be captured by a generic sequence processing model, but (2) capturing the other phenomena may require models with more language-specific mechanisms},
	institution = {{PsyArXiv}},
	type = {preprint},
	author = {Arehalli, Suhas and Linzen, Tal},
	urldate = {2020-04-07},
	date = {2020-02-12},
	doi = {10.31234/osf.io/97qcg},
}

@inproceedings{mccoy_right_2019,
	location = {Florence, Italy},
	title = {Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},
	url = {https://www.aclweb.org/anthology/P19-1334},
	doi = {10.18653/v1/P19-1334},
	shorttitle = {Right for the Wrong Reasons},
	abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference ({NLI}), the task of determining whether one sentence entails another. We hypothesize that statistical {NLI} models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called {HANS} (Heuristic Analysis for {NLI} Systems), which contains many examples where the heuristics fail. We find that models trained on {MNLI}, including {BERT}, a state-of-the-art model, perform very poorly on {HANS}, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in {NLI} systems, and that the {HANS} dataset can motivate and measure progress in this area.},
	eventtitle = {{ACL} 2019},
	pages = {3428--3448},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {{McCoy}, Tom and Pavlick, Ellie and Linzen, Tal},
	urldate = {2020-04-07},
	date = {2019-07},
}

@article{pimentel_information-theoretic_nodate,
	title = {Information-Theoretic Probing for Linguistic Structure},
	abstract = {The success of neural networks on a diverse set of {NLP} tasks has led researchers to question how much do these networks actually know about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotation in that linguistic task from the network’s learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that such models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic formalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate. The empirical portion of our paper focuses on obtaining tight estimates for how much information {BERT} knows about parts of speech in a set of ﬁve typologically diverse languages that are often underrepresented in parsing research, plus English, totaling six languages. We ﬁnd {BERT} accounts for only at most 5\% more information than traditional, type-based word embeddings.},
	pages = {12},
	author = {Pimentel, Tiago and Valvoda, Josef and Maudslay, Rowan Hall and Zmigrod, Ran and Williams, Adina and Cotterell, Ryan},
	langid = {english},
}

@article{li_visualizing_2020,
	title = {Visualizing Neural Networks with the Grand Tour},
	volume = {5},
	issn = {2476-0757},
	url = {https://distill.pub/2020/grand-tour},
	doi = {10.23915/distill.00025},
	abstract = {By focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks.},
	pages = {e25},
	number = {3},
	journaltitle = {Distill},
	shortjournal = {Distill},
	author = {Li, Mingwei and Zhao, Zhenge and Scheidegger, Carlos},
	urldate = {2020-04-05},
	date = {2020-03-16},
	langid = {english},
}

@article{raganato_fixed_2020,
	title = {Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation},
	url = {http://arxiv.org/abs/2002.10260},
	abstract = {Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that attention heads learn simple positional patterns which are often redundant. In this paper, we propose to replace all but one attention head of each encoder layer with ﬁxed – non-learnable – attentive patterns that are solely based on position and do not require any external knowledge. Our experiments show that ﬁxing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases {BLEU} scores by up to 3 points in low-resource scenarios.},
	journaltitle = {{arXiv}:2002.10260 [cs]},
	author = {Raganato, Alessandro and Scherrer, Yves and Tiedemann, Jörg},
	urldate = {2020-04-05},
	date = {2020-02-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.10260},
	keywords = {Computer Science - Computation and Language},
}

@article{hao_visualizing_2019,
	title = {Visualizing and Understanding the Effectiveness of {BERT}},
	url = {http://arxiv.org/abs/1908.05620},
	abstract = {Language model pre-training, such as {BERT}, has achieved remarkable results in many {NLP} tasks. However, it is unclear why the pretraining-then-ﬁne-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and optimization trajectories of ﬁne-tuning {BERT} on speciﬁc datasets. First, we ﬁnd that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch. We also demonstrate that the ﬁnetuning procedure is robust to overﬁtting, even though {BERT} is highly over-parameterized for downstream tasks. Second, the visualization results indicate that ﬁne-tuning {BERT} tends to generalize better because of the ﬂat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of {BERT} are more invariant during ﬁne-tuning, which suggests that the layers that are close to input learn more transferable representations of language.},
	journaltitle = {{arXiv}:1908.05620 [cs]},
	author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
	urldate = {2020-04-05},
	date = {2019-08-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1908.05620},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@online{ethayarajh_bert_2020,
	title = {{BERT}, {ELMo}, \& {GPT}-2: How contextual are contextualized word representations?},
	url = {https://kawine.github.io/blog/nlp/2020/02/03/contextual.html},
	shorttitle = {{BERT}, {ELMo}, \& {GPT}-2},
	abstract = {Incorporating context into word embeddings - as exemplified by {BERT}, {ELMo}, and {GPT}-2 - has proven to be a watershed idea in {NLP}. Replacing static vectors (e.g., word2vec) with contextualized word representations has led to significant improvements on virtually every {NLP} task.},
	titleaddon = {Kawin Ethayarajh},
	author = {Ethayarajh, Kawin},
	urldate = {2020-04-05},
	date = {2020-02-03},
	langid = {english},
}

@article{talmor_olmpics_2019,
	title = {{oLMpics} -- On what Language Model Pre-training Captures},
	url = {http://arxiv.org/abs/1912.13283},
	abstract = {Recent success of pre-trained language models ({LMs}) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether {LM} representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a {LM} on a task should be attributed to the pre-trained representations or to the process of ﬁne-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no ﬁne-tuning), as well as comparing the learning curve of a ﬁne-tuned {LM} to the learning curve of multiple controls, which paints a rich picture of the {LM} capabilities. Our main ﬁndings are that: (a) different {LMs} exhibit qualitatively different reasoning abilities, e.g., {ROBERTA} succeeds in reasoning tasks where {BERT} fails completely; (b) {LMs} do not reason in an abstract manner and are context-dependent, e.g., while {ROBERTA} can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our ﬁndings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.},
	journaltitle = {{arXiv}:1912.13283 [cs]},
	author = {Talmor, Alon and Elazar, Yanai and Goldberg, Yoav and Berant, Jonathan},
	urldate = {2020-04-05},
	date = {2019-12-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1912.13283},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{pruthi_learning_nodate,
	title = {Learning to Deceive with Attention-Based Explanations},
	abstract = {Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention’s reliability as a tool for auditing algorithms in the context of fairness and accountability1.},
	pages = {11},
	author = {Pruthi, Danish and Gupta, Mansi and Dhingra, Bhuwan and Neubig, Graham and Lipton, Zachary C},
	langid = {english},
}

@article{advani_high-dimensional_2017,
	title = {High-dimensional dynamics of generalization error in neural networks},
	url = {http://arxiv.org/abs/1710.03667},
	abstract = {We perform an average case analysis of the generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant "high-dimensional" regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that naive application of worst-case theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
	journaltitle = {{arXiv}:1710.03667 [physics, q-bio, stat]},
	author = {Advani, Madhu S. and Saxe, Andrew M.},
	urldate = {2020-03-25},
	date = {2017-10-10},
	eprinttype = {arxiv},
	eprint = {1710.03667},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{poerner_evaluating_2019,
	title = {Evaluating neural network explanation methods using hybrid documents and morphological agreement},
	url = {http://arxiv.org/abs/1801.06422},
	abstract = {The behavior of deep neural networks ({DNNs}) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for {NLP}. To this end, we design two novel evaluation paradigms that cover two important classes of {NLP} problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce {LIMSSE}, an explanation method inspired by {LIME} that is designed for {NLP}. We show empirically that {LIMSSE}, {LRP} and {DeepLIFT} are the most effective explanation methods and recommend them for explaining {DNNs} in {NLP}.},
	journaltitle = {{arXiv}:1801.06422 [cs]},
	author = {Poerner, Nina and Roth, Benjamin and Schütze, Hinrich},
	urldate = {2020-03-22},
	date = {2019-05-06},
	eprinttype = {arxiv},
	eprint = {1801.06422},
	keywords = {Computer Science - Computation and Language},
}

@article{edwards_estimating_2020,
	title = {Estimating Q(s,s') with Deep Deterministic Dynamics Gradients},
	url = {http://arxiv.org/abs/2002.09505},
	abstract = {In this paper, we introduce a novel form of value function, \$Q(s, s')\$, that expresses the utility of transitioning from a state \$s\$ to a neighboring state \$s'\$ and then acting optimally thereafter. In order to derive an optimal policy, we develop a forward dynamics model that learns to make next-state predictions that maximize this value. This formulation decouples actions from values while still learning off-policy. We highlight the benefits of this approach in terms of value function transfer, learning within redundant action spaces, and learning off-policy from state observations generated by sub-optimal or completely random policies. Code and videos are available at {\textbackslash}url\{sites.google.com/view/qss-paper\}.},
	journaltitle = {{arXiv}:2002.09505 [cs, stat]},
	author = {Edwards, Ashley D. and Sahni, Himanshu and Liu, Rosanne and Hung, Jane and Jain, Ankit and Wang, Rui and Ecoffet, Adrien and Miconi, Thomas and Isbell, Charles and Yosinski, Jason},
	urldate = {2020-03-22},
	date = {2020-02-21},
	eprinttype = {arxiv},
	eprint = {2002.09505},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{unterthiner_predicting_2020,
	title = {Predicting Neural Network Accuracy from Weights},
	url = {http://arxiv.org/abs/2002.11448},
	abstract = {We study the prediction of the accuracy of a neural network given only its weights with the goal of better understanding network training and performance. To do so, we propose a formal setting which frames this task and connects to previous work in this area. We collect (and release) a large dataset of almost 80k convolutional neural networks trained on four image datasets. We demonstrate that strong predictors of accuracy exist. Moreover, they can achieve good predictions while only using simple statistics of the weights. Surprisingly, these predictors are able to rank networks trained on unobserved datasets or using different architectures.},
	journaltitle = {{arXiv}:2002.11448 [cs, stat]},
	author = {Unterthiner, Thomas and Keysers, Daniel and Gelly, Sylvain and Bousquet, Olivier and Tolstikhin, Ilya},
	urldate = {2020-03-22},
	date = {2020-02-26},
	eprinttype = {arxiv},
	eprint = {2002.11448},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kovaleva_revealing_2019,
	title = {Revealing the Dark Secrets of {BERT}},
	url = {http://arxiv.org/abs/1908.08593},
	abstract = {{BERT}-based architectures currently give state-of-the-art performance on many {NLP} tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of {BERT}. Using a subset of {GLUE} tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual {BERT}'s heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned {BERT} models.},
	journaltitle = {{arXiv}:1908.08593 [cs, stat]},
	author = {Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
	urldate = {2020-03-22},
	date = {2019-09-11},
	eprinttype = {arxiv},
	eprint = {1908.08593},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dodge_fine-tuning_2020,
	title = {Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping},
	url = {http://arxiv.org/abs/2002.06305},
	shorttitle = {Fine-Tuning Pretrained Language Models},
	abstract = {Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the {GLUE} benchmark, fine-tuning {BERT} hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning.},
	journaltitle = {{arXiv}:2002.06305 [cs]},
	author = {Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
	urldate = {2020-03-22},
	date = {2020-02-14},
	eprinttype = {arxiv},
	eprint = {2002.06305},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{li_train_2020,
	title = {Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers},
	url = {http://arxiv.org/abs/2002.11794},
	shorttitle = {Train Large, Then Compress},
	abstract = {Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for {NLP} tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.},
	journaltitle = {{arXiv}:2002.11794 [cs]},
	author = {Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joseph E.},
	urldate = {2020-03-22},
	date = {2020-02-26},
	eprinttype = {arxiv},
	eprint = {2002.11794},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{schein_poisson-randomized_2019,
	title = {Poisson-Randomized Gamma Dynamical Systems},
	url = {http://arxiv.org/abs/1910.12991},
	abstract = {This paper presents the Poisson-randomized gamma dynamical system ({PRGDS}), a model for sequentially observed count tensors that encodes a strong inductive bias toward sparsity and burstiness. The {PRGDS} is based on a new motif in Bayesian latent variable modeling, an alternating chain of discrete Poisson and continuous gamma latent states that is analytically convenient and computationally tractable. This motif yields closed-form complete conditionals for all variables by way of the Bessel distribution and a novel discrete distribution that we call the shifted confluent hypergeometric distribution. We draw connections to closely related models and compare the {PRGDS} to these models in studies of real-world count data sets of text, international events, and neural spike trains. We find that a sparse variant of the {PRGDS}, which allows the continuous gamma latent states to take values of exactly zero, often obtains better predictive performance than other models and is uniquely capable of inferring latent structures that are highly localized in time.},
	journaltitle = {{arXiv}:1910.12991 [cs, stat]},
	author = {Schein, Aaron and Linderman, Scott W. and Zhou, Mingyuan and Blei, David M. and Wallach, Hanna},
	urldate = {2020-01-20},
	date = {2019-10-28},
	eprinttype = {arxiv},
	eprint = {1910.12991},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{allen_understanding_2019,
	title = {On Understanding Knowledge Graph Representation},
	url = {http://arxiv.org/abs/1909.11611},
	abstract = {Many methods have been developed to represent knowledge graph data, which implicitly exploit low-rank latent structure in the data to encode known information and enable unknown facts to be inferred. To predict whether a relationship holds between entities, their embeddings are typically compared in the latent space following a relation-speciﬁc mapping. Whilst link prediction has steadily improved, the latent structure, and hence why such models capture semantic information, remains unexplained. We build on recent theoretical interpretation of word embeddings as a basis to consider an explicit structure for representations of relations between entities. For identiﬁable relation types, we are able to predict properties and justify the relative performance of leading knowledge graph representation methods, including their often overlooked ability to make independent predictions.},
	journaltitle = {{arXiv}:1909.11611 [cs, stat]},
	author = {Allen, Carl and Balazevic, Ivana and Hospedales, Timothy M.},
	urldate = {2020-01-11},
	date = {2019-09-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.11611},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hestness_deep_2017,
	title = {Deep Learning Scaling is Predictable, Empirically},
	url = {http://arxiv.org/abs/1712.00409},
	abstract = {Deep learning ({DL}) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As {DL} application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
	journaltitle = {{arXiv}:1712.00409 [cs, stat]},
	author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
	urldate = {2020-01-10},
	date = {2017-12-01},
	eprinttype = {arxiv},
	eprint = {1712.00409},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hendriks_acquisition_2020,
	title = {The acquisition of compositional meaning},
	volume = {375},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0312},
	doi = {10.1098/rstb.2019.0312},
	abstract = {How do people create meaning from a string of sounds or pattern of dots? Insights into this process can be obtained from the way children acquire sentence meanings. According to the well-known principle of compositionality, the meaning of an expression is a function of the meanings of its parts and the way they are syntactically combined. However, children frequently seem to ignore syntactic structure in their sentence interpretations, suggesting that syntax is merely one of the sources of information constraining meaning and does not have a special status. A fundamental assumption in the argument in favour of compositionality is that speakers and listeners generally agree upon the meanings of sentences. Remarkably, however, children as listeners do not always understand what they are able to produce as speakers, and vice versa. For example, children's production of word order appears to develop ahead of their comprehension of word order in the acquisition of languages like English and Dutch. Such production–comprehension asymmetries are not uncommon in child language and motivate a view of compositionality as a principle pertaining to the result of perspective taking, and of meaning composition as a process of speaker–listener coordination.This article is part of the theme issue ‘Towards mechanistic models of meaning composition’.},
	pages = {20190312},
	number = {1791},
	journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	shortjournal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Hendriks, Petra},
	urldate = {2020-01-09},
	date = {2020-02-03},
}

@article{martin_tensors_2020,
	title = {Tensors and compositionality in neural systems},
	volume = {375},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0306},
	doi = {10.1098/rstb.2019.0306},
	abstract = {Neither neurobiological nor process models of meaning composition specify the operator through which constituent parts are bound together into compositional structures. In this paper, we argue that a neurophysiological computation system cannot achieve the compositionality exhibited in human thought and language if it were to rely on a multiplicative operator to perform binding, as the tensor product ({TP})-based systems that have been widely adopted in cognitive science, neuroscience and artificial intelligence do. We show via simulation and two behavioural experiments that {TPs} violate variable-value independence, but human behaviour does not. Specifically, {TPs} fail to capture that in the statements fuzzy cactus and fuzzy penguin, both cactus and penguin are predicated by fuzzy(x) and belong to the set of fuzzy things, rendering these arguments similar to each other. Consistent with that thesis, people judged arguments that shared the same role to be similar, even when those arguments themselves (e.g., cacti and penguins) were judged to be dissimilar when in isolation. By contrast, the similarity of the {TPs} representing fuzzy(cactus) and fuzzy(penguin) was determined by the similarity of the arguments, which in this case approaches zero. Based on these results, we argue that neural systems that use {TPs} for binding cannot approximate how the human mind and brain represent compositional information during processing. We describe a contrasting binding mechanism that any physiological or artificial neural system could use to maintain independence between a role and its argument, a prerequisite for compositionality and, thus, for instantiating the expressive power of human thought and language in a neural system.This article is part of the theme issue ‘Towards mechanistic models of meaning composition’.},
	pages = {20190306},
	number = {1791},
	journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	shortjournal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Martin, Andrea E. and Doumas, Leonidas A. A.},
	urldate = {2020-01-09},
	date = {2020-02-03},
}

@article{martin_modelling_2020,
	title = {Modelling meaning composition from formalism to mechanism},
	volume = {375},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0298},
	doi = {10.1098/rstb.2019.0298},
	abstract = {Human thought and language have extraordinary expressive power because meaningful parts can be assembled into more complex semantic structures. This partly underlies our ability to compose meanings into endlessly novel configurations, and sets us apart from other species and current computing devices. Crucially, human behaviour, including language use and linguistic data, indicates that composing parts into complex structures does not threaten the existence of constituent parts as independent units in the system: parts and wholes exist simultaneously yet independently from one another in the mind and brain. This independence is evident in human behaviour, but it seems at odds with what is known about the brain's exquisite sensitivity to statistical patterns: everyday language use is productive and expressive precisely because it can go beyond statistical regularities. Formal theories in philosophy and linguistics explain this fact by assuming that language and thought are compositional: systems of representations that separate a variable (or role) from its values (fillers), such that the meaning of a complex expression is a function of the values assigned to the variables. The debate on whether and how compositional systems could be implemented in minds, brains and machines remains vigorous. However, it has not yet resulted in mechanistic models of semantic composition: how, then, are the constituents of thoughts and sentences put and held together? We review and discuss current efforts at understanding this problem, and we chart possible routes for future research.This article is part of the theme issue ‘Towards mechanistic models of meaning composition’.},
	pages = {20190298},
	number = {1791},
	journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	shortjournal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Martin, Andrea E. and Baggio, Giosuè},
	urldate = {2020-01-09},
	date = {2020-02-03},
}

@article{kim_interpretability_2018,
	title = {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors ({TCAV})},
	url = {http://arxiv.org/abs/1711.11279},
	shorttitle = {Interpretability Beyond Feature Attribution},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors ({CAVs}), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use {CAVs} as part of a technique, Testing with {CAVs} ({TCAV}), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how {CAVs} may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
	journaltitle = {{arXiv}:1711.11279 [stat]},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	urldate = {2020-01-08},
	date = {2018-06-07},
	eprinttype = {arxiv},
	eprint = {1711.11279},
	keywords = {Statistics - Machine Learning},
}

@article{zhang_dive_nodate,
	title = {Dive into Deep Learning},
	pages = {904},
	author = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
	langid = {english},
}
@article{ilyas_adversarial_2019,
	title = {Adversarial Examples Are Not Bugs, They Are Features},
	url = {http://arxiv.org/abs/1905.02175},
	abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
	journaltitle = {{arXiv}:1905.02175 [cs, stat]},
	author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	urldate = {2020-01-04},
	date = {2019-08-12},
	eprinttype = {arxiv},
	eprint = {1905.02175},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zhou_understanding_2019,
	title = {Understanding Knowledge Distillation in Non-autoregressive Machine Translation},
	url = {https://openreview.net/forum?id=BygFVAEKDH},
	abstract = {Non-autoregressive machine translation ({NAT}) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models....},
	eventtitle = {International Conference on Learning Representations},
	author = {Zhou, Chunting and Gu, Jiatao and Neubig, Graham},
	urldate = {2019-12-28},
	date = {2019-09-25},
}

@article{zhang_expressivity_2019,
	title = {The Expressivity and Training of Deep Neural Networks: toward the Edge of Chaos?},
	url = {http://arxiv.org/abs/1910.04970},
	shorttitle = {The Expressivity and Training of Deep Neural Networks},
	abstract = {Expressivity is one of the most signiﬁcant issues in assessing neural networks. In this paper, we provide a quantitative analysis of the expressivity for the deep neural network ({DNN}) from its dynamic model, where the Hilbert space is employed to analyze the convergence and criticality. We study the feature mapping of several widely used activation functions obtained by Hermite polynomials, and ﬁnd sharp declines or even saddle points in the feature space, which stagnate the information transfer in {DNNs}. We then present a new activation function design based on the Hermite polynomials for better utilization of spatial representation. Moreover, we analyze the information transfer of {DNNs}, emphasizing the convergence problem caused by the mismatch between input and topological structure. We also study the effects of input perturbations and regularization operators on critical expressivity. Our theoretical analysis reveals that {DNNs} use spatial domains for information representation and evolve to the edge of chaos as depth increases. In actual training, whether a particular network can ultimately arrive the edge of chaos depends on its ability to overcome convergence and pass information to the required network depth. Finally, we demonstrate the empirical performance of the proposed hypothesis via multivariate time series prediction and image classiﬁcation examples.},
	journaltitle = {{arXiv}:1910.04970 [cs, stat]},
	author = {Zhang, Gege and Li, Gangwei and Shen, Ningwei and Zhang, Weidong},
	urldate = {2019-12-25},
	date = {2019-12-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.04970},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{xing_walk_2018,
	title = {A Walk with {SGD}},
	url = {http://arxiv.org/abs/1802.08770},
	abstract = {We present novel empirical observations regarding how stochastic gradient descent ({SGD}) navigates the loss landscape of over-parametrized deep neural networks ({DNNs}). These observations expose the qualitatively different roles of learning rate and batch-size in {DNN} optimization and generalization. Speciﬁcally we study the {DNN} loss surface along the trajectory of {SGD} by interpolating the loss surface between parameters from consecutive iterations and tracking various metrics during training. We ﬁnd that the loss interpolation between parameters before and after each training iteration’s update is roughly convex with a minimum (valley ﬂoor) in between for most of the training. Based on this and other metrics, we deduce that for most of the training update steps, {SGD} moves in valley like regions of the loss surface by jumping from one valley wall to another at a height above the valley ﬂoor. This ’bouncing between walls at a height’ mechanism helps {SGD} traverse larger distance for small batch sizes and large learning rates which we ﬁnd play qualitatively different roles in the dynamics. While a large learning rate maintains a large height from the valley ﬂoor, a small batch size injects noise facilitating exploration. We ﬁnd this mechanism is crucial for generalization because the valley ﬂoor has barriers and this exploration above the valley ﬂoor allows {SGD} to quickly travel far away from the initialization point (without being affected by barriers) and ﬁnd ﬂatter regions, corresponding to better generalization.},
	journaltitle = {{arXiv}:1802.08770 [cs, stat]},
	author = {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
	urldate = {2019-12-25},
	date = {2018-05-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1802.08770},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhou_deconstructing_2019,
	title = {Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask},
	url = {http://arxiv.org/abs/1905.01067},
	shorttitle = {Deconstructing Lottery Tickets},
	abstract = {The recent “Lottery Ticket Hypothesis” paper by Frankle \& Carbin showed that a simple approach to creating sparse networks (keeping the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket ({LT}) algorithm, showing that each may be varied signiﬁcantly without impacting the overall results. Ablating these factors leads to new insights for why {LT} networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the reinitialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86\% on {MNIST}, 41\% on {CIFAR}-10).},
	journaltitle = {{arXiv}:1905.01067 [cs, stat]},
	author = {Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
	urldate = {2019-12-25},
	date = {2019-09-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.01067},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{nguyen_understanding_2019,
	title = {Understanding Neural Networks via Feature Visualization: A survey},
	url = {http://arxiv.org/abs/1904.08939},
	shorttitle = {Understanding Neural Networks via Feature Visualization},
	abstract = {A neuroscience method to understanding the brain is to ﬁnd and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artiﬁcial or biological brain to ﬁre strongly. Those methods are known as Activation Maximization ({AM}) [ ] or Feature Visualization via Optimization. In this chapter, we ( ) review existing {AM} techniques in the literature; ( ) discuss a probabilistic interpretation for {AM}; and ( ) review the applications of {AM} in debugging and explaining networks.},
	journaltitle = {{arXiv}:1904.08939 [cs, stat]},
	author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
	urldate = {2019-12-25},
	date = {2019-04-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.08939},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_measuring_2018,
	title = {Measuring the Intrinsic Dimension of Objective Landscapes},
	url = {http://arxiv.org/abs/1804.08838},
	abstract = {Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difﬁculty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions ﬁrst appear, and deﬁne this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difﬁculty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from {MNIST}, and playing Atari Pong from pixels is about as hard as classifying {CIFAR}-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.},
	journaltitle = {{arXiv}:1804.08838 [cs, stat]},
	author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
	urldate = {2019-12-25},
	date = {2018-04-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.08838},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{perrone_notes_2019,
	title = {Notes on Category Theory with examples from basic mathematics},
	url = {http://arxiv.org/abs/1912.10642},
	abstract = {These notes were originally developed as lecture notes for a category theory course. They should be well-suited to anyone that wants to learn category theory from scratch and has a scientific mind. There is no need to know advanced mathematics, nor any of the disciplines where category theory is traditionally applied, such as algebraic geometry or theoretical computer science. The only knowledge that is assumed from the reader is linear algebra. All concepts are explained by giving concrete examples from different, non-specialized areas of mathematics (such as basic group theory, graph theory, and probability). Not every example is helpful for every reader, but hopefully every reader can find at least one helpful example per concept. The reader is encouraged to read all the examples, this way they may even learn something new about a different field. Particular emphasis is given to the Yoneda lemma and its significance, with both intuitive explanations, detailed proofs, and specific examples. Another common theme in these notes is the relationship between categories and directed multigraphs, which is treated in detail. From the applied point of view, this shows why categorical thinking can help whenever some process is taking place on a graph. Form the pure math point of view, this can be seen as the 1-dimensional first step into the theory of simplicial sets. Finally, monads and comonads are treated on an equal footing, differently to most literature in which comonads are often overlooked as "just the dual to monads". Theorems, interpretations and concrete examples are given for monads as well as for comonads.},
	journaltitle = {{arXiv}:1912.10642 [cs, math]},
	author = {Perrone, Paolo},
	urldate = {2019-12-25},
	date = {2019-12-23},
	eprinttype = {arxiv},
	eprint = {1912.10642},
	keywords = {Computer Science - Logic in Computer Science, Mathematics - Category Theory, Mathematics - History and Overview},
}

@inproceedings{k_cross-lingual_2019,
	title = {Cross-Lingual Ability of Multilingual {BERT}: An Empirical Study},
	url = {https://openreview.net/forum?id=HJeT3yrtDr},
	shorttitle = {Cross-Lingual Ability of Multilingual {BERT}},
	abstract = {Recent work has exhibited the surprising cross-lingual abilities of multilingual {BERT} (M-{BERT}) -- surprising since it is trained without any cross-lingual objective and with no aligned data. In...},
	eventtitle = {International Conference on Learning Representations},
	author = {K, Karthikeyan and Wang, Zihan and Mayhew, Stephen and Roth, Dan},
	urldate = {2019-12-20},
	date = {2019-09-25},
}

@article{rekdal_academic_2014,
	title = {Academic urban legends},
	volume = {44},
	issn = {0306-3127},
	url = {https://doi.org/10.1177/0306312714535679},
	doi = {10.1177/0306312714535679},
	abstract = {Many of the messages presented in respectable scientific publications are, in fact, based on various forms of rumors. Some of these rumors appear so frequently, and in such complex, colorful, and entertaining ways that we can think of them as academic urban legends. The explanation for this phenomenon is usually that authors have lazily, sloppily, or fraudulently employed sources, and peer reviewers and editors have not discovered these weaknesses in the manuscripts during evaluation. To illustrate this phenomenon, I draw upon a remarkable case in which a decimal point error appears to have misled millions into believing that spinach is a good nutritional source of iron. Through this example, I demonstrate how an academic urban legend can be conceived and born, and can continue to grow and reproduce within academia and beyond.},
	pages = {638--654},
	number = {4},
	journaltitle = {Social Studies of Science},
	shortjournal = {Soc Stud Sci},
	author = {Rekdal, Ole Bjørn},
	urldate = {2019-12-17},
	date = {2014-08-01},
	langid = {english},
	keywords = {academic shortcuts, academic urban legends, citation practices, iron, spinach},
}

@inproceedings{tenney_what_2018,
	title = {What do you learn from context? Probing for sentence structure in contextualized word representations},
	url = {https://openreview.net/forum?id=SJzSgnRcKX},
	shorttitle = {What do you learn from context?},
	abstract = {Contextualized representation models such as {ELMo} (Peters et al., 2018a) and {BERT} (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream {NLP} tasks....},
	eventtitle = {International Conference on Learning Representations},
	author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and {McCoy}, R. Thomas and Kim, Najoung and Durme, Benjamin Van and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
	urldate = {2019-12-16},
	date = {2018-09-27},
}

@article{hooker_selective_nodate,
	title = {{SELECTIVE} {BRAIN} {DAMAGE}: {MEASURING} {THE} {DISPARATE} {IMPACT} {OF} {MODEL} {PRUNING}},
	abstract = {Neural network pruning techniques have demonstrated it is possible to remove the majority of weights in a network with surprisingly little degradation to test set accuracy. However, this measure of performance conceals signiﬁcant differences in how different classes and images are impacted by pruning. We ﬁnd that certain examples, which we term pruning identiﬁed exemplars ({PIEs}), and classes are systematically more impacted by the introduction of sparsity. Removing {PIE} images from the test-set greatly improves top-1 accuracy for both pruned and non-pruned models. These hard-to-generalize-to images tend to be mislabelled, of lower image quality, depict multiple objects or require ﬁne-grained classiﬁcation. These ﬁndings shed light on previously unknown trade-offs, and suggest that a high degree of caution should be exercised before pruning is used in sensitive domains.},
	pages = {19},
	author = {Hooker, Sara and Courville, Aaron and Dauphin, Yann and Frome, Andrea},
	langid = {english},
}

@inproceedings{singh_bert_2019,
	location = {Hong Kong, China},
	title = {{BERT} is Not an Interlingua and the Bias of Tokenization},
	url = {https://www.aclweb.org/anthology/D19-6106},
	doi = {10.18653/v1/D19-6106},
	abstract = {Multilingual transfer learning can beneﬁt both high- and low-resource languages, but the source of these improvements is not well understood. Cananical Correlation Analysis ({CCA}) of the internal representations of a pretrained, multilingual {BERT} model reveals that the model partitions representations for each language rather than using a common, shared, interlingual space. This effect is magniﬁed at deeper layers, suggesting that the model does not progressively abstract semantic content while disregarding languages. Hierarchical clustering based on the {CCA} similarity scores between languages reveals a tree structure that mirrors the phylogenetic trees hand-designed by linguists. The subword tokenization employed by {BERT} provides a stronger bias towards such structure than character- and wordlevel tokenizations. We release a subset of the {XNLI} dataset translated into an additional 14 languages at https://www.github. com/salesforce/xnli\_extension to assist further research into multilingual representations.},
	eventtitle = {Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource {NLP} ({DeepLo} 2019)},
	pages = {47--55},
	booktitle = {Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource {NLP} ({DeepLo} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Singh, Jasdeep and {McCann}, Bryan and Socher, Richard and Xiong, Caiming},
	urldate = {2019-12-16},
	date = {2019},
	langid = {english},
}

@article{jin_towards_2019,
	title = {Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models},
	url = {http://arxiv.org/abs/1911.06194},
	shorttitle = {Towards Hierarchical Importance Attribution},
	abstract = {The impressive performance of neural networks on natural language processing tasks attributes to their ability to model complicated word and phrase interactions. Existing ﬂat, word level explanations of predictions hardly unveil how neural networks handle compositional semantics to reach predictions. To tackle the challenge, we study hierarchical explanation of neural network predictions. We identify non-additivity and independent importance attributions within hierarchies as two desirable properties for highlighting word and phrase interactions. We show prior efforts on hierarchical explanations, e.g. contextual decomposition, however, do not satisfy the desired properties mathematically. In this paper, we propose a formal way to quantify the importance of each word or phrase for hierarchical explanations. Following the formulation, we propose Sampling and Contextual Decomposition ({SCD}) algorithm and Sampling and Occlusion ({SOC}) algorithm. Human and metrics evaluation on both {LSTM} models and {BERT} Transformer models on multiple datasets show that our algorithms outperform prior hierarchical explanation algorithms. Our algorithms apply to hierarchical visualization of compositional semantics, extraction of classiﬁcation rules and improving human trust of models.},
	journaltitle = {{arXiv}:1911.06194 [cs, stat]},
	author = {Jin, Xisen and Du, Junyi and Wei, Zhongyu and Xue, Xiangyang and Ren, Xiang},
	urldate = {2019-12-16},
	date = {2019-11-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1911.06194},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rethmeier_tx-ray:_2019,
	title = {{TX}-Ray: Quantifying and Explaining Model-Knowledge Transfer in (Un-)Supervised {NLP}},
	url = {http://arxiv.org/abs/1912.00982},
	shorttitle = {{TX}-Ray},
	abstract = {While state-of-the-art {NLP} explainability ({XAI}) methods focus on supervised, per-instance end or diagnostic probing task evaluation [4, 2, 10], this is insufﬁcient to interpret and quantify model knowledge transfer during unsupervised or supervised training. By instead expressing each neuron as an interpretable token-activation distribution (§2.1) collected over many instances (Fig. 1), one can quantify and thus guide visual exploration of neuron-knowledge change between model training stages to analyze transfer beyond probing tasks and the per-instance level. This allows one to analyze: ({RQ}1) how neurons abstract knowledge during unsupervised pretraining; ({RQ}2) how pretrained neurons zero-shot transfer knowledge to new domain data; and ({RQ}3) how supervised tasks reconﬁgure pretrained neuron knowledge abstractions. Since the meaningfulness of {XAI} methods is hard to quantify [11, 4], we analyze three example learning setups ({RQ}1-3) to empirically verify that our method ({TX}-Ray): identiﬁes transfer (ir-)relevant neurons through pruning experiments in {RQ}3, and that its transfer metrics coincide with traditional measures like perplexity ({RQ}1). We also ﬁnd, that {TX}-Ray guided pruning of supervision task (ir-)relevant neuron-knowledge ({RQ}3) can identify ‘lottery ticket’-like [9, 40] model components that concentrate model performance or improve robustness. Upon visual inspection of the knowledge in four categories of pruned neurons, we ﬁnd that some ‘tickets’, i.e. task-relevant neuron-knowledge, appear (over-)ﬁt to the end-task (Tab. 2d), while others, i.e. task-irrelevant neurons, reduce overﬁtting (Tab. 2a), which helps to decide which ‘tickets’ (components) generalize and transfer model-knowledge [25] or specialize it. Finally, through {RQ}1-3, we ﬁnd that that {TX}-Ray helps to explore and quantify dynamics of (continual) knowledge transfer and that it can shed light on neuron-knowledge specialization and generalization, to complement (costly) supervised probing task procurement and established ‘summary’ statistics like perplexity, {ROC} or F scores. As a basis for future extensions, we will release {TX}-Ray on https://github.com/copenlu/tx-ray.},
	journaltitle = {{arXiv}:1912.00982 [cs, stat]},
	author = {Rethmeier, Nils and Saxena, Vageesh Kumar and Augenstein, Isabelle},
	urldate = {2019-12-15},
	date = {2019-12-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1912.00982},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{neyshabur_implicit_2017,
	title = {Implicit Regularization in Deep Learning},
	url = {http://arxiv.org/abs/1709.01953},
	abstract = {In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.},
	journaltitle = {{arXiv}:1709.01953 [cs]},
	author = {Neyshabur, Behnam},
	urldate = {2019-12-15},
	date = {2017-09-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.01953},
	keywords = {Computer Science - Machine Learning},
}

@article{kaur_are_2019,
	title = {Are Perceptually-Aligned Gradients a General Property of Robust Classifiers?},
	url = {http://arxiv.org/abs/1910.08640},
	abstract = {For a standard convolutional neural network, optimizing over the input pixels to maximize the score of some target class will generally produce a grainy-looking version of the original image. However, Santurkar et al. (2019) demonstrated that for adversarially-trained neural networks, this optimization produces images that uncannily resemble the target class. In this paper, we show that these perceptuallyaligned gradients also occur under randomized smoothing, an alternative means of constructing adversarially-robust classiﬁers. Our ﬁnding supports the hypothesis that perceptually-aligned gradients may be a general property of robust classiﬁers. We hope that our results will inspire research aimed at explaining this link between perceptually-aligned gradients and adversarial robustness.},
	journaltitle = {{arXiv}:1910.08640 [cs, stat]},
	author = {Kaur, Simran and Cohen, Jeremy and Lipton, Zachary C.},
	urldate = {2019-12-15},
	date = {2019-10-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.08640},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{petzka_reparameterization-invariant_2019,
	title = {A Reparameterization-Invariant Flatness Measure for Deep Neural Networks},
	url = {http://arxiv.org/abs/1912.00058},
	abstract = {The performance of deep neural networks is often attributed to their automated, task-related feature construction. It remains an open question, though, why this leads to solutions with good generalization, even in cases where the number of parameters is larger than the number of samples. Back in the 90s, Hochreiter and Schmidhuber observed that ﬂatness of the loss surface around a local minimum correlates with low generalization error. For several ﬂatness measures, this correlation has been empirically validated. However, it has recently been shown that existing measures of ﬂatness cannot theoretically be related to generalization due to a lack of invariance with respect to reparameterizations. We propose a natural modiﬁcation of existing ﬂatness measures that results in invariance to reparameterization.},
	journaltitle = {{arXiv}:1912.00058 [cs, stat]},
	author = {Petzka, Henning and Adilova, Linara and Kamp, Michael and Sminchisescu, Cristian},
	urldate = {2019-12-15},
	date = {2019-11-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1912.00058},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kim_compositionality_nodate,
	title = {Compositionality as Directional Consistency in Sequential Neural Networks},
	abstract = {Sequential neural networks have shown success on a variety of natural language tasks, but through what internal mechanisms they achieve systematic compositionality crucial to language understanding is still an open question. In particular, gated networks such as Gated Recurrent Units ({GRUs}) are known to signiﬁcantly outperform Simple Recurrent Neural Networks ({SRNs}). We conduct an exploratory study comparing the abilities of {SRNs} and {GRUs} to make compositional generalizations, using adjective semantics as testing ground. Our results demonstrate that {GRUs} generalize more systematically than {SRNs}. On analyzing the learned representations, we ﬁnd that {GRUs} encode the compositional contribution of adjectives as directionally consistent linear displacements. This consistency correlates with generalization accuracy within {GRUs}, suggesting that it is an effective strategy for deriving more compositionally generalizable representations.},
	pages = {10},
	author = {Kim, Najoung and Linzen, Tal},
	langid = {english},
}

@article{pang_crossover_2019,
	title = {A crossover code for high-dimensional composition},
	url = {http://arxiv.org/abs/1911.06775},
	abstract = {We present a novel way to encode compositional information in high-dimensional ({HD}) vectors. Inspired by chromosomal crossover, random {HD} vectors are recursively interwoven, with a fraction of one vector's components masked out and replaced by those from another using a context-dependent mask. Unlike many {HD} computing schemes, "crossover" codes highly overlap with their base elements' and sub-structures' codes without sacrificing relational information, allowing fast element readout and decoding by greedy reconstruction. Crossover is mathematically tractable and has several properties desirable for robust, flexible representation.},
	journaltitle = {{arXiv}:1911.06775 [q-bio]},
	author = {Pang, Rich},
	urldate = {2019-12-15},
	date = {2019-11-15},
	eprinttype = {arxiv},
	eprint = {1911.06775},
	keywords = {Quantitative Biology - Neurons and Cognition},
}

@article{lampinen_analytic_2019,
	title = {An analytic theory of generalization dynamics and transfer learning in deep linear networks},
	url = {http://arxiv.org/abs/1809.10374},
	abstract = {Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and {SNR}. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the {SNRs} and input feature alignments of pairs of tasks.},
	journaltitle = {{arXiv}:1809.10374 [cs, stat]},
	author = {Lampinen, Andrew K. and Ganguli, Surya},
	urldate = {2019-12-14},
	date = {2019-01-04},
	eprinttype = {arxiv},
	eprint = {1809.10374},
	keywords = {Computer Science - Machine Learning, F.m, I.2.6, Statistics - Machine Learning},
}

@article{raunak_compositionality_2019,
	title = {On Compositionality in Neural Machine Translation},
	url = {http://arxiv.org/abs/1911.01497},
	abstract = {We investigate two specific manifestations of compositionality in Neural Machine Translation ({NMT}) : (1) Productivity - the ability of the model to extend its predictions beyond the observed length in training data and (2) Systematicity - the ability of the model to systematically recombine known parts and rules. We evaluate a standard Sequence to Sequence model on tests designed to assess these two properties in {NMT}. We quantitatively demonstrate that inadequate temporal processing, in the form of poor encoder representations is a bottleneck for both Productivity and Systematicity. We propose a simple pre-training mechanism which alleviates model performance on the two properties and leads to a significant improvement in {BLEU} scores.},
	journaltitle = {{arXiv}:1911.01497 [cs]},
	author = {Raunak, Vikas and Kumar, Vaibhav and Metze, Florian and Callan, Jaimie},
	urldate = {2019-12-14},
	date = {2019-11-14},
	eprinttype = {arxiv},
	eprint = {1911.01497},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{bietti_inductive_2019,
	title = {On the Inductive Bias of Neural Tangent Kernels},
	url = {http://arxiv.org/abs/1905.12173},
	abstract = {State-of-the-art neural networks are heavily over-parameterized, making the optimization algorithm a crucial ingredient for learning predictive models with good generalization properties. A recent line of work has shown that in a certain over-parameterized regime, the learning dynamics of gradient descent are governed by a certain kernel obtained at initialization, called the neural tangent kernel. We study the inductive bias of learning in such a regime by analyzing this kernel and the corresponding function space ({RKHS}). In particular, we study smoothness, approximation, and stability properties of functions with finite norm, including stability to image deformations in the case of convolutional networks, and compare to other known kernels for similar architectures.},
	journaltitle = {{arXiv}:1905.12173 [cs, stat]},
	author = {Bietti, Alberto and Mairal, Julien},
	urldate = {2019-12-14},
	date = {2019-10-31},
	eprinttype = {arxiv},
	eprint = {1905.12173},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{blumenfeld_is_2019,
	title = {Is Feature Diversity Necessary in Neural Network Initialization?},
	url = {http://arxiv.org/abs/1912.05137},
	abstract = {Standard practice in training neural networks involves initializing the weights in an independent fashion. The results of recent work suggest that feature "diversity" at initialization plays an important role in training the network. However, other initialization schemes with reduced feature diversity have also been shown to be viable. In this work, we conduct a series of experiments aimed at elucidating the importance of feature diversity at initialization. We show that a complete lack of diversity is harmful to training, but its effects can be counteracted by a relatively small addition of noise - even the noise in standard non-deterministic {GPU} computations is sufficient. Furthermore, we construct a deep convolutional network with identical features at initialization and almost all of the weights initialized at 0 that can be trained to reach accuracy matching its standard-initialized counterpart.},
	journaltitle = {{arXiv}:1912.05137 [cs, stat]},
	author = {Blumenfeld, Yaniv and Gilboa, Dar and Soudry, Daniel},
	urldate = {2019-12-14},
	date = {2019-12-11},
	eprinttype = {arxiv},
	eprint = {1912.05137},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{suzgun_memory-augmented_nodate,
	title = {Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages},
	abstract = {We introduce three memory-augmented Recurrent Neural Networks ({MARNNs}) and explore their capabilities on a series of simple language modeling tasks whose solutions require stack-based mechanisms. We provide the ﬁrst demonstration of neural networks recognizing the generalized Dyck languages, which express the core of what it means to be a language with hierarchical structure. Our memory-augmented architectures are easy to train in an end-to-end fashion and can learn the Dyck languages over as many as six parenthesis-pairs, in addition to two deterministic palindrome languages and the string-reversal transduction task, by emulating pushdown automata. Our experiments highlight the increased modeling capacity of memory-augmented models over simple {RNNs}, while inﬂecting our understanding of the limitations of these models.},
	pages = {16},
	author = {Suzgun, Mirac and Gehrmann, Sebastian and Belinkov, Yonatan and Shieber, Stuart M},
	langid = {english},
}

@article{htut_attention_nodate,
	title = {Do Attention Heads in {BERT} Track Syntactic Dependencies?},
	abstract = {We investigate the extent to which individual attention heads in pretrained transformer language models, such as {BERT} and {RoBERTa}, implicitly capture syntactic dependency relations. We employ two methods—taking the maximum attention weight and computing the maximum spanning tree—to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency ({UD}) trees. We show that, for some {UD} relation types, there exist heads that can recover the dependency type signiﬁcantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze {BERT} ﬁne-tuned on two datasets—the syntaxoriented {CoLA} and the semantics-oriented {MNLI}—to investigate whether ﬁne-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing signiﬁcantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that {BERT}-style models are known to learn.},
	pages = {7},
	author = {Htut, Phu Mon and Phang, Jason and Bordia, Shikha and Bowman, Samuel R},
	langid = {english},
}

@inproceedings{kavumba_when_2019,
	location = {Hong Kong, China},
	title = {When Choosing Plausible Alternatives, Clever Hans can be Clever},
	url = {https://www.aclweb.org/anthology/D19-6004},
	doi = {10.18653/v1/D19-6004},
	abstract = {Pretrained language models, such as {BERT} and {RoBERTa}, have shown large improvements in the commonsense reasoning benchmark {COPA}. However, recent work found that many improvements in benchmarks of natural language understanding are not due to models learning the task, but due to their increasing ability to exploit superﬁcial cues, such as tokens that occur more often in the correct answer than the wrong one. Are {BERT}’s and {RoBERTa}’s good performance on {COPA} also caused by this? We ﬁnd superﬁcial cues in {COPA}, as well as evidence that {BERT} exploits these cues. To remedy this problem, we introduce Balanced {COPA}, an extension of {COPA} that does not suffer from easy-toexploit single token cues. We analyze {BERT}’s and {RoBERTa}’s performance on original and Balanced {COPA}, ﬁnding that {BERT} relies on superﬁcial cues when they are present, but still achieves comparable performance once they are made ineffective, suggesting that {BERT} learns the task to a certain degree when forced to. In contrast, {RoBERTa} does not appear to rely on superﬁcial cues.},
	eventtitle = {Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing},
	pages = {33--42},
	booktitle = {Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Kavumba, Pride and Inoue, Naoya and Heinzerling, Benjamin and Singh, Keshav and Reisert, Paul and Inui, Kentaro},
	urldate = {2019-12-14},
	date = {2019},
	langid = {english},
}

@article{hooker_benchmark_nodate,
	title = {A Benchmark for Interpretability Methods in Deep Neural Networks},
	abstract = {We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classiﬁcation datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches—{VarGrad} and {SmoothGrad}-Squared—outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.},
	pages = {17},
	author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
	langid = {english},
}

@article{jiang_how_nodate,
	title = {How Can We Know What Language Models Know?},
	abstract = {Recent work has presented intriguing results examining the knowledge contained in language models ({LM}) by having the {LM} ﬁll in the blanks of prompts such as “Obama is a by profession”. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as “Obama worked as a ” may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the {LM} does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an {LM}. In this paper, we attempt to more accurately estimate the knowledge contained in {LMs} by automatically discovering better prompts to use in this querying process. Speciﬁcally, we propose miningbased and paraphrasing-based methods to automatically generate high-quality and diverse prompts and ensemble methods to combine answers from different prompts. Extensive experiments on the {LAMA} benchmark for extracting relational knowledge from {LMs} demonstrate that our methods can improve accuracy from 31.1\% to 38.1\%, providing a tighter lower bound on what {LMs} know. We have released the code and the resulting {LM} Prompt And Query Archive ({LPAQA}) at https:// github.com/jzbjyb/{LPAQA}.},
	pages = {12},
	author = {Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
	langid = {english},
}

@article{press_improving_nodate,
	title = {Improving Transformer Models by Reordering their Sublayers},
	abstract = {Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern achieve better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more selfattention at the bottom and more feedforward sublayers at the top. We propose a new transformer design pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on the {WikiText}-103 language modeling benchmark, at no cost in parameters, memory, or training time.},
	pages = {8},
	author = {Press, Oﬁr and Smith, Noah A and Levy, Omer},
	langid = {english},
}

@article{evci_rigging_nodate,
	title = {Rigging the Lottery: Making All Tickets Winners},
	abstract = {Sparse neural networks have been shown to be more parameter and compute efﬁcient compared to dense networks and in some cases are used to decrease wall clock inference times. There is a large body of work on training dense networks to yield sparse networks for inference (Molchanov et al., 2017; Zhu \& Gupta, 2018; Narang et al., 2017; Li et al., 2016; Guo et al., 2016). This limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a ﬁxed parameter count and a ﬁxed computational cost throughout training, without sacriﬁcing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer ﬂoating-point operations ({FLOPs}) to achieve a given level of accuracy compared to prior techniques. Importantly,by adjusting the topology it can start from any initialization – not just “lucky” ones. We demonstrate state-of-the-art sparse training results with {ResNet}-50, {MobileNet} v1 and {MobileNet} v2 on the {ImageNet}-2012 dataset, {WideResNets} on the {CIFAR}-10 dataset and {RNNs} on the {WikiText}-103 dataset. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static.},
	pages = {21},
	author = {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
	langid = {english},
}

@article{neyshabur_exploring_nodate,
	title = {Exploring Generalization in Deep Learning},
	abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and {PAC}-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
	pages = {19},
	author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and {McAllester}, David and Srebro, Nathan},
	langid = {english},
}

@article{cao_towards_nodate,
	title = {Towards Understanding the Spectral Bias of Deep Learning},
	abstract = {An intriguing phenomenon observed during training neural networks is the spectral bias, where neural networks are biased towards learning less complex functions. The priority of learning functions with low complexity might be at the core of explaining generalization ability of neural network, and certain eﬀorts have been made to provide theoretical explanation for spectral bias. However, there is still no satisfying theoretical result justifying the underlying mechanism of spectral bias. In this paper, we give a comprehensive and rigorous explanation for spectral bias and relate it with the neural tangent kernel function proposed in recent work. We prove that the training process of neural networks can be decomposed along diﬀerent directions deﬁned by the eigenfunctions of the neural tangent kernel, where each direction has its own convergence rate and the rate is determined by the corresponding eigenvalue. We then provide a case study when the input data is uniformly distributed over the unit sphere, and show that lower degree spherical harmonics are easier to be learned by over-parameterized neural networks.},
	pages = {26},
	author = {Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
	langid = {english},
}

@article{toneva_empirical_2019,
	title = {{AN} {EMPIRICAL} {STUDY} {OF} {EXAMPLE} {FORGETTING} {DURING} {DEEP} {NEURAL} {NETWORK} {LEARNING}},
	abstract = {Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classiﬁcation tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We deﬁne a “forgetting event” to have occurred when an individual training example transitions from being classiﬁed correctly to incorrectly over the course of learning. Across several benchmark data sets, we ﬁnd that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set’s (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a signiﬁcant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.},
	pages = {19},
	author = {Toneva, Mariya and Sordoni, Alessandro},
	date = {2019},
	langid = {english},
}

@article{adlam_random_nodate,
	title = {A {RANDOM} {MATRIX} {PERSPECTIVE} {ON} {MIXTURES} {OF} {NONLINEARITIES} {IN} {HIGH} {DIMENSIONS}},
	abstract = {One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired signiﬁcant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to inﬁnity. We analyze the performance of a simple regression model trained on the random features F = f (W X + B) for a random weight matrix W and random bias vector B, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis directly generalizes to such distributions, even those not expressible with a traditional additive bias. Intriguingly, we ﬁnd that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.},
	pages = {15},
	author = {Adlam, Ben and Levinson, Jake and Pennington, Jeffrey},
	langid = {english},
}

@article{nakkiran_deep_2019,
	title = {Deep Double Descent: Where Bigger Models and More Data Hurt},
	url = {http://arxiv.org/abs/1912.02292},
	shorttitle = {Deep Double Descent},
	abstract = {We show that a variety of modern deep learning tasks exhibit a “double-descent” phenomenon where, as we increase model size, performance ﬁrst gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by deﬁning a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	journaltitle = {{arXiv}:1912.02292 [cs, stat]},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	urldate = {2019-12-14},
	date = {2019-12-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1912.02292},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{toneva_interpreting_2019,
	title = {Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)},
	url = {http://arxiv.org/abs/1905.11833},
	abstract = {Neural networks models for {NLP} are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. This has generated a lot of research interest in interpreting the representations learned by these networks. We propose here a novel interpretation approach that relies on the only processing system we have that does understand language: the human brain. We use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent {NLP} models - {ELMo}, {USE}, {BERT} and Transformer-{XL}. We study how their representations differ across layer depth, context length, and attention type. Our results reveal differences in the context-related representations across these models. Further, in the transformer models, we ﬁnd an interaction between layer depth and context length, and between layer depth and attention type. We ﬁnally hypothesize that altering {BERT} to better align with brain recordings would enable it to also better understand language. Probing the altered {BERT} using syntactic {NLP} tasks reveals that the model with increased brain-alignment outperforms the original model. Cognitive neuroscientists have already begun using {NLP} networks to study the brain, and this work closes the loop to allow the interaction between {NLP} and cognitive neuroscience to be a true cross-pollination.},
	journaltitle = {{arXiv}:1905.11833 [cs, q-bio]},
	author = {Toneva, Mariya and Wehbe, Leila},
	urldate = {2019-12-13},
	date = {2019-11-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.11833},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}

@article{moradshahi_hubert_2019,
	title = {{HUBERT} Untangles {BERT} to Improve Transfer across {NLP} Tasks},
	url = {http://arxiv.org/abs/1910.12647},
	abstract = {We introduce {HUBERT} which combines the structured-representational power of Tensor-Product Representations ({TPRs}) and {BERT}, a pre-trained bidirectional Transformer language model. We show that there is shared structure between different {NLP} datasets that {HUBERT}, but not {BERT}, is able to learn and leverage. We validate the effectiveness of our model on the {GLUE} benchmark and {HANS} dataset. Our experiment results show that untangling data-specific semantics from general language structure is key for better transfer among {NLP} tasks.},
	journaltitle = {{arXiv}:1910.12647 [cs, stat]},
	author = {Moradshahi, Mehrad and Palangi, Hamid and Lam, Monica S. and Smolensky, Paul and Gao, Jianfeng},
	urldate = {2019-12-13},
	date = {2019-10-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.12647},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{arora_convergence_2019,
	title = {A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
	url = {http://arxiv.org/abs/1810.02281},
	abstract = {We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as x → {WN} {WN}−1 · · · W1x) by minimizing the 2 loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deﬁcient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results signiﬁcantly extend previous analyses, e.g., of deep linear residual networks (Bartlett et al., 2018).},
	journaltitle = {{arXiv}:1810.02281 [cs, stat]},
	author = {Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
	urldate = {2019-12-13},
	date = {2019-10-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.02281},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{morcos_one_2019,
	title = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
	url = {http://arxiv.org/abs/1906.02773},
	shorttitle = {One ticket to win them all},
	abstract = {The success of lottery ticket initializations [7] suggests that small, sparsiﬁed networks can be trained so long as the network is initialized appropriately. Unfortunately, ﬁnding these “winning ticket” initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training conﬁguration (optimizer and dataset) and evaluating their performance on another conﬁguration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion {MNIST}, {SVHN}, {CIFAR}-10/100, {ImageNet}, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufﬁciently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.},
	journaltitle = {{arXiv}:1906.02773 [cs, stat]},
	author = {Morcos, Ari S. and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
	urldate = {2019-12-13},
	date = {2019-10-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.02773},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{abebe_roles_2019,
	title = {Roles for Computing in Social Change},
	url = {http://arxiv.org/abs/1912.04883},
	doi = {10.1145/3351095.3372871},
	abstract = {A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the ﬁeld. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as ﬁxed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems — roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the signiﬁcant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly deﬁned — changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We oﬀer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing’s capacity to solve social problems on its own.},
	journaltitle = {{arXiv}:1912.04883 [cs]},
	author = {Abebe, Rediet and Barocas, Solon and Kleinberg, Jon and Levy, Karen and Raghavan, Manish and Robinson, David G.},
	urldate = {2019-12-13},
	date = {2019-12-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1912.04883},
	keywords = {Computer Science - Computers and Society},
}

@inproceedings{jawahar_what_2019,
	location = {Florence, Italy},
	title = {What Does {BERT} Learn about the Structure of Language?},
	url = {https://www.aclweb.org/anthology/P19-1356},
	doi = {10.18653/v1/P19-1356},
	abstract = {{BERT} is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that {BERT} networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by {BERT}. Our findings are fourfold. {BERT}'s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of {BERT} compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. {BERT} requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying {BERT} mimics classical, tree-like structures.},
	eventtitle = {{ACL} 2019},
	pages = {3651--3657},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Jawahar, Ganesh and Sagot, Benoît and Seddah, Djamé},
	urldate = {2019-12-12},
	date = {2019-07},
}

@online{noauthor_language_nodate,
	title = {Language, trees, and geometry in neural networks},
	url = {https://pair-code.github.io/interpretability/bert-tree/},
	urldate = {2019-12-12},
}

@article{casper_removable_2019,
	title = {Removable and/or Repeated Units Emerge in Overparametrized Deep Neural Networks},
	url = {http://arxiv.org/abs/1912.04783},
	abstract = {Deep neural networks ({DNNs}) perform well on a variety of tasks despite the fact that most networks used in practice are vastly overparametrized and even capable of perfectly ﬁtting randomly labeled data. Recent evidence suggests that developing compressible representations is key for adjusting the complexity of overparametrized networks to the task at hand [4, 42]. In this paper, we provide new empirical evidence that supports this hypothesis by identifying two types of units that emerge when the network’s width is increased: removable units which can be dropped out of the network without signiﬁcant change to the output and repeated units whose activities are highly correlated with other units. The emergence of these units implies capacity constraints as the function the network represents could be expressed by a smaller network without these units. In a series of experiments with {AlexNet}, {ResNet} and Inception networks in the {CIFAR}-10 and {ImageNet} datasets, and also using shallow networks with synthetic data, we show that {DNNs} consistently increase either the number of removable units, repeated units, or both at greater widths for a comprehensive set of hyperparameters. These results suggest that the mechanisms by which networks in the deep learning regime adjust their complexity operate at the unit level and highlight the need for additional research into what drives the emergence of such units.},
	journaltitle = {{arXiv}:1912.04783 [cs, stat]},
	author = {Casper, Stephen and Boix, Xavier and D'Amario, Vanessa and Guo, Ling and Vinken, Kasper and Kreiman, Gabriel},
	urldate = {2019-12-12},
	date = {2019-12-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1912.04783},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mccoy_berts_2019,
	title = {{BERTs} of a feather do not generalize together: Large variability in generalization across models with similar test set performance},
	url = {http://arxiv.org/abs/1911.02969},
	shorttitle = {{BERTs} of a feather do not generalize together},
	abstract = {If the same neural architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of {BERT} on the Multi-genre Natural Language Inference ({MNLI}) dataset and evaluated them on the {HANS} dataset, which measures syntactic generalization in natural language inference. On the {MNLI} development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6\% and 84.8\%. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap (e.g., knowing that "the doctor visited the lawyer" does not entail "the lawyer visited the doctor"), accuracy ranged from 0.00\% to 66.2\%. Such variation likely arises from the presence of many local minima that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases.},
	journaltitle = {{arXiv}:1911.02969 [cs]},
	author = {{McCoy}, R. Thomas and Min, Junghyun and Linzen, Tal},
	urldate = {2019-12-12},
	date = {2019-11-07},
	eprinttype = {arxiv},
	eprint = {1911.02969},
	keywords = {Computer Science - Computation and Language},
}
@article{voita_analyzing_2019,
	title = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
	shorttitle = {Analyzing Multi-Head Self-Attention},
	journaltitle = {{arXiv} preprint {arXiv}:1905.09418},
	author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
	date = {2019},
}

@incollection{kindermans__2019,
	title = {The (un) reliability of saliency methods},
	pages = {267--280},
	booktitle = {Explainable {AI}: Interpreting, Explaining and Visualizing Deep Learning},
	publisher = {Springer},
	author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Schütt, Kristof T. and Dähne, Sven and Erhan, Dumitru and Kim, Been},
	date = {2019},
}

@article{guidotti_survey_2019,
	title = {A survey of methods for explaining black box models},
	volume = {51},
	pages = {93},
	number = {5},
	journaltitle = {{ACM} computing surveys ({CSUR})},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	date = {2019},
}

@online{jacovi_problem_2019,
	title = {The Problem of Faithfulness in (Neural Network) {NLP} Interpretations},
	url = {https://medium.com/@alonjacovi/the-problem-of-faithfulness-in-neural-network-nlp-interpretations-ee98d7027cbd},
	abstract = {The movement towards understanding and interpreting {NLP} models needs no introduction — it’s growing very fast, and there will even be a…},
	titleaddon = {Medium},
	author = {Jacovi, Alon},
	urldate = {2019-12-10},
	date = {2019-11-01},
	langid = {english},
}

@inproceedings{chakraborty_interpretability_2017,
	title = {Interpretability of deep learning models: a survey of results},
	shorttitle = {Interpretability of deep learning models},
	pages = {1--6},
	booktitle = {2017 {IEEE} {SmartWorld}, Ubiquitous Intelligence \& Computing, Advanced \& Trusted Computed, Scalable Computing \& Communications, Cloud \& Big Data Computing, Internet of People and Smart City Innovation ({SmartWorld}/{SCALCOM}/{UIC}/{ATC}/{CBDCom}/{IOP}/{SCI})},
	publisher = {{IEEE}},
	author = {Chakraborty, Supriyo and Tomsett, Richard and Raghavendra, Ramya and Harborne, Daniel and Alzantot, Moustafa and Cerutti, Federico and Srivastava, Mani and Preece, Alun and Julier, Simon and Rao, Raghuveer M.},
	date = {2017},
}

@article{alvarez-melis_robustness_2018,
	title = {On the Robustness of Interpretability Methods},
	url = {http://arxiv.org/abs/1806.08049},
	abstract = {We argue that robustness of explanations---i.e., that similar inputs should give rise to similar explanations---is a key desideratum for interpretability. We introduce metrics to quantify robustness and demonstrate that current methods do not perform well according to these metrics. Finally, we propose ways that robustness can be enforced on existing interpretability approaches.},
	journaltitle = {{arXiv}:1806.08049 [cs, stat]},
	author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
	urldate = {2019-12-10},
	date = {2018-06-20},
	eprinttype = {arxiv},
	eprint = {1806.08049},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{lowe_putting_2019,
	title = {Putting An End to End-to-End: Gradient-Isolated Learning of Representations},
	url = {http://papers.nips.cc/paper/8568-putting-an-end-to-end-to-end-gradient-isolated-learning-of-representations.pdf},
	shorttitle = {Putting An End to End-to-End},
	pages = {3033--3045},
	booktitle = {Advances in Neural Information Processing Systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Löwe, Sindy and O\{{\textbackslash}textbackslash\}textquotesingle Connor, Peter and Veeling, Bastiaan},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d\{{\textbackslash}textbackslash\}textquotesingle and Fox, E. and Garnett, R.},
	urldate = {2019-12-08},
	date = {2019},
}

@article{scholkopf_causality_2019,
	title = {Causality for Machine Learning},
	url = {http://arxiv.org/abs/1911.10500},
	abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence ({AI}), and for a long time had little connection to the field of machine learning. This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and {AI} are intrinsically related to causality, and explains how the field is beginning to understand them.},
	journaltitle = {{arXiv}:1911.10500 [cs, stat]},
	author = {Schölkopf, Bernhard},
	urldate = {2019-12-06},
	date = {2019-11-24},
	eprinttype = {arxiv},
	eprint = {1911.10500},
	note = {version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2, I.5, K.4, Statistics - Machine Learning},
}

@article{shallue_measuring_2019,
	title = {Measuring the Effects of Data Parallelism on Neural Network Training},
	volume = {20},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v20/18-789.html},
	pages = {1--49},
	number = {112},
	journaltitle = {Journal of Machine Learning Research},
	author = {Shallue, Christopher J. and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E.},
	urldate = {2019-12-03},
	date = {2019},
}

@article{mroueh_sobolev_2019,
	title = {Sobolev Independence Criterion},
	url = {http://arxiv.org/abs/1910.14212},
	abstract = {We propose the Sobolev Independence Criterion ({SIC}), an interpretable dependency measure between a high dimensional random variable X and a response variable Y . {SIC} decomposes to the sum of feature importance scores and hence can be used for nonlinear feature selection. {SIC} can be seen as a gradient regularized Integral Probability Metric ({IPM}) between the joint distribution of the two random variables and the product of their marginals. We use sparsity inducing gradient penalties to promote input sparsity of the critic of the {IPM}. In the kernel version we show that {SIC} can be cast as a convex optimization problem by introducing auxiliary variables that play an important role in feature selection as they are normalized feature importance scores. We then present a neural version of {SIC} where the critic is parameterized as a homogeneous neural network, improving its representation power as well as its interpretability. We conduct experiments validating {SIC} for feature selection in synthetic and real-world experiments. We show that {SIC} enables reliable and interpretable discoveries, when used in conjunction with the holdout randomization test and knockoffs to control the False Discovery Rate. Code is available at http://github.com/ibm/sic.},
	journaltitle = {{arXiv}:1910.14212 [cs, stat]},
	author = {Mroueh, Youssef and Sercu, Tom and Rigotti, Mattia and Padhi, Inkit and Santos, Cicero Dos},
	urldate = {2019-12-03},
	date = {2019-10-30},
	eprinttype = {arxiv},
	eprint = {1910.14212},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{bommasani_long-distance_2019,
	location = {Florence, Italy},
	title = {Long-Distance Dependencies Don't Have to Be Long: Simplifying through Provably (Approximately) Optimal Permutations},
	url = {https://www.aclweb.org/anthology/P19-2012},
	doi = {10.18653/v1/P19-2012},
	shorttitle = {Long-Distance Dependencies Don't Have to Be Long},
	abstract = {Neural models at the sentence level often operate on the constituent words/tokens in a way that encodes the inductive bias of processing the input in a similar fashion to how humans do. However, there is no guarantee that the standard ordering of words is computationally efficient or optimal. To help mitigate this, we consider a dependency parse as a proxy for the inter-word dependencies in a sentence and simplify the sentence with respect to combinatorial objectives imposed on the sentence-parse pair. The associated optimization results in permuted sentences that are provably (approximately) optimal with respect to minimizing dependency parse lengths and that are demonstrably simpler. We evaluate our general-purpose permutations within a fine-tuning schema for the downstream task of subjectivity analysis. Our fine-tuned baselines reflect a new state of the art for the {SUBJ} dataset and the permutations we introduce lead to further improvements with a 2.0\% increase in classification accuracy (absolute) and a 45\% reduction in classification error (relative) over the previous state of the art.},
	pages = {89--99},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
	publisher = {Association for Computational Linguistics},
	author = {Bommasani, Rishi},
	urldate = {2019-12-03},
	date = {2019-07},
}

@inproceedings{anonymous_compositional_2019,
	title = {Compositional languages emerge in a neural iterated learning model},
	url = {https://openreview.net/forum?id=HkePNpVKPB},
	abstract = {The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_measuring_2019,
	title = {Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},
	url = {https://openreview.net/forum?id=SygcCnNKwr},
	shorttitle = {Measuring Compositional Generalization},
	abstract = {State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_permutation_2019,
	title = {Permutation Equivariant Models for Compositional Generalization in Language},
	url = {https://openreview.net/forum?id=SylVNerFvr},
	abstract = {Humans understand novel sentences by composing meanings and roles of core language components. In contrast, neural network models for natural language modeling fail when such compositional...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_locality_2019,
	title = {Locality and Compositionality in Zero-Shot Learning},
	url = {https://openreview.net/forum?id=Hye_V0NKwr},
	abstract = {In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning ({ZSL}). 
  In order to well-isolate the importance of these properties in learned...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_discovering_2019,
	title = {Discovering the compositional structure of vector representations with Role Learning Networks},
	url = {https://openreview.net/forum?id=BklMDCVtvr},
	abstract = {Neural networks ({NNs}) are able to perform tasks that rely on compositional structure even though they lack obvious mechanisms for representing this structure. To analyze the internal...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_towards_2019,
	title = {Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models},
	url = {https://openreview.net/forum?id=BkxRRkSKwr},
	shorttitle = {Towards Hierarchical Importance Attribution},
	abstract = {Deep neural networks have achieved impressive performance in handling complicated semantics in natural language, while mostly treated as black boxes. To explain how the model handles compositional...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_asymptotic_2019,
	title = {The asymptotic spectrum of the Hessian of {DNN} throughout training},
	url = {https://openreview.net/forum?id=SkgscaNYPS},
	abstract = {The dynamics of {DNNs} during gradient descent is described by the so-called Neural Tangent Kernel ({NTK}). In this article, we show that the {NTK} allows one to gain precise insight into the Hessian of...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_finding_2019,
	title = {Finding Winning Tickets with Limited (or No) Supervision},
	url = {https://openreview.net/forum?id=SJx_QJHYDB},
	abstract = {The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_understanding_2019,
	title = {Understanding Why Neural Networks Generalize Well Through {GSNR} of Parameters},
	url = {https://openreview.net/forum?id=HyevIJStwH},
	abstract = {As deep neural networks ({DNNs}) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_self-induced_2019,
	title = {Self-Induced Curriculum Learning in Neural Machine Translation},
	url = {https://openreview.net/forum?id=rJxRmlStDB},
	abstract = {Self-supervised neural machine translation ({SS}-{NMT}) learns how to extract/select suitable training data from comparable (rather than parallel) corpora and how to translate, in a way that the two...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_mode_2019,
	title = {Mode Connectivity and Sparse Neural Networks},
	url = {https://openreview.net/forum?id=rkeO-lrYwr},
	abstract = {We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, {SGD}...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_iterative_2019,
	title = {On Iterative Neural Network Pruning, Reinitialization, and the Similarity of Masks},
	url = {https://openreview.net/forum?id=B1xgQkrYwS},
	abstract = {We examine how recently documented, fundamental phenomena in deep learn-ing models subject to pruning are affected by changes in the pruning procedure. Specifically, we analyze differences in the...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@inproceedings{anonymous_exact_2019,
	title = {{EXACT} {ANALYSIS} {OF} {CURVATURE} {CORRECTED} {LEARNING} {DYNAMICS} {IN} {DEEP} {LINEAR} {NETWORKS}},
	url = {https://openreview.net/forum?id=ryx4TlHKDS},
	abstract = {Deep neural networks exhibit complex learning dynamics due to the highly non-convex loss landscape, which causes slow convergence and vanishing gradient problems. Second order approaches, such as...},
	eventtitle = {Submitted to International Conference on Learning Representations},
	author = {Anonymous},
	urldate = {2019-12-03},
	date = {2019-09-25},
}

@article{santoro_relational_2018,
	title = {Relational recurrent neural networks},
	url = {http://arxiv.org/abs/1806.01822},
	abstract = {Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a {\textbackslash}textit\{Relational Memory Core\} ({RMC}) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the {RMC} on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in {RL} domains (e.g. Mini {PacMan}), program evaluation, and language modeling, achieving state-of-the-art results on the {WikiText}-103, Project Gutenberg, and {GigaWord} datasets.},
	journaltitle = {{arXiv}:1806.01822 [cs, stat]},
	author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
	urldate = {2019-12-02},
	date = {2018-06-28},
	eprinttype = {arxiv},
	eprint = {1806.01822},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{belinkov_linguistic_2019,
	title = {On the Linguistic Representational Power of Neural Machine Translation Models},
	url = {http://arxiv.org/abs/1911.00317},
	abstract = {Despite the recent success of deep neural networks in natural language processing ({NLP}), their interpretability remains a challenge. We analyze the representations learned by neural machine translation models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word-structure captured within the learned representations, an important aspect in translating morphologically-rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual {NMT} models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in {NMT} models and their ability to capture various linguistic phenomena. We show that deep {NMT} models learn a non-trivial amount of linguistic information. Notable findings include: i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers; (iii) Representations learned using characters are more informed about wordmorphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.},
	journaltitle = {{arXiv}:1911.00317 [cs]},
	author = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
	urldate = {2019-11-29},
	date = {2019-11-01},
	eprinttype = {arxiv},
	eprint = {1911.00317},
	keywords = {Computer Science - Computation and Language},
}

@article{prasad_using_2019,
	title = {Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models},
	journaltitle = {{arXiv} preprint {arXiv}:1909.10579},
	author = {Prasad, Grusha and van Schijndel, Marten and Linzen, Tal},
	date = {2019},
}

@inproceedings{li_specializing_2019,
	location = {Hong Kong, China},
	title = {Specializing Word Embeddings (for Parsing) by Information Bottleneck},
	url = {https://www.aclweb.org/anthology/D19-1276},
	doi = {10.18653/v1/D19-1276},
	abstract = {Pre-trained word embeddings like {ELMo} and {BERT} contain rich syntactic and semantic information, resulting in state-of-the-art performance on various tasks. We propose a very fast variational information bottleneck ({VIB}) method to nonlinearly compress these embeddings, keeping only the information that helps a discriminative parser. We compress each word embedding to either a discrete tag or a continuous vector. In the discrete version, our automatically compressed tags form an alternative tag set: we show experimentally that our tags capture most of the information in traditional {POS} tag annotations, but our tag sequences can be parsed more accurately at the same level of tag granularity. In the continuous version, we show experimentally that moderately compressing the word embeddings by our method yields a more accurate parser in 8 of 9 languages, unlike simple dimensionality reduction.},
	eventtitle = {{EMNLP} 2019},
	pages = {2744--2754},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Xiang Lisa and Eisner, Jason},
	urldate = {2019-11-09},
	date = {2019-11},
}

@inproceedings{hewitt_designing_2019,
	location = {Hong Kong, China},
	title = {Designing and Interpreting Probes with Control Tasks},
	url = {https://www.aclweb.org/anthology/D19-1275},
	doi = {10.18653/v1/D19-1275},
	abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like {ELMo}), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe's capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on {ELMo} representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of {MLPs}, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of {ELMo} yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.},
	eventtitle = {{EMNLP} 2019},
	pages = {2733--2743},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hewitt, John and Liang, Percy},
	urldate = {2019-11-09},
	date = {2019-11},
}

@article{li_enhanced_2019,
	title = {Enhanced Convolutional Neural Tangent Kernels},
	url = {http://arxiv.org/abs/1911.00809},
	abstract = {Recent research shows that for training with \${\textbackslash}ell\_2\$ loss, convolutional neural networks ({CNNs}) whose width (number of channels in convolutional layers) goes to infinity correspond to regression with respect to the {CNN} Gaussian Process kernel ({CNN}-{GP}) if only the last layer is trained, and correspond to regression with respect to the Convolutional Neural Tangent Kernel ({CNTK}) if all layers are trained. An exact algorithm to compute {CNTK} (Arora et al., 2019) yielded the finding that classification accuracy of {CNTK} on {CIFAR}-10 is within 6-7\% of that of that of the corresponding {CNN} architecture (best figure being around 78\%) which is interesting performance for a fixed kernel. Here we show how to significantly enhance the performance of these kernels using two ideas. (1) Modifying the kernel using a new operation called Local Average Pooling ({LAP}) which preserves efficient computability of the kernel and inherits the spirit of standard data augmentation using pixel shifts. Earlier papers were unable to incorporate naive data augmentation because of the quadratic training cost of kernel regression. This idea is inspired by Global Average Pooling ({GAP}), which we show for {CNN}-{GP} and {CNTK} is equivalent to full translation data augmentation. (2) Representing the input image using a pre-processing technique proposed by Coates et al. (2011), which uses a single convolutional layer composed of random image patches. On {CIFAR}-10, the resulting kernel, {CNN}-{GP} with {LAP} and horizontal flip data augmentation, achieves 89\% accuracy, matching the performance of {AlexNet} (Krizhevsky et al., 2012). Note that this is the best such result we know of for a classifier that is not a trained neural network. Similar improvements are obtained for Fashion-{MNIST}.},
	journaltitle = {{arXiv}:1911.00809 [cs, stat]},
	author = {Li, Zhiyuan and Wang, Ruosong and Yu, Dingli and Du, Simon S. and Hu, Wei and Salakhutdinov, Ruslan and Arora, Sanjeev},
	urldate = {2019-11-05},
	date = {2019-11-02},
	eprinttype = {arxiv},
	eprint = {1911.00809},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{garipov_loss_2018,
	title = {Loss surfaces, mode connectivity, and fast ensembling of dnns},
	pages = {8789--8798},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P. and Wilson, Andrew G.},
	date = {2018},
}

@article{zhang_word_2019,
	title = {Word Embedding Visualization Via Dictionary Learning},
	journaltitle = {{arXiv} preprint {arXiv}:1910.03833},
	author = {Zhang, Juexiao and Chen, Yubei and Cheung, Brian and Olshausen, Bruno A.},
	date = {2019},
}

@inproceedings{elad_direct_2019,
	title = {Direct Validation of the Information Bottleneck Principle for Deep Nets},
	url = {http://openaccess.thecvf.com/content_ICCVW_2019/html/SDL-CV/Elad_Direct_Validation_of_the_Information_Bottleneck_Principle_for_Deep_Nets_ICCVW_2019_paper.html},
	eventtitle = {Proceedings of the {IEEE} International Conference on Computer Vision Workshops},
	pages = {0--0},
	author = {Elad, Adar and Haviv, Doron and Blau, Yochai and Michaeli, Tomer},
	urldate = {2019-10-30},
	date = {2019},
}

@article{yun_small_2018,
	title = {Small nonlinearities in activation functions create bad local minima in neural networks},
	url = {https://openreview.net/forum?id=rke_YiRct7},
	abstract = {We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with},
	author = {Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
	urldate = {2019-10-28},
	date = {2018-09-27},
}
@article{combes_learning_2019,
	title = {On the Learning Dynamics of Deep Neural Networks},
	url = {http://arxiv.org/abs/1809.06848},
	abstract = {While a lot of progress has been made in recent years, the dynamics of learning in deep nonlinear neural networks remain to this day largely misunderstood. In this work, we study the case of binary classification and prove various properties of learning in such networks under strong assumptions such as linear separability of the data. Extending existing results from the linear case, we confirm empirical observations by proving that the classification error also follows a sigmoidal shape in nonlinear architectures. We show that given proper initialization, learning expounds parallel independent modes and that certain regions of parameter space might lead to failed training. We also demonstrate that input norm and features' frequency in the dataset lead to distinct convergence speeds which might shed some light on the generalization capabilities of deep neural networks. We provide a comparison between the dynamics of learning with cross-entropy and hinge losses, which could prove useful to understand recent progress in the training of generative adversarial networks. Finally, we identify a phenomenon that we baptize {\textbackslash}textit\{gradient starvation\} where the most frequent features in a dataset prevent the learning of other less frequent but equally informative features.},
	journaltitle = {{arXiv}:1809.06848 [cs, stat]},
	author = {Combes, Remi Tachet des and Pezeshki, Mohammad and Shabanian, Samira and Courville, Aaron and Bengio, Yoshua},
	urldate = {2019-10-26},
	date = {2019-09-06},
	eprinttype = {arxiv},
	eprint = {1809.06848},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{anonymous_truth_2019,
	title = {Truth or backpropaganda? An empirical investigation of deep learning theory},
	url = {https://openreview.net/forum?id=HyxyIgHFvr},
	shorttitle = {Truth or backpropaganda?},
	abstract = {We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  We study the prevalence of local minima in loss landscapes, whether...},
	author = {Anonymous},
	urldate = {2019-10-25},
	date = {2019-09-25},
}

@article{pham_promoting_2019,
	title = {Promoting the Knowledge of Source Syntax in Transformer {NMT} Is Not Needed},
	volume = {23},
	rights = {Hereby I transfer exclusively to the Journal " Computación y         Sistemas ", published by the Computing Research Center ({CIC}-{IPN}),      t    he Copyright of the aforementioned paper. I also accept that these       rights will not be transferred to any other publication, in any other       format, language or other existing means of developing.      I certify that the paper has not been previously disclosed or simultaneo      usly submitted to any other publication, and that it does not contain       material whose publication would violate the Copyright or other       proprietary rights of any person, company or institution. I certify that       I have the permission from the institution or company where I work or       study to publish this work.      The representative author accepts the responsibility for the publication      of this paper on behalf of each and every one of the authors.           This transfer is subject to the following conditions:        The authors retain all ownership rights (such as patent rights) of this work, except for the publishing rights transferred to the {CIC}, through this document.        Authors retain the right to publish the work in whole or in part in any book they are the authors or publishers. They can also make use of this work in conferences, courses, personal web pages, and so on.        Authors may include working as part of his thesis, for non-profit distribution only.},
	issn = {2007-9737},
	url = {https://www.cys.cic.ipn.mx/ojs/index.php/CyS/article/view/3265},
	doi = {10.13053/cys-23-3-3265},
	abstract = {The utility of linguistic annotation in neural machine translation seemed to had been established in past papers. The experiments were however limited to recurrent sequence-to-sequence architectures and relatively small data settings. We focus on the state-of-the-art Transformer model and use comparably larger corpora. Specifically, we try to promote the knowledge of source-side syntax using multi-task learning either through simple data manipulation techniques or through a dedicated model component. In particular, we train one of Transformer attention heads to produce source-side dependency tree. Overall, our results cast some doubt on the utility of multi-task setups with linguistic information. The data manipulation techniques, recommended in previous works, prove  ineffective in large data settings. The treatment of self-attention as dependencies seems much more promising: it helps in translation and reveals that Transformer model can very easily grasp the syntactic structure. An important but curious result is, however, that identical gains are obtained by using trivial ``linear trees'' instead of true dependencies. The reason for the gain thus may not be coming from the added linguistic knowledge but from some simpler regularizing effect we induced on self-attention matrices.},
	number = {3},
	journaltitle = {Computación y Sistemas},
	author = {Pham, Thuong Hai and Macháček, Dominik and Bojar, Ondřej},
	urldate = {2019-10-24},
	date = {2019-09-25},
	langid = {english},
	keywords = {Multi-Task {NMT}, Syntax, multi-task {NMT}, transformer {NMT}},
}

@article{yeh_concept-based_2019,
	title = {On Concept-Based Explanations in Deep Neural Networks},
	journaltitle = {{arXiv} preprint {arXiv}:1910.07969},
	author = {Yeh, Chih-Kuan and Kim, Been and Arik, Sercan O. and Li, Chun-Liang and Ravikumar, Pradeep and Pfister, Tomas},
	date = {2019},
}

@article{murdoch_definitions_2019,
	title = {Definitions, methods, and applications in interpretable machine learning},
	rights = {© 2019 . https://www.pnas.org/site/aboutpnas/licenses.{xhtmlPublished} under the {PNAS} license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/early/2019/10/15/1900654116},
	doi = {10.1073/pnas.1900654116},
	abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant ({PDR}) framework for discussing interpretations. The {PDR} framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the {PDR} framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
	pages = {201900654},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
	urldate = {2019-10-23},
	date = {2019-09-25},
	langid = {english},
	pmid = {31619572},
	keywords = {explainability, interpretability, machine learning, relevancy},
}

@article{soulos_discovering_2019,
	title = {Discovering the Compositional Structure of Vector Representations with Role Learning Networks},
	url = {http://arxiv.org/abs/1910.09113},
	abstract = {Neural networks ({NNs}) are able to perform tasks that rely on compositional structure even though they lack obvious mechanisms for representing this structure. To analyze the internal representations that enable such success, we propose {ROLE}, a technique that detects whether these representations implicitly encode symbolic structure. {ROLE} learns to approximate the representations of a target encoder E by learning a symbolic constituent structure and an embedding of that structure into E's representational vector space. The constituents of the approximating symbol structure are defined by structural positions --- roles --- that can be filled by symbols. We show that when E is constructed to explicitly embed a particular type of structure (string or tree), {ROLE} successfully extracts the ground-truth roles defining that structure. We then analyze a {GRU} seq2seq network trained to perform a more complex compositional task ({SCAN}), where there is no ground truth role scheme available. For this model, {ROLE} successfully discovers an interpretable symbolic structure that the model implicitly uses to perform the {SCAN} task, providing a comprehensive account of the representations that drive the behavior of a frequently-used but hard-to-interpret type of model. We verify the causal importance of the discovered symbolic structure by showing that, when we systematically manipulate hidden embeddings based on this symbolic structure, the model's resulting output is changed in the way predicted by our analysis. Finally, we use {ROLE} to explore whether popular sentence embedding models are capturing compositional structure and find evidence that they are not; we conclude by discussing how insights from {ROLE} can be used to impart new inductive biases to improve the compositional abilities of such models.},
	journaltitle = {{arXiv}:1910.09113 [cs, stat]},
	author = {Soulos, Paul and {McCoy}, Tom and Linzen, Tal and Smolensky, Paul},
	urldate = {2019-10-22},
	date = {2019-10-20},
	eprinttype = {arxiv},
	eprint = {1910.09113},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fort_large_2019,
	title = {Large Scale Structure of Neural Network Loss Landscapes},
	url = {http://arxiv.org/abs/1906.04724},
	abstract = {There are many surprising and perhaps counter-intuitive properties of optimization of deep neural networks. We propose and experimentally verify a unified phenomenological model of the loss landscape that incorporates many of them. High dimensionality plays a key role in our model. Our core idea is to model the loss landscape as a set of high dimensional {\textbackslash}emph\{wedges\} that together form a large-scale, inter-connected structure and towards which optimization is drawn. We first show that hyperparameter choices such as learning rate, network width and \$L\_2\$ regularization, affect the path optimizer takes through the landscape in a similar ways, influencing the large scale curvature of the regions the optimizer explores. Finally, we predict and demonstrate new counter-intuitive properties of the loss-landscape. We show an existence of low loss subspaces connecting a set (not only a pair) of solutions, and verify it experimentally. Finally, we analyze recently popular ensembling techniques for deep networks in the light of our model.},
	journaltitle = {{arXiv}:1906.04724 [cs, stat]},
	author = {Fort, Stanislav and Jastrzebski, Stanislaw},
	urldate = {2019-10-21},
	date = {2019-06-11},
	eprinttype = {arxiv},
	eprint = {1906.04724},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{noauthor_mythos_nodate,
	title = {The Mythos of Model Interpretability - {ACM} Queue},
	url = {https://queue.acm.org/detail.cfm?id=3241340},
	urldate = {2019-10-18},
}

@article{andreas_measuring_2019,
	title = {Measuring Compositionality in Representation Learning},
	url = {http://arxiv.org/abs/1902.07181},
	abstract = {Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.},
	journaltitle = {{arXiv}:1902.07181 [cs, stat]},
	author = {Andreas, Jacob},
	urldate = {2019-10-15},
	date = {2019-02-19},
	eprinttype = {arxiv},
	eprint = {1902.07181},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{li_visualizing_2018,
	title = {Visualizing the Loss Landscape of Neural Nets},
	url = {http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf},
	pages = {6389--6399},
	booktitle = {Advances in Neural Information Processing Systems 31},
	publisher = {Curran Associates, Inc.},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	urldate = {2019-10-10},
	date = {2018},
}

@online{noauthor_generalization_2018,
	title = {The Generalization Mystery: Sharp vs Flat Minima},
	url = {https://www.inference.vc/sharp-vs-flat-minima-are-still-a-mystery-to-me/},
	shorttitle = {The Generalization Mystery},
	abstract = {I set out to write about the following paper I saw people talk about on twitter and reddit:  Hao Li, Zheng Xu, Gavin Taylor, Tom Goldstein Visualizing the Loss Landscape of Neural Nets It's related to this pretty insightful paper:  Laurent Dinh, Razvan Pascanu, Samy Bengio, Yoshua Bengio (2017) Sharp...},
	titleaddon = {{inFERENCe}},
	urldate = {2019-10-10},
	date = {2018-01-18},
	langid = {english},
}

@article{goldt_modelling_2019,
	title = {Modelling the influence of data structure on learning in neural networks},
	url = {http://arxiv.org/abs/1909.11500},
	abstract = {The lack of crisp mathematical models that capture the structure of real-world data sets is a major obstacle to the detailed theoretical understanding of deep neural networks. Here, we first demonstrate the effect of structured data sets by experimentally comparing the dynamics and the performance of two-layer networks trained on two different data sets: (i) an unstructured synthetic data set containing random i.i.d. inputs, and (ii) a simple canonical data set containing {MNIST} images. Our analysis reveals two phenomena related to the dynamics of the networks and their ability to generalise that only appear when training on structured data sets. Second, we introduce a generative model for data sets, where high-dimensional inputs lie on a lower-dimensional manifold and have labels that depend only on their position within this manifold. We call it the hidden manifold model and we experimentally demonstrate that training networks on data sets drawn from this model reproduces both the phenomena seen during training on {MNIST}.},
	journaltitle = {{arXiv}:1909.11500 [cond-mat, stat]},
	author = {Goldt, Sebastian and Mézard, Marc and Krzakala, Florent and Zdeborová, Lenka},
	urldate = {2019-10-02},
	date = {2019-09-25},
	eprinttype = {arxiv},
	eprint = {1909.11500},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
}

@article{swayamdipta_shallow_2019,
	title = {Shallow Syntax in Deep Water},
	url = {http://arxiv.org/abs/1908.11047},
	abstract = {Shallow syntax provides an approximation of phrase-syntactic structure of sentences; it can be produced with high accuracy, and is computationally cheap to obtain. We investigate the role of shallow syntax-aware representations for {NLP} tasks using two techniques. First, we enhance the {ELMo} architecture to allow pretraining on predicted shallow syntactic parses, instead of just raw text, so that contextual embeddings make use of shallow syntactic context. Our second method involves shallow syntactic features obtained automatically on downstream task data. Neither approach leads to a significant gain on any of the four downstream tasks we considered relative to {ELMo}-only baselines. Further analysis using black-box probes confirms that our shallow-syntax-aware contextual embeddings do not transfer to linguistic tasks any more easily than {ELMo}'s embeddings. We take these findings as evidence that {ELMo}-style pretraining discovers representations which make additional awareness of shallow syntax redundant.},
	journaltitle = {{arXiv}:1908.11047 [cs]},
	author = {Swayamdipta, Swabha and Peters, Matthew and Roof, Brendan and Dyer, Chris and Smith, Noah A.},
	urldate = {2019-09-26},
	date = {2019-08-29},
	eprinttype = {arxiv},
	eprint = {1908.11047},
	keywords = {Computer Science - Computation and Language},
}

@article{anonymous_playing_2019,
	title = {Playing the lottery with rewards and multiple languages: lottery tickets in {RL} and {NLP}},
	url = {https://openreview.net/forum?id=S1xnXRVFwH},
	shorttitle = {Playing the lottery with rewards and multiple languages},
	abstract = {The lottery ticket hypothesis proposes that over-parameterization of deep neural networks ({DNNs}) aids training by increasing the probability of a “lucky” sub-network initialization being present...},
	author = {Anonymous},
	urldate = {2019-09-26},
	date = {2019-09-25},
}

@article{anonymous_sooner_2019,
	title = {The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets},
	url = {https://openreview.net/forum?id=BJlNs0VYPB},
	shorttitle = {The Sooner The Better},
	abstract = {The recent success of the lottery ticket hypothesis by Frankle \& Carbin (2018) suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. Several...},
	author = {Anonymous},
	urldate = {2019-09-26},
	date = {2019-09-25},
}

@article{anonymous_early_2019,
	title = {The Early Phase of Neural Network Training},
	url = {https://openreview.net/forum?id=Hkl1iRNFwS},
	abstract = {Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks...},
	author = {Anonymous},
	urldate = {2019-09-26},
	date = {2019-09-25},
}

@article{anonymous_dynamics_2019,
	title = {On the Dynamics and Convergence of Weight Normalization for Training Neural Networks},
	url = {https://openreview.net/forum?id=HJggj3VKPH},
	abstract = {We present a proof of convergence for {ReLU} networks trained with weight normalization. In the analysis, we consider over-parameterized 2-layer {ReLU} networks initialized at random and trained with...},
	author = {Anonymous},
	urldate = {2019-09-26},
	date = {2019-09-25},
}

@article{anonymous_why_2019,
	title = {Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
	url = {https://openreview.net/forum?id=BJgnXpVYwS},
	shorttitle = {Why Gradient Clipping Accelerates Training},
	abstract = {We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural...},
	author = {Anonymous},
	urldate = {2019-09-26},
	date = {2019-09-25},
}

@article{anonymous_towards_2019,
	title = {Towards Interpreting Deep Neural Networks via Understanding Layer Behaviors},
	url = {https://openreview.net/forum?id=rkxMKerYwr},
	abstract = {Deep neural networks ({DNNs}) have achieved unprecedented practical success in many applications.
  However, how to interpret {DNNs} is still an open problem.
  In particular, what do hidden layers behave...},
	author = {Anonymous},
	urldate = {2019-09-26},
	date = {2019-09-25},
}

@article{anonymous_learning_2019,
	title = {Learning to Rank Learning Curves},
	url = {https://openreview.net/forum?id=BJxAHgSYDB},
	abstract = {Many automated machine learning methods, such as those for hyperparameter and neural architecture optimization, are computationally expensive because they involve training many different model...},
	author = {Anonymous},
	urldate = {2019-09-26},
	date = {2019-09-25},
}

@article{anonymous_how_2019,
	title = {{HOW} {THE} {CHOICE} {OF} {ACTIVATION} {AFFECTS} {TRAINING} {OF} {OVERPARAMETRIZED} {NEURAL} {NETS}},
	url = {https://openreview.net/forum?id=rkgfdeBYvH},
	abstract = {It is well-known that overparametrized neural networks trained using gradient based methods quickly achieve small training error with appropriate hyperparameter settings. Recent papers have proved...},
	author = {Anonymous},
	urldate = {2019-09-26},
	date = {2019-09-25},
}

@article{anonymous_drawing_2019,
	title = {Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks},
	url = {https://openreview.net/forum?id=BJxsrgStvr},
	shorttitle = {Drawing Early-Bird Tickets},
	abstract = {(Frankle \& Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies...},
	author = {Anonymous},
	urldate = {2019-09-26},
	date = {2019-09-25},
}

@article{jumelet_analysing_2019,
	title = {Analysing Neural Language Models: Contextual Decomposition Reveals Default Reasoning in Number and Gender Assignment},
	shorttitle = {Analysing Neural Language Models},
	journaltitle = {{arXiv} preprint {arXiv}:1909.08975},
	author = {Jumelet, Jaap and Zuidema, Willem and Hupkes, Dieuwke},
	date = {2019},
}

@article{dyer_critical_2019,
	title = {A Critical Analysis of Biased Parsers in Unsupervised Parsing},
	url = {http://arxiv.org/abs/1909.09428},
	abstract = {A series of recent papers has used a parsing algorithm due to Shen et al. (2018) to recover phrase-structure trees based on proxies for "syntactic depth." These proxy depths are obtained from the representations learned by recurrent language models augmented with mechanisms that encourage the (unsupervised) discovery of hierarchical structure latent in natural language sentences. Using the same parser, we show that proxies derived from a conventional {LSTM} language model produce trees comparably well to the specialized architectures used in previous work. However, we also provide a detailed analysis of the parsing algorithm, showing (1) that it is incomplete---that is, it can recover only a fraction of possible trees---and (2) that it has a marked bias for right-branching structures which results in inflated performance in right-branching languages like English. Our analysis shows that evaluating with biased parsing algorithms can inflate the apparent structural competence of language models.},
	journaltitle = {{arXiv}:1909.09428 [cs]},
	author = {Dyer, Chris and Melis, Gábor and Blunsom, Phil},
	urldate = {2019-09-23},
	date = {2019-09-20},
	eprinttype = {arxiv},
	eprint = {1909.09428},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{he_towards_2019,
	title = {Towards Understanding Neural Machine Translation with Word Importance},
	journaltitle = {{arXiv} preprint {arXiv}:1909.00326},
	author = {He, Shilin and Tu, Zhaopeng and Wang, Xing and Wang, Longyue and Lyu, Michael R. and Shi, Shuming},
	date = {2019},
}

@inproceedings{saphra_sparsity_2019,
	title = {Sparsity Emerges Naturally in Neural Language Models},
	url = {https://openreview.net/forum?id=H1ets1h56E},
	abstract = {Concerns about interpretability, computational resources, and principled inductive priors have motivated efforts to engineer sparse  neural  models for {NLP} tasks. If sparsity is important for {NLP}...},
	booktitle = {{ICML} Workshop Deep Phenomena},
	author = {Saphra, Naomi and Lopez, Adam},
	urldate = {2019-09-19},
	date = {2019-05-28},
}

@incollection{arras_explaining_2019,
	location = {Cham},
	title = {Explaining and Interpreting {LSTMs}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_11},
	series = {Lecture Notes in Computer Science},
	abstract = {While neural networks have acted as a strong unifying force in the design of modern {AI} systems, the neural network architectures themselves remain highly heterogeneous due to the variety of tasks to be solved. In this chapter, we explore how to adapt the Layer-wise Relevance Propagation ({LRP}) technique used for explaining the predictions of feed-forward networks to the {LSTM} architecture used for sequential data modeling and forecasting. The special accumulators and gated interactions present in the {LSTM} require both a new propagation scheme and an extension of the underlying theoretical framework to deliver faithful explanations.},
	pages = {211--238},
	booktitle = {Explainable {AI}: Interpreting, Explaining and Visualizing Deep Learning},
	publisher = {Springer International Publishing},
	author = {Arras, Leila and Arjona-Medina, José and Widrich, Michael and Montavon, Grégoire and Gillhofer, Michael and Müller, Klaus-Robert and Hochreiter, Sepp and Samek, Wojciech},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Muller, Klaus-Robert},
	urldate = {2019-09-19},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-030-28954-6_11},
	keywords = {Explainable artificial intelligence, Interpretability, {LSTM}, Model transparency, Recurrent neural networks},
}

@article{warstadt_investigating_2019,
	title = {Investigating {BERT}'s Knowledge of Language: Five Analysis Methods with {NPIs}},
	url = {http://arxiv.org/abs/1909.02597},
	shorttitle = {Investigating {BERT}'s Knowledge of Language},
	abstract = {Though state-of-the-art sentence representation models can perform tasks requiring significant knowledge of grammar, it is an open question how best to evaluate their grammatical knowledge. We explore five experimental methods inspired by prior work evaluating pretrained sentence representation models. We use a single linguistic phenomenon, negative polarity item ({NPI}) licensing in English, as a case study for our experiments. {NPIs} like "any" are grammatical only if they appear in a licensing environment like negation ("Sue doesn't have any cats" vs. "Sue has any cats"). This phenomenon is challenging because of the variety of {NPI} licensing environments that exist. We introduce an artificially generated dataset that manipulates key features of {NPI} licensing for the experiments. We find that {BERT} has significant knowledge of these features, but its success varies widely across different experimental methods. We conclude that a variety of methods is necessary to reveal all relevant aspects of a model's grammatical knowledge in a given domain.},
	journaltitle = {{arXiv}:1909.02597 [cs]},
	author = {Warstadt, Alex and Cao, Yu and Grosu, Ioana and Peng, Wei and Blix, Hagen and Nie, Yining and Alsop, Anna and Bordia, Shikha and Liu, Haokun and Parrish, Alicia and Wang, Sheng-Fu and Phang, Jason and Mohananey, Anhad and Htut, Phu Mon and Jeretič, Paloma and Bowman, Samuel R.},
	urldate = {2019-09-19},
	date = {2019-09-05},
	eprinttype = {arxiv},
	eprint = {1909.02597},
	keywords = {Computer Science - Computation and Language},
}

@article{lan_lca:_2019,
	title = {{LCA}: Loss Change Allocation for Neural Network Training},
	url = {http://arxiv.org/abs/1909.01440},
	shorttitle = {{LCA}},
	abstract = {Neural networks enjoy widespread use, but many aspects of their training, representation, and operation are poorly understood. In particular, our view into the training process is limited, with a single scalar loss being the most common viewport into this high-dimensional, dynamic process. We propose a new window into training called Loss Change Allocation ({LCA}), in which credit for changes to the network loss is conservatively partitioned to the parameters. This measurement is accomplished by decomposing the components of an approximate path integral along the training trajectory using a Runge-Kutta integrator. This rich view shows which parameters are responsible for decreasing or increasing the loss during training, or which parameters "help" or "hurt" the network's learning, respectively. {LCA} may be summed over training iterations and/or over neurons, channels, or layers for increasingly coarse views. This new measurement device produces several insights into training. (1) We find that barely over 50\% of parameters help during any given iteration. (2) Some entire layers hurt overall, moving on average against the training gradient, a phenomenon we hypothesize may be due to phase lag in an oscillatory training process. (3) Finally, increments in learning proceed in a synchronized manner across layers, often peaking on identical iterations.},
	journaltitle = {{arXiv}:1909.01440 [cs, stat]},
	author = {Lan, Janice and Liu, Rosanne and Zhou, Hattie and Yosinski, Jason},
	urldate = {2019-09-16},
	date = {2019-09-03},
	eprinttype = {arxiv},
	eprint = {1909.01440},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{noauthor_introducing_2019,
	title = {Introducing {LCA}: Loss Change Allocation for Neural Network Training},
	url = {https://eng.uber.com/loss-change-allocation/},
	shorttitle = {Introducing {LCA}},
	abstract = {Uber {AI} Labs proposes Loss Change Allocation ({LCA}), a new method that provides a rich window into the neural network training process.},
	titleaddon = {Uber Engineering Blog},
	urldate = {2019-09-16},
	date = {2019-09-10},
	langid = {american},
}

@article{voita_bottom-up_2019,
	title = {The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives},
	url = {http://arxiv.org/abs/1909.01380},
	shorttitle = {The Bottom-up Evolution of Representations in the Transformer},
	abstract = {We seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We focus on the Transformers for our analysis as they have been shown effective on various tasks, including machine translation ({MT}), standard left-to-right language models ({LM}) and masked language modeling ({MLM}). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and how this process depends on the choice of learning objective. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for {MLM}, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top {MLM} layers.},
	journaltitle = {{arXiv}:1909.01380 [cs]},
	author = {Voita, Elena and Sennrich, Rico and Titov, Ivan},
	urldate = {2019-09-16},
	date = {2019-09-03},
	eprinttype = {arxiv},
	eprint = {1909.01380},
	keywords = {Computer Science - Computation and Language},
}

@article{yang_mean_2019,
	title = {A Mean Field Theory of Batch Normalization},
	url = {http://arxiv.org/abs/1902.08129},
	abstract = {We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.},
	journaltitle = {{arXiv}:1902.08129 [cond-mat]},
	author = {Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S.},
	urldate = {2019-09-05},
	date = {2019-02-21},
	eprinttype = {arxiv},
	eprint = {1902.08129},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Dynamical Systems},
}

@online{dittmer_singular_2018,
	title = {Singular Values for {ReLU} Layers},
	url = {/paper/Singular-Values-for-ReLU-Layers-Dittmer-King/c1bbf0ec57b3411b0e05659cf4a37f7b79551d51},
	abstract = {Despite their prevalence in neural networks we still lack a thorough theoretical characterization of {ReLU} layers. This paper aims to further our understanding of {ReLU} layers by studying how the activation function {ReLU} interacts with the linear component of the layer and what role this interaction plays in the success of the neural network in achieving its intended task. To this end, we introduce two new tools: {ReLU} singular values of operators and the Gaussian mean width of operators. By presenting on the one hand theoretical justifications, results, and interpretations of these two concepts and on the other hand numerical experiments and results of the {ReLU} singular values and the Gaussian mean width being applied to trained neural networks, we hope to give a comprehensive, singular-value-centric view of {ReLU} layers. We find that {ReLU} singular values and the Gaussian mean width do not only enable theoretical insights, but also provide one with metrics which seem promising for practical applications. In particular, these measures can be used to distinguish correctly and incorrectly classified data as it traverses the network. We conclude by introducing two tools based on our findings: double-layers and harmonic pruning.},
	titleaddon = {undefined},
	author = {Dittmer, Sören and King, Emily J. and Maass, Peter},
	urldate = {2019-09-05},
	date = {2018},
	langid = {english},
}

@online{liu_deep_2019,
	title = {Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective},
	url = {/paper/Deep-Learning-Theory-Review%3A-An-Optimal-Control-and-Liu-Theodorou/4fc08dca4cfbff4200b3330f09fb9e385b791ca2},
	shorttitle = {Deep Learning Theory Review},
	abstract = {Attempts from different disciplines to provide a fundamental understanding of deep learning have advanced rapidly in recent years, yet a unified framework remains relatively limited. In this article, we provide one possible way to align existing branches of deep learning theory through the lens of dynamical system and optimal control. By viewing deep neural networks as discrete-time nonlinear dynamical systems, we can analyze how information propagates through layers using mean field theory. When optimization algorithms are further recast as controllers, the ultimate goal of training processes can be formulated as an optimal control problem. In addition, we can reveal convergence and generalization properties by studying the stochastic dynamics of optimization algorithms. This viewpoint features a wide range of theoretical study from information bottleneck to statistical physics. It also provides a principled way for hyper-parameter tuning when optimal control theory is introduced. Our framework fits nicely with supervised learning and can be extended to other learning problems, such as Bayesian learning, adversarial training, and specific forms of meta learning, without efforts. The review aims to shed lights on the importance of dynamics and optimal control when developing deep learning theory.},
	author = {Liu, Guan-Horng and Theodorou, Evangelos A.},
	urldate = {2019-09-05},
	date = {2019},
	langid = {english},
}

@online{panigrahi_effect_2019,
	title = {Effect of Activation Functions on the Training of Overparametrized Neural Nets},
	url = {/paper/Effect-of-Activation-Functions-on-the-Training-of-Panigrahi-Shetty/bf6529305a9640b8878c7728f8ac3c36134d60e2},
	abstract = {It is well-known that overparametrized neural networks trained using gradient-based methods quickly achieve small training error with appropriate hyperparameter settings. Recent papers have proved this statement theoretically for highly overparametrized networks under reasonable assumptions. The limiting case when the network size approaches infinity has also been considered. These results either assume that the activation function is {ReLU} or they crucially depend on the minimum eigenvalue of a certain Gram matrix depending on the data, random initialization and the activation function. In the latter case, existing works only prove that this minimum eigenvalue is non-zero and do not provide quantitative bounds. On the empirical side, a contemporary line of investigations has proposed a number of alternative activation functions which tend to perform better than {ReLU} at least in some settings but no clear understanding has emerged. This state of affairs underscores the importance of theoretically understanding the impact of activation functions on training. In the present paper, we provide theoretical results about the effect of activation function on the training of highly overparametrized 2-layer neural networks. We show that for smooth activations, such as tanh and swish, the minimum eigenvalue can be exponentially small depending on the span of the dataset implying that the training can be very slow. In contrast, for activations with a “kink,” such as {ReLU},{SELU},{ELU}, all eigenvalues are large under minimal assumptions on the data. Several new ideas are involved. Finally, we corroborate our results empirically.},
	titleaddon = {undefined},
	author = {Panigrahi, Abhishek and Shetty, Abhishek and Goyal, Navin},
	urldate = {2019-09-05},
	date = {2019},
	langid = {english},
}
@online{wolnitza_feature_2019,
	title = {Feature selection of neural networks is skewed towards the less abstract cue},
	url = {/paper/Feature-selection-of-neural-networks-is-skewed-the-Wolnitza-Dellen/1fa975ceb36dd0cb5cf4ecf199e16d856b14acc5},
	abstract = {Artificial neural networks ({ANNs}) have become an important tool for image classification with many applications in research and industry. However, it remains largely unknown how relevant image features are selected and how data properties affect this process. In particular, we are interested whether the abstraction level of image cues correlating with class membership influences feature selection. We perform experiments with binary images that contain a combination of cues, representing two different levels of abstractions: one is a pattern drawn from a random distribution where class membership correlates with the statistics of the pattern, the other a combination of symbol-like entities, where the symbolic code correlates with class membership. When the network is trained with data in which both cues are equally significant, we observe that the cues at the lower abstraction level, i.e., the pattern, is learned, while the symbolic information is largely ignored, even in networks with many layers. Symbol-like entities are only learned if the importance of low-level cues is reduced compared to the high-level ones. These findings raise important questions about the relevance of features that are learned by deep {ANNs} and how learning could be shifted towards symbolic features.},
	titleaddon = {undefined},
	author = {Wolnitza, Marcell and Dellen, Babette},
	urldate = {2019-09-05},
	date = {2019},
	langid = {english},
}

@online{gigante_visualizing_2019,
	title = {Visualizing the {PHATE} of Neural Networks},
	url = {/paper/Visualizing-the-PHATE-of-Neural-Networks-Gigante-Charles/c5f5515a564187ae59b6e60dcddaa6d842efd8c3},
	abstract = {Understanding why and how certain neural networks outperform others is key to guiding future development of network architectures and optimization methods. To this end, we introduce a novel visualization algorithm that reveals the internal geometry of such networks: Multislice {PHATE} (M-{PHATE}), the first method designed explicitly to visualize how a neural network\&\#39;s hidden representations of data evolve throughout the course of training. We demonstrate that our visualization provides intuitive, detailed summaries of the learning dynamics beyond simple global measures (i.e., validation loss and accuracy), without the need to access validation data. Furthermore, M-{PHATE} better captures both the dynamics and community structure of the hidden units as compared to visualization based on standard dimensionality reduction methods (e.g., {ISOMAP}, t-{SNE}). We demonstrate M-{PHATE} with two vignettes: continual learning and generalization. In the former, the M-{PHATE} visualizations display the mechanism of \&quot;catastrophic forgetting\&quot; which is a major challenge for learning in task-switching contexts. In the latter, our visualizations reveal how increased heterogeneity among hidden units correlates with improved generalization performance. An implementation of M-{PHATE}, along with scripts to reproduce the figures in this paper, is available at https://github.com/scottgigante/M-{PHATE}.},
	titleaddon = {undefined},
	author = {Gigante, Scott and Charles, Adam S. and Krishnaswamy, Smita and Mishne, Gal},
	urldate = {2019-09-05},
	date = {2019},
	langid = {english},
}

@article{allen_what_2018,
	title = {What the Vec? Towards Probabilistically Grounded Embeddings},
	url = {http://arxiv.org/abs/1805.12164},
	shorttitle = {What the Vec?},
	abstract = {Word2Vec (W2V) and Glove are popular word embedding algorithms that perform well on a variety of natural language processing tasks. The algorithms are fast, efficient and their embeddings widely used. Moreover, the W2V algorithm has recently been adopted in the field of graph embedding, where it underpins several leading algorithms. However, despite their ubiquity and the relative simplicity of their common architecture, what the embedding parameters of W2V and Glove learn and why that it useful in downstream tasks largely remains a mystery. We show that different interactions of {PMI} vectors encode semantic properties that can be captured in low dimensional word embeddings by suitable projection, theoretically explaining why the embeddings of W2V and Glove work, and, in turn, revealing an interesting mathematical interconnection between the semantic relationships of relatedness, similarity, paraphrase and analogy.},
	journaltitle = {{arXiv}:1805.12164 [cs, stat]},
	author = {Allen, Carl and Balažević, Ivana and Hospedales, Timothy},
	urldate = {2019-09-03},
	date = {2018-05-30},
	eprinttype = {arxiv},
	eprint = {1805.12164},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{geman_neural_1992,
	title = {Neural networks and the bias/variance dilemma},
	volume = {4},
	pages = {1--58},
	number = {1},
	journaltitle = {Neural computation},
	author = {Geman, Stuart and Bienenstock, Elie and Doursat, René},
	date = {1992},
}

@article{belkin_reconciling_2018,
	title = {Reconciling modern machine learning and the bias-variance trade-off},
	url = {http://arxiv.org/abs/1812.11118},
	abstract = {The question of generalization in machine learning---how algorithms are able to learn predictors from a training sample to make accurate predictions out-of-sample---is revisited in light of the recent breakthroughs in modern machine learning technology. The classical approach to understanding generalization is based on bias-variance trade-offs, where model complexity is carefully calibrated so that the fit on the training sample reflects performance out-of-sample. However, it is now common practice to fit highly complex models like deep neural networks to data with (nearly) zero training error, and yet these interpolating predictors are observed to have good out-of-sample accuracy even for noisy data. How can the classical understanding of generalization be reconciled with these observations from modern machine learning practice? In this paper, we bridge the two regimes by exhibiting a new "double descent" risk curve that extends the traditional U-shaped bias-variance curve beyond the point of interpolation. Specifically, the curve shows that as soon as the model complexity is high enough to achieve interpolation on the training sample---a point that we call the "interpolation threshold"---the risk of suitably chosen interpolating predictors from these models can, in fact, be decreasing as the model complexity increases, often below the risk achieved using non-interpolating models. The double descent risk curve is demonstrated for a broad range of models, including neural networks and random forests, and a mechanism for producing this behavior is posited.},
	journaltitle = {{arXiv}:1812.11118 [cs, stat]},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	urldate = {2019-08-29},
	date = {2018-12-28},
	eprinttype = {arxiv},
	eprint = {1812.11118},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hupkes_compositionality_2019,
	title = {The compositionality of neural networks: integrating symbolism and connectionism},
	url = {http://arxiv.org/abs/1908.08351},
	shorttitle = {The compositionality of neural networks},
	abstract = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub {PCFG} {SET} and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution based and a transformer model. We provide an in depth analysis of the results, that uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
	journaltitle = {{arXiv}:1908.08351 [cs, stat]},
	author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
	urldate = {2019-08-23},
	date = {2019-08-22},
	eprinttype = {arxiv},
	eprint = {1908.08351},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wiegreffe_attention_2019,
	title = {Attention is not not Explanation},
	url = {http://arxiv.org/abs/1908.04626},
	abstract = {Attention mechanisms play a central role in {NLP} systems, especially within recurrent neural network ({RNN}) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in {RNN} models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
	journaltitle = {{arXiv}:1908.04626 [cs]},
	author = {Wiegreffe, Sarah and Pinter, Yuval},
	urldate = {2019-08-22},
	date = {2019-08-13},
	eprinttype = {arxiv},
	eprint = {1908.04626},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{schluter_when_2018,
	location = {Brussels, Belgium},
	title = {When data permutations are pathological: the case of neural natural language inference},
	url = {https://www.aclweb.org/anthology/D18-1534},
	doi = {10.18653/v1/D18-1534},
	shorttitle = {When data permutations are pathological},
	abstract = {Consider two competitive machine learning models, one of which was considered state-of-the art, and the other a competitive baseline. Suppose that by just permuting the examples of the training set, say by reversing the original order, by shuffling, or by mini-batching, you could report substantially better/worst performance for the system of your choice, by multiple percentage points. In this paper, we illustrate this scenario for a trending {NLP} task: Natural Language Inference ({NLI}). We show that for the two central {NLI} corpora today, the learning process of neural systems is far too sensitive to permutations of the data. In doing so we reopen the question of how to judge a good neural architecture for {NLI}, given the available dataset and perhaps, further, the soundness of the {NLI} task itself in its current state.},
	eventtitle = {{EMNLP} 2018},
	pages = {4935--4939},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Schluter, Natalie and Varab, Daniel},
	urldate = {2019-08-13},
	date = {2018-10},
}

@inproceedings{gorman_we_2019,
	location = {Florence, Italy},
	title = {We Need to Talk about Standard Splits},
	url = {https://www.aclweb.org/anthology/P19-1267},
	abstract = {It is standard practice in speech \& language technology to rank systems according to their performance on a test set held out for evaluation. However, few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018, each of which claimed state-of-the-art performance on a widely-used “standard split”. While we replicate results on the standard split, we fail to reliably reproduce some rankings when we repeat this analysis with randomly generated training-testing splits. We argue that randomly generated splits should be used in system evaluation.},
	eventtitle = {{ACL} 2019},
	pages = {2786--2791},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Gorman, Kyle and Bedrick, Steven},
	urldate = {2019-08-13},
	date = {2019-07},
}

@article{brunner_validity_2019,
	title = {On the Validity of Self-Attention as Explanation in Transformer Models},
	url = {http://arxiv.org/abs/1908.04211},
	abstract = {Explainability of deep learning systems is a vital requirement for many applications. However, it is still an unsolved problem. Recent self-attention based models for natural language processing, such as the Transformer or {BERT}, offer hope of greater explainability by providing attention maps that can be directly inspected. Nevertheless, by just looking at the attention maps one often overlooks that the attention is not over words but over hidden embeddings, which themselves can be mixed representations of multiple embeddings. We investigate to what extent the implicit assumption made in many recent papers - that hidden embeddings at all layers still correspond to the underlying words - is justified. We quantify how much embeddings are mixed based on a gradient based attribution method and find that already after the first layer less than 50\% of the embedding is attributed to the underlying word, declining thereafter to a median contribution of 7.5\% in the last layer. While throughout the layers the underlying word remains as the one contributing most to the embedding, we argue that attention visualizations are misleading and should be treated with care when explaining the underlying deep learning system.},
	journaltitle = {{arXiv}:1908.04211 [cs]},
	author = {Brunner, Gino and Liu, Yang and Pascual, Damián and Richter, Oliver and Wattenhofer, Roger},
	urldate = {2019-08-13},
	date = {2019-08-12},
	eprinttype = {arxiv},
	eprint = {1908.04211},
	keywords = {46-04, Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7, I.7.0},
}

@article{sawatzky_visualizing_2019,
	title = {Visualizing {RNN} States with Predictive Semantic Encodings},
	url = {http://arxiv.org/abs/1908.00588},
	abstract = {Recurrent Neural Networks are an effective and prevalent tool used to model sequential data such as natural language text. However, their deep nature and massive number of parameters pose a challenge for those intending to study precisely how they work. We present a visual technique that gives a high level intuition behind the semantics of the hidden states within Recurrent Neural Networks. This semantic encoding allows for hidden states to be compared throughout the model independent of their internal details. The proposed technique is displayed in a proof of concept visualization tool which is demonstrated to visualize the natural language processing task of language modelling.},
	journaltitle = {{arXiv}:1908.00588 [cs]},
	author = {Sawatzky, Lindsey and Bergner, Steven and Popowich, Fred},
	urldate = {2019-08-13},
	date = {2019-08-01},
	eprinttype = {arxiv},
	eprint = {1908.00588},
	keywords = {Computer Science - Computation and Language},
}

@article{liang_knowledge_2019,
	title = {Knowledge Isomorphism between Neural Networks},
	url = {http://arxiv.org/abs/1908.01581},
	abstract = {This paper aims to analyze knowledge isomorphism between pre-trained deep neural networks. We propose a generic definition for knowledge isomorphism between neural networks at different fuzziness levels, and design a task-agnostic and model-agnostic method to disentangle and quantify isomorphic features from intermediate layers of a neural network. As a generic tool, our method can be broadly used for different applications. In preliminary experiments, we have used knowledge isomorphism as a tool to diagnose feature representations of neural networks. Knowledge isomorphism provides new insights to explain the success of existing deep-learning techniques, such as knowledge distillation and network compression. More crucially, it has been shown that knowledge isomorphism can also be used to refine pre-trained networks and boost performance.},
	journaltitle = {{arXiv}:1908.01581 [cs, stat]},
	author = {Liang, Ruofan and Li, Tianlin and Li, Longfei and Zhang, Quanshi},
	urldate = {2019-08-13},
	date = {2019-08-05},
	eprinttype = {arxiv},
	eprint = {1908.01581},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kuditipudi_explaining_2019,
	title = {Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets},
	url = {http://arxiv.org/abs/1906.06247},
	abstract = {Mode connectivity is a surprising phenomenon in the loss landscape of deep nets. Optima---at least those discovered by gradient-based optimization---turn out to be connected by simple paths on which the loss function is almost constant. Often, these paths can be chosen to be piece-wise linear, with as few as two segments. We give mathematical explanations for this phenomenon, assuming generic properties (such as dropout stability and noise stability) of well-trained deep nets, which have previously been identified as part of understanding the generalization properties of deep nets. Our explanation holds for realistic multilayer nets, and experiments are presented to verify the theory.},
	journaltitle = {{arXiv}:1906.06247 [cs, stat]},
	author = {Kuditipudi, Rohith and Wang, Xiang and Lee, Holden and Zhang, Yi and Li, Zhiyuan and Hu, Wei and Arora, Sanjeev and Ge, Rong},
	urldate = {2019-08-07},
	date = {2019-06-14},
	eprinttype = {arxiv},
	eprint = {1906.06247},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jacot_neural_2018,
	title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
	url = {http://arxiv.org/abs/1806.07572},
	shorttitle = {Neural Tangent Kernel},
	abstract = {At initialization, artificial neural networks ({ANNs}) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an {ANN} during training can also be described by a kernel: during gradient descent on the parameters of an {ANN}, the network function \$f\_{\textbackslash}theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel ({NTK}). This kernel is central to describe the generalization features of {ANNs}. While the {NTK} is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of {ANNs} in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting {NTK}. We prove the positive-definiteness of the limiting {NTK} when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_{\textbackslash}theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the {NTK}, hence suggesting a theoretical motivation for early stopping. Finally we study the {NTK} numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
	journaltitle = {{arXiv}:1806.07572 [cs, math, stat]},
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément},
	urldate = {2019-08-07},
	date = {2018-06-20},
	eprinttype = {arxiv},
	eprint = {1806.07572},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Probability, Statistics - Machine Learning},
}

@online{schwartz_green_2019,
	title = {Green {AI}},
	url = {/paper/Green-AI-Schwartz-Dodge/a7b40b547461294364a9a004ce249817d0329112},
	abstract = {The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers from emerging economies to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or “price tag” of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make {AI} both greener and more inclusive—enabling any inspired undergraduate with a laptop to write high-quality research papers. Green {AI} is an emerging focus at the Allen Institute for {AI}.},
	author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
	urldate = {2019-08-06},
	date = {2019},
	langid = {english},
}

@article{jordan_gated_2019,
	title = {Gated recurrent units viewed through the lens of continuous time dynamical systems},
	url = {http://arxiv.org/abs/1906.01005},
	abstract = {Gated recurrent units ({GRUs}) are specialized memory elements for building recurrent neural networks. Despite their incredible success in natural language, speech, and video processing, little is understood about the specific dynamics representable in a {GRU} network, along with the constraints these dynamics impose when generalizing a specific task. As a result, it is difficult to know a priori how successful a {GRU} network will perform on a given task. Using a continuous time analysis, we gain intuition on the inner workings of {GRU} networks. We restrict our presentation to low dimensions to allow for a comprehensive visualization. We found a surprisingly rich repertoire of dynamical features that includes stable limit cycles (nonlinear oscillations), multi-stable dynamics with various topologies, and homoclinic orbits. We contextualize the usefulness of the different kinds of dynamics and experimentally test their existence.},
	journaltitle = {{arXiv}:1906.01005 [cs, stat]},
	author = {Jordan, Ian D. and Sokol, Piotr Aleksander and Park, Il Memming},
	urldate = {2019-07-26},
	date = {2019-06-03},
	eprinttype = {arxiv},
	eprint = {1906.01005},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{maheswaranathan_universality_2019,
	title = {Universality and individuality in neural dynamics across large populations of recurrent networks},
	url = {http://arxiv.org/abs/1907.08549},
	abstract = {Task-based modeling with recurrent neural networks ({RNNs}) has emerged as a popular way to infer the computational function of different brain regions. These models are quantitatively assessed by comparing the low-dimensional neural representations of the model with the brain, for example using canonical correlation analysis ({CCA}). However, the nature of the detailed neurobiological inferences one can draw from such efforts remains elusive. For example, to what extent does training neural networks to solve common tasks uniquely determine the network dynamics, independent of modeling architectural choices? Or alternatively, are the learned dynamics highly sensitive to different model choices? Knowing the answer to these questions has strong implications for whether and how we should use task-based {RNN} modeling to understand brain dynamics. To address these foundational questions, we study populations of thousands of networks, with commonly used {RNN} architectures, trained to solve neuroscientifically motivated tasks and characterize their nonlinear dynamics. We find the geometry of the {RNN} representations can be highly sensitive to different network architectures, yielding a cautionary tale for measures of similarity that rely representational geometry, such as {CCA}. Moreover, we find that while the geometry of neural dynamics can vary greatly across architectures, the underlying computational scaffold---the topological structure of fixed points, transitions between them, limit cycles, and linearized dynamics---often appears universal across all architectures.},
	journaltitle = {{arXiv}:1907.08549 [cs, q-bio]},
	author = {Maheswaranathan, Niru and Williams, Alex H. and Golub, Matthew D. and Ganguli, Surya and Sussillo, David},
	urldate = {2019-07-26},
	date = {2019-07-19},
	eprinttype = {arxiv},
	eprint = {1907.08549},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@article{wilcox_hierarchical_2019,
	title = {Hierarchical Representation in Neural Language Models: Suppression and Recovery of Expectations},
	url = {http://arxiv.org/abs/1906.04068},
	shorttitle = {Hierarchical Representation in Neural Language Models},
	abstract = {Deep learning sequence models have led to a marked increase in performance for a range of Natural Language Processing tasks, but it remains an open question whether they are able to induce proper hierarchical generalizations for representing natural language from linear input alone. Work using artificial languages as training input has shown that {LSTMs} are capable of inducing the stack-like data structures required to represent context-free and certain mildly context-sensitive languages---formal language classes which correspond in theory to the hierarchical structures of natural language. Here we present a suite of experiments probing whether neural language models trained on linguistic data induce these stack-like data structures and deploy them while incrementally predicting words. We study two natural language phenomena: center embedding sentences and syntactic island constraints on the filler--gap dependency. In order to properly predict words in these structures, a model must be able to temporarily suppress certain expectations and then recover those expectations later, essentially pushing and popping these expectations on a stack. Our results provide evidence that models can successfully suppress and recover expectations in many cases, but do not fully recover their previous grammatical state.},
	journaltitle = {{arXiv}:1906.04068 [cs]},
	author = {Wilcox, Ethan and Levy, Roger and Futrell, Richard},
	urldate = {2019-07-22},
	date = {2019-06-10},
	eprinttype = {arxiv},
	eprint = {1906.04068},
	keywords = {Computer Science - Computation and Language},
}

@article{lin_open_2019,
	title = {Open Sesame: Getting Inside {BERT}'s Linguistic Knowledge},
	url = {http://arxiv.org/abs/1906.01698},
	shorttitle = {Open Sesame},
	abstract = {How and to what extent does {BERT} encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like {BERT} perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of {BERT}'s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores {BERT}'s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that {BERT} encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that {BERT}'s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.},
	journaltitle = {{arXiv}:1906.01698 [cs]},
	author = {Lin, Yongjie and Tan, Yi Chern and Frank, Robert},
	urldate = {2019-07-22},
	date = {2019-06-04},
	eprinttype = {arxiv},
	eprint = {1906.01698},
	keywords = {Computer Science - Computation and Language},
}

@article{de_lhoneux_what_2019,
	title = {What Should/Do/Can {LSTMs} Learn When Parsing Auxiliary Verb Constructions?},
	url = {http://arxiv.org/abs/1907.07950},
	abstract = {This article is a linguistic investigation of a neural parser. We look at transitivity and agreement information of auxiliary verb constructions ({AVCs}) in comparison to finite main verbs ({FMVs}). This comparison is motivated by theoretical work in dependency grammar and in particular the work of Tesni{\textbackslash}`ere (1959) where {AVCs} and {FMVs} are both instances of a nucleus, the basic unit of syntax. An {AVC} is a dissociated nucleus, it consists of at least two words, and a {FMV} is its non-dissociated counterpart, consisting of exactly one word. We suggest that the representation of {AVCs} and {FMVs} should capture similar information. We use diagnostic classifiers to probe agreement and transitivity information in vectors learned by a transition-based neural parser in four typologically different languages. We find that the parser learns different information about {AVCs} and {FMVs} if only sequential models ({BiLSTMs}) are used in the architecture but similar information when a recursive layer is used. We find explanations for why this is the case by looking closely at how information is learned in the network and looking at what happens with different dependency representations of {AVCs}.},
	journaltitle = {{arXiv}:1907.07950 [cs]},
	author = {de Lhoneux, Miryam and Stymne, Sara and Nivre, Joakim},
	urldate = {2019-07-19},
	date = {2019-07-18},
	eprinttype = {arxiv},
	eprint = {1907.07950},
	keywords = {Computer Science - Computation and Language},
}

@article{niven_probing_2019,
	title = {Probing Neural Network Comprehension of Natural Language Arguments},
	url = {http://arxiv.org/abs/1907.07355},
	abstract = {We are surprised to find that {BERT}'s peak performance of 77\% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.},
	journaltitle = {{arXiv}:1907.07355 [cs]},
	author = {Niven, Timothy and Kao, Hung-Yu},
	urldate = {2019-07-18},
	date = {2019-07-17},
	eprinttype = {arxiv},
	eprint = {1907.07355},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{bartlett_gradient_2018,
	title = {Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks},
	url = {http://proceedings.mlr.press/v80/bartlett18a.html},
	abstract = {We analyze algorithms for approximating a function \$f(x) = {\textbackslash}Phi x\$ mapping \${\textbackslash}Re{\textasciicircum}d\$ to \${\textbackslash}Re{\textasciicircum}d\$ using deep linear neural networks, i.e. that learn a function \$h\$ parameterized by matrices \${\textbackslash}Theta\_1,....},
	eventtitle = {International Conference on Machine Learning},
	pages = {521--530},
	booktitle = {International Conference on Machine Learning},
	author = {Bartlett, Peter and Helmbold, Dave and Long, Philip},
	urldate = {2019-07-18},
	date = {2018-07-03},
	langid = {english},
}

@article{sankar_analysis_2018,
	title = {On the Analysis of Trajectories of Gradient Descent in the Optimization of Deep Neural Networks},
	url = {http://arxiv.org/abs/1807.08140},
	abstract = {Theoretical analysis of the error landscape of deep neural networks has garnered significant interest in recent years. In this work, we theoretically study the importance of noise in the trajectories of gradient descent towards optimal solutions in multi-layer neural networks. We show that adding noise (in different ways) to a neural network while training increases the rank of the product of weight matrices of a multi-layer linear neural network. We thus study how adding noise can assist reaching a global optimum when the product matrix is full-rank (under certain conditions). We establish theoretical foundations between the noise induced into the neural network - either to the gradient, to the architecture, or to the input/output to a neural network - and the rank of product of weight matrices. We corroborate our theoretical findings with empirical results.},
	journaltitle = {{arXiv}:1807.08140 [cs, math, stat]},
	author = {Sankar, Adepu Ravi and Srinivasan, Vishwak and Balasubramanian, Vineeth N.},
	urldate = {2019-07-17},
	date = {2018-07-21},
	eprinttype = {arxiv},
	eprint = {1807.08140},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{murdoch_beyond_2018,
	title = {Beyond Word Importance: Contextual Decomposition to Extract Interactions from {LSTMs}},
	url = {https://openreview.net/forum?id=rkRwGg-0Z},
	shorttitle = {Beyond Word Importance},
	abstract = {The driving force behind the recent success of {LSTMs} has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to...},
	booktitle = {{ICLR}},
	author = {Murdoch, W. James and Liu, Peter J. and Yu, Bin},
	urldate = {2018-10-02},
	date = {2018-02-15},
}

@article{dettmers_sparse_2019,
	title = {Sparse Networks from Scratch: Faster Training without Losing Performance},
	url = {http://arxiv.org/abs/1907.04840},
	shorttitle = {Sparse Networks from Scratch},
	abstract = {We demonstrate the possibility of what we call sparse learning: accelerated training of deep neural networks that maintain sparse weights throughout training while achieving performance levels competitive with dense networks. We accomplish this by developing sparse momentum, an algorithm which uses exponentially smoothed gradients (momentum) to identify layers and weights which reduce the error efficiently. Sparse momentum redistributes pruned weights across layers according to the mean momentum magnitude of each layer. Within a layer, sparse momentum grows weights according to the momentum magnitude of zero-valued weights. We demonstrate state-of-the-art sparse performance on {MNIST}, {CIFAR}-10, and {ImageNet}, decreasing the mean error by a relative 8\%, 15\%, and 6\% compared to other sparse algorithms. Furthermore, we show that our algorithm can reliably find the equivalent of winning lottery tickets from random initialization: Our algorithm finds sparse configurations with 20\% or fewer weights which perform as well, or better than their dense counterparts. Sparse momentum also decreases the training time: It requires a single training run -- no re-training is required -- and increases training speed up to 11.85x. In our analysis, we show that our sparse networks might be able to reach dense performance levels by learning more general features which are useful to a broader range of classes than dense networks.},
	journaltitle = {{arXiv}:1907.04840 [cs, stat]},
	author = {Dettmers, Tim and Zettlemoyer, Luke},
	urldate = {2019-07-11},
	date = {2019-07-10},
	eprinttype = {arxiv},
	eprint = {1907.04840},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@online{suzgun_lstm_2019,
	title = {{LSTM} Networks Can Perform Dynamic Counting},
	url = {/paper/LSTM-Networks-Can-Perform-Dynamic-Counting-Suzgun-Gehrmann/24abc97ac582e50b0798d3c74f383dab958acd95},
	abstract = {In this paper, we systematically assess the ability of standard recurrent networks to perform dynamic counting and to encode hierarchical representations. All the neural models in our experiments are designed to be small-sized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour at test time. Our results demonstrate that the Long Short-Term Memory ({LSTM}) networks can learn to recognize the well-balanced parenthesis language (Dyck-\$1\$) and the shuffles of multiple Dyck-\$1\$ languages, each defined over different parenthesis-pairs, by emulating simple real-time \$k\$-counter machines. To the best of our knowledge, this work is the first study to introduce the shuffle languages to analyze the computational power of neural networks. We also show that a single-layer {LSTM} with only one hidden unit is practically sufficient for recognizing the Dyck-\$1\$ language. However, none of our recurrent networks was able to yield a good performance on the Dyck-\$2\$ language learning task, which requires a model to have a stack-like mechanism for recognition.},
	titleaddon = {undefined},
	author = {Suzgun, Mirac and Gehrmann, Sebastian and Belinkov, Yonatan and Shieber, Stuart M.},
	urldate = {2019-07-10},
	date = {2019},
	langid = {english},
}

@online{kuncoro_scalable_2019,
	title = {Scalable Syntax-Aware Language Models Using Knowledge Distillation},
	url = {/paper/Scalable-Syntax-Aware-Language-Models-Using-Kuncoro-Dyer/43aa04ad9bb7cddc35b947e639b6178f62504070},
	abstract = {Prior work has shown that, on small amounts of training data, syntactic neural language models learn structurally sensitive generalisations more successfully than sequential language models. However, their computational complexity renders scaling difficult, and it remains an open question whether structural biases are still necessary when sequential models have access to ever larger amounts of training data. To answer this question, we introduce an efficient knowledge distillation ({KD}) technique that transfers knowledge from a syntactic language model trained on a small corpus to an {LSTM} language model, hence enabling the {LSTM} to develop a more structurally sensitive representation of the larger training data it learns from. On targeted syntactic evaluations, we find that, while sequential {LSTMs} perform much better than previously reported, our proposed technique substantially improves on this baseline, yielding a new state of the art. Our findings and analysis affirm the importance of structural biases, even in models that learn from large amounts of data.},
	titleaddon = {undefined},
	author = {Kuncoro, Adhiguna and Dyer, Chris and Rimell, Laura and Clark, Stephen and Blunsom, Phil},
	urldate = {2019-07-09},
	date = {2019},
	langid = {english},
}

@online{lee_wide_2019,
	title = {Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
	url = {/paper/Wide-Neural-Networks-of-Any-Depth-Evolve-as-Linear-Lee-Xiao/829908b36a8b5d8a93cffad570a2759c3774e0f4},
	abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
	titleaddon = {undefined},
	author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S. and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
	urldate = {2019-07-09},
	date = {2019},
	langid = {english},
}

@online{clark_what_2019,
	title = {What Does {BERT} Look At? An Analysis of {BERT}'s Attention},
	url = {/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT&#39;s-Clark-Khandelwal/b6f1b2831bb561dddf87a002c70f6c9a85bba3ad},
	shorttitle = {What Does {BERT} Look At?},
	abstract = {Large pre-trained neural networks such as {BERT} have had great recent success in {NLP}, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to {BERT}. {BERT}’s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in {BERT}’s attention.},
	titleaddon = {undefined},
	author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
	urldate = {2019-07-09},
	date = {2019},
	langid = {english},
}

@online{serrano_is_2019,
	title = {Is Attention Interpretable?},
	url = {/paper/Is-Attention-Interpretable-Serrano-Smith/47098c504b9e69759ab5afbc3f042e5a9f3c5bd3},
	abstract = {Attention mechanisms have recently boosted performance on a range of {NLP} tasks. Because attention layers explicitly weight input components\&\#39; representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components\&\#39; overall importance to a model, it is by no means a fail-safe indicator.},
	titleaddon = {undefined},
	author = {Serrano, Sofia and Smith, Noah A.},
	urldate = {2019-07-09},
	date = {2019},
	langid = {english},
}

@online{hahn_theoretical_2019,
	title = {Theoretical Limitations of Self-Attention in Neural Sequence Models},
	url = {/paper/Theoretical-Limitations-of-Self-Attention-in-Neural-Hahn/b3564be8b79f25585acb035f3deaf4ae93c26d8f},
	abstract = {Transformers are emerging as the new workhorse of {NLP}, showing great success across tasks. Unlike {LSTMs}, transformers process input sequences entirely through selfattention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. Our results precisely describe theoretical limitations of the techniques underlying recent advances in {NLP}. Transformers are emerging as the new workhorse of {NLP}; achieving the state-of-the art in tasks such as language modeling, machine translation, and creating pretrained contextualized word embeddings. Eschewing recurrent computations, transformers are entirely based on self-attention, performing their computations in a largely parallel fashion. This enables them to scale to very long sequences (Vaswani et al., 2017; Dai et al., 2019; Child et al., 2019). On the other hand, it has been suggested that this limits their expressiveness, as they cannot process input sequentially (Tran et al., 2018; Dehghani et al., 2018; Shen et al., 2018a; Chen et al., 2018; Hao et al., 2019). One aspect thought to be challenging for sequence models is hierarchical structure and recursion. Hierarchical structure is widely thought to be essential to modeling natural language, in particular its syntax (Everaert et al., 2015). Consequently, many researchers have studied the capability of recurrent neural network models to capture context-free languages (e.g., Kalinke and Lehmann (1998); Gers and Schmidhuber (2001); Grüning (2006); Weiss et al. (2018); Sennhauser and Berwick (2018)) and linguistic phenomena involving hierarchical structure (e.g., Linzen et al. (2016); Gulordava et al. (2018)). Some experimental evidence suggests that transformers might not be as strong as {LSTMs} at modeling hierarchical structure (Tran et al., 2018), though analysis studies have shown that transformer-based models encode a good amount of syntactic knowledge (e.g., Clark et al. (2019); Lin et al. (2019); Tenney et al. (2019)). In this work, we examine these questions from a theoretical perspective, asking whether models entirely based on self-attention are theoretically capable of modeling hierarchical structures involving unbounded recursion. Formally, we study their ability to perform two computations that are thought to be essential to hierarchical structure: First, their ability to correctly close brackets, a basic problem underlying all nonregular contextfree languages and formalized by the {DYCK} language (Chomsky and Schützenberger, 1963). Second, their ability to evaluate iterated negation, which is a basic component of the task of evaluating logical formulas, and which amounts to evaluating the {PARITY} of bitstrings. We show that neither of these problems can be solved by transformers and similar models relying entirely on selfattention, unless the number of layers or parameters increases with the input length. Besides representing basic building blocks of hierarchical structure, these languages also represent large classes of regular and context-free languages, meaning that our results will carry over to wide classes of other formal languages. Our results therefore also yield more generally strong limitations on the ability of self-attention to model large classes of finitestate languages and context-free languages. While theoretical studies have investigated the ar X iv :1 90 6. 06 75 5v 1 [ cs .C L ] 1 6 Ju n 20 19 power of recurrent neural network models in depth (e.g., Siegelman and Sontag (1995); Bengio et al. (1994); Weiss et al. (2018); Miller and Hardt (2018); Merrill (2019)), the theoretical study of self-attention has begun only recently (Pérez et al., 2019). Our study provides the first theoretical results about limitations in the power of transformers. After discussing related work (Section 1), we first introduce self-attention (Section 2) and two fundamental formal languages representing regular and context-free languages (Section 3). We then prove that self-attention cannot model these languages using either hard (Section 4) and soft (Section 5) attention. Finally, we discuss our results (Section 6).},
	titleaddon = {undefined},
	author = {Hahn, Michael},
	urldate = {2019-07-09},
	date = {2019},
	langid = {english},
}

@online{arora_implicit_2019,
	title = {Implicit Regularization in Deep Matrix Factorization},
	url = {/paper/Implicit-Regularization-in-Deep-Matrix-Arora-Cohen/217a85f667778567d7ffc8b56060783caf5803b0},
	abstract = {Efforts to understand the generalization mystery in deep learning have led to the belief that gradient-based optimization induces a form of implicit regularization, a bias towards models of low \&quot;complexity.\&quot; We study the implicit regularization of gradient descent over deep linear neural networks for matrix completion and sensing --- a model referred to as deep matrix factorization. Our first finding, supported by theory and experiments, is that adding depth to a matrix factorization enhances an implicit tendency towards low-rank solutions, oftentimes leading to more accurate recovery. Secondly, we present theoretical and empirical arguments questioning a nascent view by which implicit regularization in matrix factorization can be captured using simple mathematical norms. Our results point to the possibility that the language of standard regularizers may not be rich enough to fully encompass the implicit regularization brought forth by gradient-based optimization.},
	titleaddon = {undefined},
	author = {Arora, Siddhartha and Cohen, Nadav and Hu, Wei and Luo, Yuping},
	urldate = {2019-07-09},
	date = {2019},
	langid = {english},
}

@online{korrel_transcoding_2019,
	title = {Transcoding compositionally: using attention to find more generalizable solutions},
	url = {/paper/Transcoding-compositionally%3A-using-attention-to-Korrel-Hupkes/65afb116efce31c072e91540b49c13c6dec53563},
	shorttitle = {Transcoding compositionally},
	abstract = {While sequence-to-sequence models have shown remarkable generalization power across several natural language tasks, their construct of solutions are argued to be less compositional than human-like generalization. In this paper, we present seq2attn, a new architecture that is specifically designed to exploit attention to find compositional patterns in the input. In seq2attn, the two standard components of an encoder-decoder model are connected via a transcoder, that modulates the information flow between them. We show that seq2attn can successfully generalize, without requiring any additional supervision, on two tasks which are specifically constructed to challenge the compositional skills of neural networks. The solutions found by the model are highly interpretable, allowing easy analysis of both the types of solutions that are found and potential causes for mistakes. We exploit this opportunity to introduce a new paradigm to test compositionality that studies the extent to which a model overgeneralizes when confronted with exceptions. We show that seq2attn exhibits such overgeneralization to a larger degree than a standard sequence-to-sequence model.},
	titleaddon = {undefined},
	author = {Korrel, Kris and Hupkes, Dieuwke and Dankers, Verna and Bruni, Elia},
	urldate = {2019-07-09},
	date = {2019},
	langid = {english},
}

@online{merrill_finding_2019,
	title = {Finding Syntactic Representations in Neural Stacks},
	url = {/paper/Finding-Syntactic-Representations-in-Neural-Stacks-Merrill-Khazan/b878fa20868f36fe7a7a950134b4b1ec955de144},
	abstract = {Neural network architectures have been augmented with differentiable stacks in order to introduce a bias toward learning hierarchy-sensitive regularities. It has, however, proven difficult to assess the degree to which such a bias is effective, as the operation of the differentiable stack is not always interpretable. In this paper, we attempt to detect the presence of latent representations of hierarchical structure through an exploration of the unsupervised learning of constituency structure. Using a technique due to Shen et al. (2018a,b), we extract syntactic trees from the pushing behavior of stack {RNNs} trained on language modeling and classification objectives. We find that our models produce parses that reflect natural language syntactic constituencies, demonstrating that stack {RNNs} do indeed infer linguistically relevant hierarchical structure.},
	titleaddon = {undefined},
	author = {Merrill, William and Khazan, Lenny and Amsel, Noah and Hao, Yiding and Mendelsohn, Simon and Frank, Robert},
	urldate = {2019-07-09},
	date = {2019},
	langid = {english},
}
@online{nye_are_2018,
	title = {Are Efficient Deep Representations Learnable?},
	url = {/paper/Are-Efficient-Deep-Representations-Learnable-Nye-Saxe/152f70fcc1a59e37b41fa4cb49afa70c08837789},
	abstract = {Many theories of deep learning have shown that a deep network can require dramatically fewer resources to represent a given function compared to a shallow network. But a question remains: can these efficient representations be learned using current deep learning techniques? In this work, we test whether standard deep learning methods can in fact find the efficient representations posited by several theories of deep representation. Specifically, we train deep neural networks to learn two simple functions with known efficient solutions: the parity function and the fast Fourier transform. We find that using gradient-based optimization, a deep network does not learn the parity function, unless initialized very close to a hand-coded exact solution. We also find that a deep linear neural network does not learn the fast Fourier transform, even in the best-case scenario of infinite training data, unless the weights are initialized very close to the exact hand-coded solution. Our results suggest that not every element of the class of compositional functions can be learned efficiently by a deep network, and further restrictions are necessary to understand what functions are both efficiently representable and learnable.},
	titleaddon = {undefined},
	author = {Nye, Maxwell and Saxe, Andrew},
	urldate = {2019-07-09},
	date = {2018},
	langid = {english},
}

@online{goldt_dynamics_2019,
	title = {Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
	url = {/paper/Dynamics-of-stochastic-gradient-descent-for-neural-Goldt-Advani/c066c129d906c91bd81ca3acc0b687ba284f10f4},
	abstract = {Deep neural networks achieve stellar generalisation even when they have enough parameters to easily fit all their training data. We study the dynamics and the performance of two-layer neural networks in the teacher-student setup, where one network, the student, is trained on data generated by another network, called the teacher, using stochastic gradient descent ({SGD}). We show how the dynamics of {SGD} is captured by a set of differential equations and prove that this description is asymptotically exact in the limit of large inputs. Using this framework, we calculate the final generalisation error of student networks that have more parameters than their teachers. We find that the final generalisation error of the student increases with network size when training only the first layer, but stays constant or even decreases with size when training both layers. We show that these different behaviours have their root in the different solutions {SGD} finds for different activation functions. Our results indicate that achieving good generalisation in neural networks goes beyond the properties of {SGD} alone and depends on the interplay of at least the algorithm, the model architecture, and the data set.},
	titleaddon = {undefined},
	author = {Goldt, Sebastian and Advani, Madhu S. and Saxe, Andrew M. and Krzakala, Florent and Zdeborová, Lenka},
	urldate = {2019-07-09},
	date = {2019},
	langid = {english},
}

@inproceedings{guan_towards_2019,
	title = {Towards a Deep and Unified Understanding of Deep Neural Models in {NLP}},
	url = {http://proceedings.mlr.press/v97/guan19a.html},
	abstract = {We define a unified information-based measure to provide quantitative explanations on how intermediate layers of deep Natural Language Processing ({NLP}) models leverage information of input words. O...},
	eventtitle = {International Conference on Machine Learning},
	pages = {2454--2463},
	booktitle = {International Conference on Machine Learning},
	author = {Guan, Chaoyu and Wang, Xiting and Zhang, Quanshi and Chen, Runjin and He, Di and Xie, Xing},
	urldate = {2019-07-08},
	date = {2019-05-24},
	langid = {english},
}

@article{guo_toward_2019,
	title = {Toward Fairness in {AI} for People with Disabilities: A Research Roadmap},
	url = {http://arxiv.org/abs/1907.02227},
	shorttitle = {Toward Fairness in {AI} for People with Disabilities},
	abstract = {{AI} technologies have the potential to dramatically impact the lives of people with disabilities ({PWD}). Indeed, improving the lives of {PWD} is a motivator for many state-of-the-art {AI} systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed {AI} systems may not work properly for {PWD}, or worse, may actively discriminate against them. These considerations regarding fairness in {AI} for {PWD} have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several {AI} technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of {AI} might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.},
	journaltitle = {{arXiv}:1907.02227 [cs]},
	author = {Guo, Anhong and Kamar, Ece and Vaughan, Jennifer Wortman and Wallach, Hanna and Morris, Meredith Ringel},
	urldate = {2019-07-06},
	date = {2019-07-04},
	eprinttype = {arxiv},
	eprint = {1907.02227},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, I.2.m, K.4.1, K.4.2},
}

@article{michel_are_2019,
	title = {Are Sixteen Heads Really Better than One?},
	url = {http://arxiv.org/abs/1905.10650},
	abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art {NLP} models such as Transformer-based {MT} models and {BERT}. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.},
	journaltitle = {{arXiv}:1905.10650 [cs]},
	author = {Michel, Paul and Levy, Omer and Neubig, Graham},
	urldate = {2019-07-05},
	date = {2019-05-25},
	eprinttype = {arxiv},
	eprint = {1905.10650},
	keywords = {Computer Science - Computation and Language},
}

@online{luo_improving_2019,
	title = {Improving Neural Language Models by Segmenting, Attending, and Predicting the Future},
	url = {/paper/Improving-Neural-Language-Models-by-Segmenting%2C-and-Luo-Jiang/b282b42dc4df3bba0d8b765d3c6c3b1759b9c925},
	abstract = {Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.},
	titleaddon = {undefined},
	author = {Luo, Hongyin and Jiang, Lan and Belinkov, Yonatan and Glass, James},
	urldate = {2019-07-04},
	date = {2019},
	langid = {english},
}

@online{durrani_one_2019,
	title = {One Size Does Not Fit All: Comparing {NMT} Representations of Different Granularities},
	url = {/paper/One-Size-Does-Not-Fit-All%3A-Comparing-NMT-of-Durrani-Dalvi/e9d5a28409e6b929070089b27321b2b5fe2e3248},
	shorttitle = {One Size Does Not Fit All},
	abstract = {Semantic Scholar extracted view of \&quot;One Size Does Not Fit All: Comparing {NMT} Representations of Different Granularities\&quot; by Nadir Durrani et al.},
	titleaddon = {undefined},
	author = {Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Belinkov, Yonatan and Nakov, Preslav},
	urldate = {2019-07-04},
	date = {2019},
	langid = {english},
}

@inproceedings{mhaskar_when_2017,
	title = {When and why are deep networks better than shallow ones?},
	booktitle = {Thirty-First {AAAI} Conference on Artificial Intelligence},
	author = {Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
	date = {2017},
}

@inproceedings{koh_understanding_2017,
	title = {Understanding black-box predictions via influence functions},
	pages = {1885--1894},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	publisher = {{JMLR}. org},
	author = {Koh, Pang Wei and Liang, Percy},
	date = {2017},
}

@article{mangalam_deep_2019,
	title = {Do deep neural networks learn shallow learnable examples first?},
	url = {https://openreview.net/forum?id=HkxHv4rn24},
	abstract = {In this paper, we empirically investigate the training journey of deep neural networks relative to fully trained shallow machine learning models. We observe that the deep neural networks ({DNNs})...},
	author = {Mangalam, Karttikeya and Prabhu, Vinay Uday},
	urldate = {2019-07-03},
	date = {2019-05-17},
}

@article{tannen_new_1981,
	title = {New York Jewish conversational style},
	volume = {1981},
	pages = {133--150},
	number = {30},
	journaltitle = {International Journal of the sociology of language},
	author = {Tannen, Deborah},
	date = {1981},
}

@article{szabo_case_2012,
	title = {The case for compositionality},
	url = {https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199541072.001.0001/oxfordhb-9780199541072-e-3},
	doi = {10.1093/oxfordhb/9780199541072.013.0003},
	abstract = {This article presents three more-or-less-traditional considerations for compositionality. The first is that the usual statement of the compositionality principle is massively ambiguous. One of the eight available readings rules out all sources of multiplicity in meaning in complex expressions besides the lexicon and the syntax. Others are more permissive—how much more is not always clear. The second claim is that traditional considerations in favour of compositionality are less powerful than is often assumed. Compositionality is best construed as an empirical hypothesis on meanings expressed in natural languages. Finally, the third claim is that, even if compositionality is true, most of the debates in philosophy, linguistics, and psychology surrounding compositionality will remain open. These debates tend to be about significantly stronger theses.},
	journaltitle = {The Oxford Handbook of Compositionality},
	author = {Szabó, Zoltán Gendler},
	urldate = {2019-07-03},
	date = {2012-02-09},
	langid = {english},
}

@article{liska_memorize_2018,
	title = {Memorize or generalize? Searching for a compositional {RNN} in a haystack},
	url = {http://arxiv.org/abs/1802.06467},
	shorttitle = {Memorize or generalize?},
	abstract = {Neural networks are very powerful learning systems, but they do not readily generalize from one task to the other. This is partly due to the fact that they do not learn in a compositional way, that is, by discovering skills that are shared by different tasks, and recombining them to solve new problems. In this paper, we explore the compositional generalization capabilities of recurrent neural networks ({RNNs}). We first propose the lookup table composition domain as a simple setup to test compositional behaviour and show that it is theoretically possible for a standard {RNN} to learn to behave compositionally in this domain when trained with standard gradient descent and provided with additional supervision. We then remove this additional supervision and perform a search over a large number of model initializations to investigate the proportion of {RNNs} that can still converge to a compositional solution. We discover that a small but non-negligible proportion of {RNNs} do reach partial compositional solutions even without special architectural constraints. This suggests that a combination of gradient descent and evolutionary strategies directly favouring the minority models that developed more compositional approaches might suffice to lead standard {RNNs} towards compositional solutions.},
	journaltitle = {{arXiv}:1802.06467 [cs]},
	author = {Liška, Adam and Kruszewski, Germán and Baroni, Marco},
	urldate = {2019-07-03},
	date = {2018-02-18},
	eprinttype = {arxiv},
	eprint = {1802.06467},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{alishahi_encoding_2017,
	title = {Encoding of phonology in a recurrent neural model of grounded speech},
	url = {http://arxiv.org/abs/1706.03815},
	doi = {10.18653/v1/K17-1037},
	abstract = {We study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number of analyses on how information about individual phonemes is encoded in the {MFCC} features extracted from the speech signal, and the activations of the layers of the model. Via experiments with phoneme decoding and phoneme discrimination we show that phoneme representations are most salient in the lower layers of the model, where low-level signals are processed at a fine-grained level, although a large amount of phonological information is retain at the top recurrent layer. We further find out that the attention mechanism following the top recurrent layer significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy. Moreover, a hierarchical clustering of phoneme representations learned by the network shows an organizational structure of phonemes similar to those proposed in linguistics.},
	pages = {368--378},
	journaltitle = {Proceedings of the 21st Conference on Computational Natural Language           Learning ({CoNLL} 2017)},
	author = {Alishahi, Afra and Barking, Marie and Chrupała, Grzegorz},
	urldate = {2019-07-03},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1706.03815},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound},
}

@article{linzen_assessing_2016,
	title = {Assessing the Ability of {LSTMs} to Learn Syntax-Sensitive Dependencies},
	url = {http://arxiv.org/abs/1611.01368},
	abstract = {The success of long short-term memory ({LSTM}) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by {LSTMs}, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the {LSTM} achieved very high overall accuracy (less than 1\% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that {LSTMs} can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.},
	journaltitle = {{arXiv}:1611.01368 [cs]},
	author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	urldate = {2018-05-10},
	date = {2016-11-04},
	eprinttype = {arxiv},
	eprint = {1611.01368},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{shi_does_2016,
	title = {Does String-Based Neural {MT} Learn Source Syntax?},
	booktitle = {{EMNLP}},
	author = {Shi, Xing and Padhi, Inkit and Knight, Kevin},
	date = {2016},
}

@article{wieting_no_2019,
	title = {No Training Required: Exploring Random Encoders for Sentence Classification},
	url = {http://arxiv.org/abs/1901.10444},
	shorttitle = {No Training Required},
	abstract = {We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.},
	journaltitle = {{arXiv}:1901.10444 [cs]},
	author = {Wieting, John and Kiela, Douwe},
	urldate = {2019-02-28},
	date = {2019-01-29},
	eprinttype = {arxiv},
	eprint = {1901.10444},
	keywords = {Computer Science - Computation and Language},
}

@article{baroni_linguistic_2019,
	title = {Linguistic generalization and compositionality in modern artificial neural networks},
	url = {http://arxiv.org/abs/1904.00157},
	abstract = {In the last decade, deep artificial neural networks have achieved astounding performance in many natural language processing tasks. Given the high productivity of language, these models must possess effective generalization abilities. It is widely assumed that humans handle linguistic productivity by means of algebraic compositional rules: Are deep networks similarly compositional? After reviewing the main innovations characterizing current deep language processing networks, I discuss a set of studies suggesting that deep networks are capable of subtle grammar-dependent generalizations, but also that they do not rely on systematic compositional rules. I argue that the intriguing behaviour of these devices (still awaiting a full understanding) should be of interest to linguists and cognitive scientists, as it offers a new perspective on possible computational strategies to deal with linguistic productivity beyond rule-based compositionality, and it might lead to new insights into the less systematic generalization patterns that also appear in natural language.},
	journaltitle = {{arXiv}:1904.00157 [cs]},
	author = {Baroni, Marco},
	urldate = {2019-04-02},
	date = {2019-03-30},
	eprinttype = {arxiv},
	eprint = {1904.00157},
	keywords = {Computer Science - Computation and Language},
}

@online{kim_unsupervised_2019,
	title = {Unsupervised Recurrent Neural Network Grammars},
	url = {/paper/Unsupervised-Recurrent-Neural-Network-Grammars-Kim-Rush/5cf31556522a127bc098d3d69b63f3e0069908e0},
	abstract = {Recurrent neural network grammars ({RNNG}) are generative models of language which jointly model syntax and surface structure by incrementally generating a syntax tree and sentence in a top-down, left-to-right order. Supervised {RNNGs} achieve strong language modeling and parsing performance, but require an annotated corpus of parse trees. In this work, we experiment with unsupervised learning of {RNNGs}. Since directly marginalizing over the space of latent trees is intractable, we instead apply amortized variational inference. To maximize the evidence lower bound, we develop an inference network parameterized as a neural {CRF} constituency parser. On language modeling, unsupervised {RNNGs} perform as well their supervised counterparts on benchmarks in English and Chinese. On constituency grammar induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms.},
	author = {Kim, Yoon and Rush, Alexander M. and Yu, Lei and Kuncoro, Adhiguna and Dyer, Chris and Melis, G'abor},
	urldate = {2019-04-16},
	date = {2019},
	langid = {english},
}

@article{kornblith_similarity_2019,
	title = {Similarity of Neural Network Representations Revisited},
	journaltitle = {{arXiv} preprint {arXiv}:1905.00414},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	date = {2019},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{zhang_are_2019,
	title = {Are All Layers Created Equal?},
	url = {http://arxiv.org/abs/1902.01996},
	abstract = {Understanding learning and generalization of deep architectures has been a major research objective in the recent years with notable theoretical progress. A main focal point of generalization studies stems from the success of excessively large networks which defy the classical wisdom of uniform convergence and learnability. We study empirically the layer-wise functional structure of over-parameterized deep models. We provide evidence for the heterogeneous characteristic of layers. To do so, we introduce the notion of (post training) re-initialization and re-randomization robustness. We show that layers can be categorized into either "robust" or "critical". In contrast to critical layers, resetting the robust layers to their initial value has no negative consequence, and in many cases they barely change throughout training. Our study provides further evidence that mere parameter counting or norm accounting is too coarse in studying generalization of deep models.},
	journaltitle = {{arXiv}:1902.01996 [cs, stat]},
	author = {Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
	urldate = {2019-02-11},
	date = {2019-02-05},
	eprinttype = {arxiv},
	eprint = {1902.01996},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{saxe_mathematical_2018,
	title = {A mathematical theory of semantic development in deep neural networks},
	url = {http://arxiv.org/abs/1810.10531},
	abstract = {An extensive body of empirical research has revealed remarkable regularities in the acquisition, organization, deployment, and neural representation of human semantic knowledge, thereby raising a fundamental conceptual question: what are the theoretical principles governing the ability of neural networks to acquire, organize, and deploy abstract knowledge by integrating across many individual experiences? We address this question by mathematically analyzing the nonlinear dynamics of learning in deep linear networks. We find exact solutions to this learning dynamics that yield a conceptual explanation for the prevalence of many disparate phenomena in semantic cognition, including the hierarchical differentiation of concepts through rapid developmental transitions, the ubiquity of semantic illusions between such transitions, the emergence of item typicality and category coherence as factors controlling the speed of semantic processing, changing patterns of inductive projection over development, and the conservation of semantic similarity in neural representations across species. Thus, surprisingly, our simple neural model qualitatively recapitulates many diverse regularities underlying semantic development, while providing analytic insight into how the statistical structure of an environment can interact with nonlinear deep learning dynamics to give rise to these regularities.},
	journaltitle = {{arXiv}:1810.10531 [cs, q-bio, stat]},
	author = {Saxe, Andrew M. and {McClelland}, James L. and Ganguli, Surya},
	urldate = {2018-12-18},
	date = {2018-10-23},
	eprinttype = {arxiv},
	eprint = {1810.10531},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{ba_layer_2016,
	title = {Layer Normalization},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	journaltitle = {{arXiv}:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	urldate = {2016-07-22},
	date = {2016-07-21},
	eprinttype = {arxiv},
	eprint = {1607.06450},
	keywords = {Computer Science - Learning, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kanuparthi_h-detach:_2018,
	title = {h-detach: Modifying the {LSTM} Gradient Towards Better Optimization},
	url = {https://openreview.net/forum?id=ryf7ioRqFX},
	shorttitle = {h-detach},
	abstract = {Recurrent neural networks are known for their notorious exploding and vanishing gradient problem ({EVGP}). This problem becomes more evident in tasks where the information needed to correctly solve...},
	author = {Kanuparthi, Bhargav and Arpit, Devansh and Kerg, Giancarlo and Ke, Nan Rosemary and Mitliagkas, Ioannis and Bengio, Yoshua},
	urldate = {2019-07-02},
	date = {2018-09-27},
}

@article{ji_gradient_2018,
	title = {Gradient descent aligns the layers of deep linear networks},
	url = {https://openreview.net/forum?id=HJflg30qKX},
	abstract = {This paper establishes risk convergence and asymptotic weight matrix alignment --- a form of implicit regularization --- of gradient flow and gradient descent when applied to deep linear networks...},
	author = {Ji, Ziwei and Telgarsky, Matus},
	urldate = {2019-07-02},
	date = {2018-09-27},
}

@article{du_gradient_2018,
	title = {Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
	url = {https://openreview.net/forum?id=S1eK3i09YQ},
	abstract = {One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is...},
	author = {Du, Simon S. and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
	urldate = {2019-07-02},
	date = {2018-09-27},
}

@article{stich_local_2018,
	title = {Local {SGD} Converges Fast and Communicates Little},
	url = {https://openreview.net/forum?id=S1g2JnRcFX},
	abstract = {Mini-batch stochastic gradient descent ({SGD}) is state of the art in large scale distributed training. The scheme can reach a linear speed-up with respect to the number of workers, but this is...},
	author = {Stich, Sebastian U.},
	urldate = {2019-07-02},
	date = {2018-09-27},
}

@article{choromanska_loss_2014,
	title = {The Loss Surfaces of Multilayer Networks},
	url = {http://arxiv.org/abs/1412.0233},
	abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and {SGD} converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
	journaltitle = {{arXiv}:1412.0233 [cs]},
	author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and {LeCun}, Yann},
	urldate = {2019-07-02},
	date = {2014-11-30},
	eprinttype = {arxiv},
	eprint = {1412.0233},
	keywords = {Computer Science - Machine Learning},
}

@online{baan_realization_2019,
	title = {On the Realization of Compositionality in Neural Networks},
	url = {/paper/On-the-Realization-of-Compositionality-in-Neural-Baan-Leible/02ccf39600b495932af1ae18d95110be07a1ed2e},
	abstract = {We present a detailed comparison of two types of sequence to sequence models trained to conduct a compositional task. The models are architecturally identical at inference time, but differ in the way that they are trained: our baseline model is trained with a task-success signal only, while the other model receives additional supervision on its attention mechanism (Attentive Guidance), which has shown to be an effective method for encouraging more compositional solutions (Hupkes et al.,2019). We first confirm that the models with attentive guidance indeed infer more compositional solutions than the baseline, by training them on the lookup table task presented by Li{\textbackslash}v\{s\}ka et al. (2019). We then do an in-depth analysis of the structural differences between the two model types, focusing in particular on the organisation of the parameter space and the hidden layer activations and find noticeable differences in both these aspects. Guided networks focus more on the components of the input rather than the sequence as a whole and develop small functional groups of neurons with specific purposes that use their gates more selectively. Results from parameter heat maps, component swapping and graph analysis also indicate that guided networks exhibit a more modular structure with a small number of specialized, strongly connected neurons.},
	titleaddon = {undefined},
	author = {Baan, J. and Leible, Jana and Nikolaus, Mitja and Rau, Domenico and Ulmer, Dennis and Baumgartner, Tim R. and Hupkes, Dieuwke and Bruni, Elia},
	urldate = {2019-07-02},
	date = {2019},
	langid = {english},
}

@article{ansuini_intrinsic_2019,
	title = {Intrinsic dimension of data representations in deep neural networks},
	url = {http://arxiv.org/abs/1905.12784},
	abstract = {Deep neural networks progressively transform their inputs across multiple processing layers. What are the geometrical properties of the representations learned by these networks? Here we study the intrinsic dimensionality ({ID}) of data-representations, i.e. the minimal number of parameters needed to describe a representation. We find that, in a trained network, the {ID} is orders of magnitude smaller than the number of units in each layer. Across layers, the {ID} first increases and then progressively decreases in the final layers. Remarkably, the {ID} of the last hidden layer predicts classification accuracy on the test set. These results can neither be found by linear dimensionality estimates (e.g., with principal component analysis), nor in representations that had been artificially linearized. They are neither found in untrained networks, nor in networks that are trained on randomized labels. This suggests that neural networks that can generalize are those that transform the data into low-dimensional, but not necessarily flat manifolds.},
	journaltitle = {{arXiv}:1905.12784 [cs, stat]},
	author = {Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H. and Zoccolan, Davide},
	urldate = {2019-07-02},
	date = {2019-05-29},
	eprinttype = {arxiv},
	eprint = {1905.12784},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{hacohen_all_2019,
	title = {All Neural Networks are Created Equal},
	url = {/paper/All-Neural-Networks-are-Created-Equal-Hacohen-Weinshall/d3fb841cc558a7e34b5acd318a9357fd41f6310f},
	abstract = {One of the unresolved questions in the context of deep learning is the triumph of {GD} based optimization, which is guaranteed to converge to one of many local minima. To shed light on the nature of the solutions that are thus being discovered, we investigate the ensemble of solutions reached by the same network architecture, with different random initialization of weights and random mini-batches. Surprisingly, we observe that these solutions are in fact very similar - more often than not, each train and test example is either classified correctly by all the networks, or by none at all. Moreover, all the networks seem to share the same learning dynamics, whereby initially the same train and test examples are incorporated into the learnt model, followed by other examples which are learnt in roughly the same order. When different neural network architectures are compared, the same learning dynamics is observed even when one architecture is significantly stronger than the other and achieves higher accuracy. Finally, when investigating other methods that involve the gradual refinement of a solution, such as boosting, once again we see the same learning pattern. In all cases, it appears as if all the classifiers start by learning to classify correctly the same train and test examples, while the more powerful classifiers continue to learn to classify correctly additional examples. These results are incredibly robust, observed for a large variety of architectures, hyperparameters and different datasets of images. Thus we observe that different classification solutions may be discovered by different means, but typically they evolve in roughly the same manner and demonstrate a similar success and failure behavior. For a given dataset, such behavior seems to be strongly correlated with effective generalization, while the induced ranking of examples may reflect inherent structure in the data.},
	titleaddon = {undefined},
	author = {Hacohen, Guy and Weinshall, Daphna},
	urldate = {2019-07-01},
	date = {2019},
	langid = {english},
}

@online{noauthor_correlating_nodate,
	title = {Correlating neural and symbolic representations of language {\textbar} {OpenReview}},
	url = {https://openreview.net/forum?id=ryx35Ehi84},
	urldate = {2019-07-01},
}

@online{tian_luck_2019,
	title = {Luck Matters: Understanding Training Dynamics of Deep {ReLU} Networks},
	url = {/paper/Luck-Matters%3A-Understanding-Training-Dynamics-of-Tian-Jiang/8a002078c0d272744684b92433512f66f1911e79},
	shorttitle = {Luck Matters},
	abstract = {We analyze the dynamics of training deep {ReLU} networks and their implications on generalization capability. Using a teacher-student setting, we discovered a novel relationship between the gradient received by hidden student nodes and the activations of teacher nodes for deep {ReLU} networks. With this relationship and the assumption of small overlapping teacher node activations, we prove that (1) student nodes whose weights are initialized to be close to teacher nodes converge to them at a faster rate, and (2) in over-parameterized regimes and 2-layer case, while a small set of lucky nodes do converge to the teacher nodes, the fan-out weights of other nodes converge to zero. This framework provides insight into multiple puzzling phenomena in deep learning like over-parameterization, implicit regularization, lottery tickets, etc. We verify our assumption by showing that the majority of {BatchNorm} biases of pre-trained {VGG}11/16 models are negative. Experiments on (1) random deep teacher networks with Gaussian inputs, (2) teacher network pre-trained on {CIFAR}-10 and (3) extensive ablation studies validate our multiple theoretical predictions.},
	titleaddon = {undefined},
	author = {Tian, Yuandong and Jiang, Tina and Gong, Qucheng and Morcos, Ari},
	urldate = {2019-07-01},
	date = {2019},
	langid = {english},
}
@online{hardt_contrastive_nodate,
	title = {Contrastive Unsupervised Learning of Semantic Representations\&\#58; A Theoretical Framework},
	url = {http://offconvex.github.io/2019/03/19/CURL/},
	abstract = {Algorithms off the convex path.},
	titleaddon = {Off the convex path},
	author = {Hardt, Moritz},
	urldate = {2019-07-01},
}

@inproceedings{sculley_machine_2014,
	title = {Machine Learning: The High Interest Credit Card of Technical Debt},
	shorttitle = {Machine Learning},
	booktitle = {{SE}4ML: Software Engineering for Machine Learning ({NIPS} 2014 Workshop)},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
	date = {2014},
}

@article{lipton_troubling_2018,
	title = {Troubling Trends in Machine Learning Scholarship},
	url = {http://arxiv.org/abs/1807.03341},
	abstract = {Collectively, machine learning ({ML}) researchers are engaged in the creation and dissemination of knowledge about data-driven algorithms. In a given paper, researchers might aspire to any subset of the following goals, among others: to theoretically characterize what is learnable, to obtain understanding through empirically rigorous experiments, or to build a working system that has high predictive accuracy. While determining which knowledge warrants inquiry may be subjective, once the topic is fixed, papers are most valuable to the community when they act in service of the reader, creating foundational knowledge and communicating as clearly as possible. Recent progress in machine learning comes despite frequent departures from these ideals. In this paper, we focus on the following four patterns that appear to us to be trending in {ML} scholarship: (i) failure to distinguish between explanation and speculation; (ii) failure to identify the sources of empirical gains, e.g., emphasizing unnecessary modifications to neural architectures when gains actually stem from hyper-parameter tuning; (iii) mathiness: the use of mathematics that obfuscates or impresses rather than clarifies, e.g., by confusing technical and non-technical concepts; and (iv) misuse of language, e.g., by choosing terms of art with colloquial connotations or by overloading established technical terms. While the causes behind these patterns are uncertain, possibilities include the rapid expansion of the community, the consequent thinness of the reviewer pool, and the often-misaligned incentives between scholarship and short-term measures of success (e.g., bibliometrics, attention, and entrepreneurial opportunity). While each pattern offers a corresponding remedy (don't do it), we also discuss some speculative suggestions for how the community might combat these trends.},
	journaltitle = {{arXiv}:1807.03341 [cs, stat]},
	author = {Lipton, Zachary C. and Steinhardt, Jacob},
	urldate = {2019-07-01},
	date = {2018-07-09},
	eprinttype = {arxiv},
	eprint = {1807.03341},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{hardt_is_nodate,
	title = {Is Optimization a Sufficient Language for Understanding Deep Learning?},
	url = {http://offconvex.github.io/2019/06/03/trajectories/},
	abstract = {Algorithms off the convex path.},
	titleaddon = {Off the convex path},
	author = {Hardt, Moritz},
	urldate = {2019-07-01},
}

@article{arora_exact_2019,
	title = {On Exact Computation with an Infinitely Wide Neural Net},
	url = {http://arxiv.org/abs/1904.11955},
	abstract = {How well does a classic deep net architecture like {AlexNet} or {VGG}19 classify on a standard dataset such as {CIFAR}-10 when its "width" --- namely, number of channels in convolutional layers, and number of nodes in fully-connected internal layers --- is allowed to increase to infinity? Such questions have come to the forefront in the quest to theoretically understand deep learning and its mysteries about optimization and generalization. They also connect deep learning to notions such as Gaussian processes and kernels. A recent paper [Jacot et al., 2018] introduced the Neural Tangent Kernel ({NTK}) which captures the behavior of fully-connected deep nets in the infinite width limit trained by gradient descent; this object was implicit in some other recent papers. A subsequent paper [Lee et al., 2019] gave heuristic Monte Carlo methods to estimate the {NTK} and its extension, Convolutional Neural Tangent Kernel ({CNTK}) and used this to try to understand the limiting behavior on datasets like {CIFAR}-10. The current paper gives the first efficient exact algorithm (based upon dynamic programming) for computing {CNTK} as well as an efficient {GPU} implementation of this algorithm. This results in a significant new benchmark for performance of a pure kernel-based method on {CIFAR}-10, being 10\% higher than the methods reported in [Novak et al., 2019], and only 5\% lower than the performance of the corresponding finite deep net architecture (once batch normalization etc. are turned off). We give the first non-asymptotic proof showing that a fully-trained sufficiently wide net is indeed equivalent to the kernel regression predictor using {NTK}. Our experiments also demonstrate that earlier Monte Carlo approximation can degrade the performance significantly, thus highlighting the power of our exact kernel computation, which we have applied even to the full {CIFAR}-10 dataset and 20-layer nets.},
	journaltitle = {{arXiv}:1904.11955 [cs, stat]},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
	urldate = {2019-07-01},
	date = {2019-04-26},
	eprinttype = {arxiv},
	eprint = {1904.11955},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{shen_ordered_2018,
	title = {Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks},
	url = {https://openreview.net/forum?id=B1l6qiR5F7},
	shorttitle = {Ordered Neurons},
	abstract = {Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are...},
	author = {Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron},
	urldate = {2019-06-28},
	date = {2018-09-27},
}

@article{thornton_childrens_2016,
	title = {Children’s Acquisition of Syntactic Knowledge},
	url = {https://oxfordre.com/view/10.1093/acrefore/9780199384655.001.0001/acrefore-9780199384655-e-72},
	doi = {10.1093/acrefore/9780199384655.013.72},
	abstract = {Children’s acquisition of language is an amazing feat. Children master the syntax, the sentence structure of their language, through exposure and interaction with caregivers and others but, notably, with no formal tuition. How children come to be in command of the syntax of their language has been a topic of vigorous debate since Chomsky argued against Skinner’s claim that language is ‘verbal behavior.’ Chomsky argued that knowledge of language cannot be learned through experience alone but is guided by a genetic component. This language component, known as ‘Universal Grammar,’ is composed of abstract linguistic knowledge and a computational system that is special to language. The computational mechanisms of Universal Grammar give even young children the capacity to form hierarchical syntactic representations for the sentences they hear and produce. The abstract knowledge of language guides children’s hypotheses as they interact with the language input in their environment, ensuring they progress toward the adult grammar. An alternative school of thought denies the existence of a dedicated language component, arguing that knowledge of syntax is learned entirely through interactions with speakers of the language. Such ‘usage-based’ linguistic theories assume that language learning employs the same learning mechanisms that are used by other cognitive systems. Usage-based accounts of language development view children’s earliest productions as rote-learned phrases that lack internal structure. Knowledge of linguistic structure emerges gradually and in a piecemeal fashion, with frequency playing a large role in the order of emergence for different syntactic structures.},
	journaltitle = {Oxford Research Encyclopedia of Linguistics},
	author = {Thornton, Rosalind},
	urldate = {2019-06-18},
	date = {2016-12-22},
	langid = {english},
}

@inproceedings{phuong_towards_2019,
	title = {Towards Understanding Knowledge Distillation},
	url = {http://proceedings.mlr.press/v97/phuong19a.html},
	abstract = {Knowledge distillation, i.e., one classifier being trained on the outputs of another classifier, is an empirically very successful technique for knowledge transfer between classifiers. It has even ...},
	eventtitle = {International Conference on Machine Learning},
	pages = {5142--5151},
	booktitle = {International Conference on Machine Learning},
	author = {Phuong, Mary and Lampert, Christoph},
	urldate = {2019-06-17},
	date = {2019-05-24},
	langid = {english},
}

@article{forcione-lambert_investigation_2019,
	title = {An Investigation of Memory in Recurrent Neural Networks},
	url = {https://openreview.net/forum?id=SJevPNShnV},
	abstract = {We investigate the learned dynamical landscape of a recurrent neural network solving a simple task requiring the interaction of two memory mechanisms: long- and short-term. Our results show that...},
	author = {Forcione-Lambert, Aude and Wolf, Guy and Lajoie, Guillaume},
	urldate = {2019-06-09},
	date = {2019-05-17},
}

@article{maheswaranathan_line_2019,
	title = {Line attractor dynamics in recurrent networks for sentiment classiﬁcation},
	url = {https://openreview.net/forum?id=SJlKDVS2hV},
	abstract = {Recurrent  neural  networks  ({RNNs})  are  a  powerful tool for modeling sequential data. Despite their widespread usage, understanding how {RNNs} solve complex problems remains elusive.  Here, we...},
	author = {Maheswaranathan, Niru and Williams, Alex H. and Golub, Matthew D. and Ganguli, Surya and Sussillo, David},
	urldate = {2019-06-09},
	date = {2019-05-17},
}

@article{liu_bad_2019,
	title = {Bad Global Minima Exist and {SGD} Can Reach Them},
	url = {https://openreview.net/forum?id=SJxooJh5TE},
	abstract = {Several recent works have aimed to explain why severely overparameterized models, generalize well when trained by Stochastic Gradient Descent ({SGD}). The emergent consensus explanation has two...},
	author = {Liu, Shengchao and Papailiopoulos, Dimitris and Achlioptas, Dimitris},
	urldate = {2019-06-09},
	date = {2019-05-28},
}

@article{feng_misleading_2019,
	title = {Misleading Failures of Partial-input Baselines},
	url = {http://arxiv.org/abs/1905.05778},
	abstract = {Recent work establishes dataset difficulty and removes annotation artifacts via partial-input baselines (e.g., hypothesis-only or image-only models). While the success of a partial-input baseline indicates a dataset is cheatable, our work cautions the converse is not necessarily true. Using artificial datasets, we illustrate how the failure of a partial-input baseline might shadow more trivial patterns that are only visible in the full input. We also identify such artifacts in real natural language inference datasets. Our work provides an alternative view on the use of partial-input baselines in future dataset creation.},
	journaltitle = {{arXiv}:1905.05778 [cs, stat]},
	author = {Feng, Shi and Wallace, Eric and Boyd-Graber, Jordan},
	urldate = {2019-06-01},
	date = {2019-05-14},
	eprinttype = {arxiv},
	eprint = {1905.05778},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{pinter_character_2019,
	title = {Character Eyes: Seeing Language through Character-Level Taggers},
	url = {http://arxiv.org/abs/1903.05041},
	shorttitle = {Character Eyes},
	abstract = {Character-level models have been used extensively in recent years in {NLP} tasks as both supplements and replacements for closed-vocabulary token-level word representations. In one popular architecture, character-level {LSTMs} are used to feed token representations into a sequence tagger predicting token-level annotations such as part-of-speech ({POS}) tags. In this work, we examine the behavior of {POS} taggers across languages from the perspective of individual hidden units within the character {LSTM}. We aggregate the behavior of these units into language-level metrics which quantify the challenges that taggers face on languages with different morphological properties, and identify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer. In a comparative experiment, we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages.},
	journaltitle = {{arXiv}:1903.05041 [cs]},
	author = {Pinter, Yuval and Marone, Marc and Eisenstein, Jacob},
	urldate = {2019-06-01},
	date = {2019-03-12},
	eprinttype = {arxiv},
	eprint = {1903.05041},
	keywords = {Computer Science - Computation and Language},
}

@article{wilcox_what_2019,
	title = {What Syntactic Structures block Dependencies in {RNN} Language Models?},
	journaltitle = {{arXiv} preprint {arXiv}:1905.10431},
	author = {Wilcox, Ethan and Levy, Roger and Futrell, Richard},
	date = {2019},
}

@article{saxe_information_2018,
	title = {On the Information Bottleneck Theory of Deep Learning},
	url = {https://openreview.net/forum?id=ry_WPG-A-},
	abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck ({IB})...},
	author = {Saxe, Andrew Michael and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan Daniel and Cox, David Daniel},
	urldate = {2019-05-22},
	date = {2018-02-15},
}

@article{tenney_bert_2019,
	title = {{BERT} Rediscovers the Classical {NLP} Pipeline},
	url = {http://arxiv.org/abs/1905.05950},
	abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many {NLP} tasks. We focus on one such model, {BERT}, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional {NLP} pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: {POS} tagging, parsing, {NER}, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
	journaltitle = {{arXiv}:1905.05950 [cs]},
	author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
	urldate = {2019-05-16},
	date = {2019-05-15},
	eprinttype = {arxiv},
	eprint = {1905.05950},
	keywords = {Computer Science - Computation and Language},
}

@article{hurley_comparing_2008,
	title = {Comparing Measures of Sparsity},
	url = {http://arxiv.org/abs/0811.4706},
	abstract = {Sparsity of representations of signals has been shown to be a key concept of fundamental importance in fields such as blind source separation, compression, sampling and signal analysis. The aim of this paper is to compare several commonlyused sparsity measures based on intuitive attributes. Intuitively, a sparse representation is one in which a small number of coefficients contain a large proportion of the energy. In this paper six properties are discussed: (Robin Hood, Scaling, Rising Tide, Cloning, Bill Gates and Babies), each of which a sparsity measure should have. The main contributions of this paper are the proofs and the associated summary table which classify commonly-used sparsity measures based on whether or not they satisfy these six propositions and the corresponding proofs. Only one of these measures satisfies all six: The Gini Index. measures based on whether or not they satisfy these six propositions and the corresponding proofs. Only one of these measures satisfies all six: The Gini Index.},
	journaltitle = {{arXiv}:0811.4706 [cs, math]},
	author = {Hurley, Niall P. and Rickard, Scott T.},
	urldate = {2019-05-14},
	date = {2008-11-28},
	eprinttype = {arxiv},
	eprint = {0811.4706},
	keywords = {Computer Science - Information Theory},
}

@article{repetti_euclid_2015,
	title = {Euclid in a Taxicab: Sparse Blind Deconvolution with Smoothed l1/l2 Regularization},
	volume = {22},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/1407.5465},
	doi = {10.1109/LSP.2014.2362861},
	shorttitle = {Euclid in a Taxicab},
	abstract = {The l1/l2 ratio regularization function has shown good performance for retrieving sparse signals in a number of recent works, in the context of blind deconvolution. Indeed, it benefits from a scale invariance property much desirable in the blind context. However, the l1/l2 function raises some difficulties when solving the nonconvex and nonsmooth minimization problems resulting from the use of such a penalty term in current restoration methods. In this paper, we propose a new penalty based on a smooth approximation to the l1/l2 function. In addition, we develop a proximal-based algorithm to solve variational problems involving this function and we derive theoretical convergence results. We demonstrate the effectiveness of our method through a comparison with a recent alternating optimization strategy dealing with the exact l1/l2 term, on an application to seismic data blind deconvolution.},
	pages = {539--543},
	number = {5},
	journaltitle = {{IEEE} Signal Processing Letters},
	shortjournal = {{IEEE} Signal Process. Lett.},
	author = {Repetti, Audrey and Pham, Mai Quyen and Duval, Laurent and Chouzenoux, Emilie and Pesquet, Jean-Christophe},
	urldate = {2019-05-14},
	date = {2015-05},
	eprinttype = {arxiv},
	eprint = {1407.5465},
	keywords = {Mathematics - Optimization and Control},
}

@inproceedings{zhang_boosting_2017,
	location = {Taipei, Taiwan},
	title = {Boosting Neural Machine Translation},
	url = {https://www.aclweb.org/anthology/I17-2046},
	abstract = {Training efficiency is one of the main problems for Neural Machine Translation ({NMT}). Deep networks need for very large data as well as many training iterations to achieve state-of-the-art performance. This results in very high computation cost, slowing down research and industrialisation. In this paper, we propose to alleviate this problem with several training methods based on data boosting and bootstrap with no modifications to the neural network. It imitates the learning process of humans, which typically spend more time when learning “difficult” concepts than easier ones. We experiment on an English-French translation task showing accuracy improvements of up to 1.63 {BLEU} while saving 20\% of training time.},
	pages = {271--276},
	booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
	publisher = {Asian Federation of Natural Language Processing},
	author = {Zhang, Dakun and Kim, Jungi and Crego, Josep and Senellart, Jean},
	urldate = {2019-05-14},
	date = {2017-11},
}

@article{rahaman_spectral_2018,
	title = {On the Spectral Bias of Neural Networks},
	url = {http://arxiv.org/abs/1806.08734},
	abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with \$100{\textbackslash}\%\$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep {ReLU} networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets {\textbackslash}emph\{easier\} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.},
	journaltitle = {{arXiv}:1806.08734 [cs, stat]},
	author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred A. and Bengio, Yoshua and Courville, Aaron},
	urldate = {2019-05-13},
	date = {2018-06-22},
	eprinttype = {arxiv},
	eprint = {1806.08734},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{park_effect_2019,
	title = {The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study},
	url = {http://arxiv.org/abs/1905.03776},
	shorttitle = {The Effect of Network Width on Stochastic Gradient Descent and Generalization},
	abstract = {We investigate how the final parameters found by stochastic gradient descent are influenced by over-parameterization. We generate families of models by increasing the number of channels in a base network, and then perform a large hyper-parameter search to study how the test error depends on learning rate, batch size, and network width. We find that the optimal {SGD} hyper-parameters are determined by a "normalized noise scale," which is a function of the batch size, learning rate, and initialization conditions. In the absence of batch normalization, the optimal normalized noise scale is directly proportional to width. Wider networks, with their higher optimal noise scale, also achieve higher test accuracy. These observations hold for {MLPs}, {ConvNets}, and {ResNets}, and for two different parameterization schemes ("Standard" and "{NTK}"). We observe a similar trend with batch normalization for {ResNets}. Surprisingly, since the largest stable learning rate is bounded, the largest batch size consistent with the optimal normalized noise scale decreases as the width increases.},
	journaltitle = {{arXiv}:1905.03776 [cs, stat]},
	author = {Park, Daniel S. and Sohl-Dickstein, Jascha and Le, Quoc V. and Smith, Samuel L.},
	urldate = {2019-05-13},
	date = {2019-05-09},
	eprinttype = {arxiv},
	eprint = {1905.03776},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{greaves-tunnell_statistical_2019,
	title = {A Statistical Investigation of Long Memory in Language and Music},
	url = {http://arxiv.org/abs/1904.03834},
	abstract = {Representation and learning of long-range dependencies is a central challenge confronted in modern applications of machine learning to sequence data. Yet despite the prominence of this issue, the basic problem of measuring long-range dependence, either in a given data source or as represented in a trained deep model, remains largely limited to heuristic tools. We contribute a statistical framework for investigating long-range dependence in current applications of sequence modeling, drawing on the statistical theory of long memory stochastic processes. By analogy with their linear predecessors in the time series literature, we identify recurrent neural networks ({RNNs}) as nonlinear processes that simultaneously attempt to learn both a feature representation for and the long-range dependency structure of an input sequence. We derive testable implications concerning the relationship between long memory in real-world data and its learned representation in a deep network architecture, which are explored through a semiparametric framework adapted to the high-dimensional setting. We establish the validity of statistical inference for a simple estimator, which yields a decision rule for long memory in {RNNs}. Experiments illustrating this statistical framework confirm the presence of long memory in a diverse collection of natural language and music data, but show that a variety of {RNN} architectures fail to capture this property even after training to benchmark accuracy in a language model.},
	journaltitle = {{arXiv}:1904.03834 [cs, eess, stat]},
	author = {Greaves-Tunnell, Alexander and Harchaoui, Zaid},
	urldate = {2019-05-13},
	date = {2019-04-08},
	eprinttype = {arxiv},
	eprint = {1904.03834},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{haviv_understanding_2019,
	title = {Understanding and Controlling Memory in Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1902.07275},
	abstract = {To be effective in sequential data processing, Recurrent Neural Networks ({RNNs}) are required to keep track of past events by creating memories. While the relation between memories and the network's hidden state dynamics was established over the last decade, previous works in this direction were of a predominantly descriptive nature focusing mainly on locating the dynamical objects of interest. In particular, it remained unclear how dynamical observables affect the performance, how they form and whether they can be manipulated. Here, we utilize different training protocols, datasets and architectures to obtain a range of networks solving a delayed classification task with similar performance, alongside substantial differences in their ability to extrapolate for longer delays. We analyze the dynamics of the network's hidden state, and uncover the reasons for this difference. Each memory is found to be associated with a nearly steady state of the dynamics which we refer to as a 'slow point'. Slow point speeds predict extrapolation performance across all datasets, protocols and architectures tested. Furthermore, by tracking the formation of the slow points we are able to understand the origin of differences between training protocols. Finally, we propose a novel regularization technique that is based on the relation between hidden state speeds and memory longevity. Our technique manipulates these speeds, thereby leading to a dramatic improvement in memory robustness over time, and could pave the way for a new class of regularization methods.},
	journaltitle = {{arXiv}:1902.07275 [cs, stat]},
	author = {Haviv, Doron and Rivkind, Alexander and Barak, Omri},
	urldate = {2019-05-13},
	date = {2019-02-19},
	eprinttype = {arxiv},
	eprint = {1902.07275},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{vargas_model_2018,
	title = {Model Comparison for Semantic Grouping},
	url = {https://openreview.net/forum?id=HJMghjA9YX},
	abstract = {We introduce a probabilistic framework for quantifying the semantic similarity between two groups of embeddings. We formulate the task of semantic similarity as a model comparison task in which we...},
	author = {Vargas, Francisco and Brestnichki, Kamen and Hammerla, Nils},
	urldate = {2019-05-13},
	date = {2018-09-27},
}

@article{brunet_understanding_2018,
	title = {Understanding the Origins of Bias in Word Embeddings},
	url = {http://arxiv.org/abs/1810.03611},
	abstract = {The power of machine learning systems not only promises great technical progress, but risks societal harm. As a recent example, researchers have shown that popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems, from automated translation services to curriculum vitae scanners, can amplify stereotypes in important contexts. Although methods have been developed to measure these biases and alter word embeddings to mitigate their biased representations, there is a lack of understanding in how word embedding bias depends on the training data. In this work, we develop a technique for understanding the origins of bias in word embeddings. Given a word embedding trained on a corpus, our method identifies how perturbing the corpus will affect the bias of the resulting embedding. This can be used to trace the origins of word embedding bias back to the original training documents. Using our method, one can investigate trends in the bias of the underlying corpus and identify subsets of documents whose removal would most reduce bias. We demonstrate our techniques on both a New York Times and Wikipedia corpus and find that our influence function-based approximations are extremely accurate.},
	journaltitle = {{arXiv}:1810.03611 [cs, stat]},
	author = {Brunet, Marc-Etienne and Alkalay-Houlihan, Colleen and Anderson, Ashton and Zemel, Richard},
	urldate = {2019-05-13},
	date = {2018-10-08},
	eprinttype = {arxiv},
	eprint = {1810.03611},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{frankle_lottery_2019,
	title = {The Lottery Ticket Hypothesis at Scale},
	url = {http://arxiv.org/abs/1903.01611},
	abstract = {Recent work on the "lottery ticket hypothesis" proposes that randomly-initialized, dense neural networks contain much smaller, fortuitously initialized subnetworks ("winning tickets") capable of training to similar accuracy as the original network at a similar speed. While strong evidence exists for the hypothesis across many settings, it has not yet been evaluated on large, state-of-the-art networks and there is even evidence against the hypothesis on deeper networks. We modify the lottery ticket pruning procedure to make it possible to identify winning tickets on deeper networks. Rather than set the weights of a winning ticket to their original initializations, we set them to the weights obtained after a small number of training iterations ("late resetting"). Using late resetting, we identify the first winning tickets for Resnet-50 on Imagenet To understand the efficacy of late resetting, we study the "stability" of neural network training to pruning, which we define as the consistency of the optimization trajectories followed by a winning ticket when it is trained in isolation and as part of the larger network. We find that later resetting produces stabler winning tickets and that improved stability correlates with higher winning ticket accuracy. This analysis offers new insights into the lottery ticket hypothesis and the dynamics of neural network learning.},
	journaltitle = {{arXiv}:1903.01611 [cs, stat]},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
	urldate = {2019-05-07},
	date = {2019-03-04},
	eprinttype = {arxiv},
	eprint = {1903.01611},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{noauthor_smarter_nodate,
	title = {Smarter training of neural networks {\textbar} {MIT} {CSAIL}},
	url = {https://www.csail.mit.edu/news/smarter-training-neural-networks},
	urldate = {2019-05-07},
}

@article{achille_emergence_2017,
	title = {Emergence of Invariance and Disentanglement in Deep Representations},
	url = {http://arxiv.org/abs/1706.01350},
	abstract = {Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a {PAC}-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.},
	journaltitle = {{arXiv}:1706.01350 [cs, stat]},
	author = {Achille, Alessandro and Soatto, Stefano},
	urldate = {2019-05-07},
	date = {2017-06-05},
	eprinttype = {arxiv},
	eprint = {1706.01350},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{goldfeld_estimating_2018,
	title = {Estimating Information Flow in Neural Networks},
	url = {http://arxiv.org/abs/1810.05728},
	abstract = {We study the flow of information and the evolution of internal representations during deep neural network ({DNN}) training, aiming to demystify the compression aspect of the information bottleneck theory. The theory suggests that {DNN} training comprises a rapid fitting phase followed by a slower compression phase, in which the mutual information \$I(X;T)\$ between the input \$X\$ and internal representations \$T\$ decreases. Several papers observe compression of estimated mutual information on different {DNN} models, but the true \$I(X;T)\$ over these networks is provably either constant (discrete \$X\$) or infinite (continuous \$X\$). This work explains the discrepancy between theory and experiments, and clarifies what was actually measured by these past works. To this end, we introduce an auxiliary (noisy) {DNN} framework for which \$I(X;T)\$ is a meaningful quantity that depends on the network's parameters. This noisy framework is shown to be a good proxy for the original (deterministic) {DNN} both in terms of performance and the learned representations. We then develop a rigorous estimator for \$I(X;T)\$ in noisy {DNNs} and observe compression in various models. By relating \$I(X;T)\$ in the noisy {DNN} to an information-theoretic communication problem, we show that compression is driven by the progressive clustering of hidden representations of inputs from the same class. Several methods to directly monitor clustering of hidden representations, both in noisy and deterministic {DNNs}, are used to show that meaningful clusters form in the \$T\$ space. Finally, we return to the estimator of \$I(X;T)\$ employed in past works, and demonstrate that while it fails to capture the true (vacuous) mutual information, it does serve as a measure for clustering. This clarifies the past observations of compression and isolates the geometric clustering of hidden representations as the true phenomenon of interest.},
	journaltitle = {{arXiv}:1810.05728 [cs, stat]},
	author = {Goldfeld, Ziv and Berg, Ewout van den and Greenewald, Kristjan and Melnyk, Igor and Nguyen, Nam and Kingsbury, Brian and Polyanskiy, Yury},
	urldate = {2019-05-06},
	date = {2018-10-12},
	eprinttype = {arxiv},
	eprint = {1810.05728},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tishby_deep_2015,
	title = {Deep Learning and the Information Bottleneck Principle},
	url = {http://arxiv.org/abs/1503.02406},
	abstract = {Deep Neural Networks ({DNNs}) are analyzed via the theoretical framework of the information bottleneck ({IB}) principle. We first show that any {DNN} can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the {DNN} and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	journaltitle = {{arXiv}:1503.02406 [cs]},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	urldate = {2019-05-06},
	date = {2015-03-09},
	eprinttype = {arxiv},
	eprint = {1503.02406},
	keywords = {Computer Science - Machine Learning},
}

@article{liu_linguistic_2019,
	title = {Linguistic Knowledge and Transferability of Contextual Representations},
	url = {http://arxiv.org/abs/1903.08855},
	abstract = {Contextual word representations derived from large-scale neural language models are successful across a diverse set of {NLP} tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of {ELMo}, the {OpenAI} transformer language model, and {BERT}) with a suite of seventeen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks ({RNNs}) and transformers. For instance, higher layers of {RNNs} are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.},
	journaltitle = {{arXiv}:1903.08855 [cs]},
	author = {Liu, Nelson F. and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E. and Smith, Noah A.},
	urldate = {2019-04-30},
	date = {2019-03-21},
	eprinttype = {arxiv},
	eprint = {1903.08855},
	keywords = {Computer Science - Computation and Language},
}

@article{gonen_lipstick_2019,
	title = {Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them},
	url = {http://arxiv.org/abs/1903.03862},
	shorttitle = {Lipstick on a Pig},
	abstract = {Word embeddings are widely used in {NLP} for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between "gender-neutralized" words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.},
	journaltitle = {{arXiv}:1903.03862 [cs]},
	author = {Gonen, Hila and Goldberg, Yoav},
	urldate = {2019-04-08},
	date = {2019-03-09},
	eprinttype = {arxiv},
	eprint = {1903.03862},
	keywords = {Computer Science - Computation and Language},
}

@article{liu_cyclical_2019,
	title = {Cyclical Annealing Schedule: A Simple Approach to Mitigating {KL} Vanishing},
	shorttitle = {Cyclical Annealing Schedule},
	journaltitle = {{arXiv} preprint {arXiv}:1903.10145},
	author = {Liu, Xiaodong and Gao, Jianfeng and Celikyilmaz, Asli and Carin, Lawrence},
	date = {2019},
}

@article{alvarez-melis_causal_2017,
	title = {A causal framework for explaining the predictions of black-box sequence-to-sequence models},
	journaltitle = {{arXiv} preprint {arXiv}:1707.01943},
	author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
	date = {2017},
}

@article{le_learning_2018,
	title = {Learning to Remember More with Less Memorization},
	url = {https://openreview.net/forum?id=r1xlvi0qYm},
	abstract = {Memory-augmented neural networks consisting of a neural controller and an external memory have shown potentials in long-term sequential learning. Current {RAM}-like memory models maintain memory...},
	author = {Le, Hung and Tran, Truyen and Venkatesh, Svetha},
	urldate = {2019-04-05},
	date = {2018-09-27},
}
@article{mccoy_rnns_2018,
	title = {{RNNs} implicitly implement tensor-product representations},
	url = {https://openreview.net/forum?id=BJx0sjC5FX},
	abstract = {Recurrent neural networks ({RNNs}) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities...},
	author = {{McCoy}, R. Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
	urldate = {2019-04-05},
	date = {2018-09-27},
}

@article{roth_kernel_2018,
	title = {Kernel {RNN} Learning ({KeRNL})},
	url = {https://openreview.net/forum?id=ryGfnoC5KQ},
	abstract = {We describe Kernel {RNN} Learning ({KeRNL}), a reduced-rank, temporal eligibility trace-based approximation to backpropagation through time ({BPTT}) for training recurrent neural networks ({RNNs}) that...},
	author = {Roth, Christopher and Kanitscheider, Ingmar and Fiete, Ila},
	urldate = {2019-04-05},
	date = {2018-09-27},
}

@article{saxton_analysing_2018,
	title = {Analysing Mathematical Reasoning Abilities of Neural Models},
	url = {https://openreview.net/forum?id=H1gR5iR5FX},
	abstract = {Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back...},
	author = {Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
	urldate = {2019-04-05},
	date = {2018-09-27},
}

@article{hu_overcoming_2018,
	title = {Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation},
	url = {https://openreview.net/forum?id=ryGvcoA5YX},
	abstract = {Learning multiple tasks sequentially is important for the development of {AI} and lifelong learning systems. However, standard neural network architectures suffer from catastrophic forgetting which...},
	author = {Hu, Wenpeng and Lin, Zhou and Liu, Bing and Tao, Chongyang and Tao, Zhengwei and Ma, Jinwen and Zhao, Dongyan and Yan, Rui},
	urldate = {2019-04-05},
	date = {2018-09-27},
}

@article{aljundi_selfless_2018,
	title = {Selfless Sequential Learning},
	url = {https://openreview.net/forum?id=Bkxbrn0cYX},
	abstract = {Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a...},
	author = {Aljundi, Rahaf and Rohrbach, Marcus and Tuytelaars, Tinne},
	urldate = {2019-04-05},
	date = {2018-09-27},
}

@article{du_attribution_2019,
	title = {On Attribution of Recurrent Neural Network Predictions via Additive Decomposition},
	journaltitle = {{arXiv} preprint {arXiv}:1903.11245},
	author = {Du, Mengnan and Liu, Ninghao and Yang, Fan and Ji, Shuiwang and Hu, Xia},
	date = {2019},
}

@online{abnar_attention_2019,
	title = {From Attention in Transformers to Dynamic Routing in Capsule Nets},
	url = {https://staff.fnwi.uva.nl/s.abnar/?p=108},
	abstract = {In this post, we go through the main building blocks of transformers and capsule networks and try to draw a connection between different components of these two models. Our main goal here is to understand if these models are inherently different and if not, how they relate. Special thanks to Sara Sabour and Mostafa Dehghani …},
	titleaddon = {Samira Abnar},
	author = {Abnar, Samira},
	urldate = {2019-03-28},
	date = {2019-03-27},
	langid = {american},
}

@article{vanmassenhove_investigating_2017,
	title = {Investigating ‘Aspect’in {NMT} and {SMT}: Translating the English Simple Past and Present Perfect},
	volume = {7},
	shorttitle = {Investigating ‘Aspect’in {NMT} and {SMT}},
	pages = {109--128},
	journaltitle = {Computational Linguistics in the Netherlands Journal},
	author = {Vanmassenhove, Eva and Du, Jinhua and Way, Andy},
	date = {2017},
}

@article{ravfogel_studying_2019,
	title = {Studying the Inductive Biases of {RNNs} with Synthetic Variations of Natural Languages},
	url = {http://arxiv.org/abs/1903.06400},
	abstract = {How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language? Cross-linguistic comparisons of {RNNs}' syntactic performance (e.g., on subject-verb agreement prediction) are complicated by the fact that any two languages differ in multiple typological properties, as well as by differences in training corpus. We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in a single typological parameter, and generate corpora for those languages based on a parsed English corpus. We report a series of experiments in which {RNNs} were trained to predict agreement features for verbs in each of those synthetic languages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that {RNNs} have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order.},
	journaltitle = {{arXiv}:1903.06400 [cs]},
	author = {Ravfogel, Shauli and Goldberg, Yoav and Linzen, Tal},
	urldate = {2019-03-25},
	date = {2019-03-15},
	eprinttype = {arxiv},
	eprint = {1903.06400},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{saphra_understanding_2019,
	title = {Understanding Learning Dynamics Of Language Models with {SVCCA}},
	rights = {All rights reserved},
	url = {http://arxiv.org/abs/1811.00225},
	abstract = {Research has shown that neural models implicitly encode linguistic features, but there has been no research showing {\textbackslash}emph\{how\} these encodings arise as the models are trained. We present the first study on the learning dynamics of neural language models, using a simple and flexible analysis method called Singular Vector Canonical Correlation Analysis ({SVCCA}), which enables us to compare learned representations across time and across models, without the need to evaluate directly on annotated data. We probe the evolution of syntactic, semantic, and topic representations and find that part-of-speech is learned earlier than topic; that recurrent layers become more similar to those of a tagger during training; and embedding layers less similar. Our results and methods could inform better learning algorithms for {NLP} models, possibly to incorporate linguistic information more effectively.},
	booktitle = {{NAACL}},
	author = {Saphra, Naomi and Lopez, Adam},
	urldate = {2019-03-21},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1811.00225},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@article{lakretz_emergence_2019,
	title = {The emergence of number and syntax units in {LSTM} language models},
	url = {http://arxiv.org/abs/1903.07435},
	abstract = {Recent work has shown that {LSTMs} trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on heuristics that do not truly take hierarchical structure into account. We present here a detailed study of the inner mechanics of number tracking in {LSTMs} at the single neuron level. We discover that long-distance number information is largely managed by two ``number units''. Importantly, the behaviour of these units is partially controlled by other units independently shown to track syntactic structure. We conclude that {LSTMs} are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in {LSTMs}.},
	journaltitle = {{arXiv}:1903.07435 [cs]},
	author = {Lakretz, Yair and Kruszewski, German and Desbordes, Theo and Hupkes, Dieuwke and Dehaene, Stanislas and Baroni, Marco},
	urldate = {2019-03-20},
	date = {2019-03-18},
	eprinttype = {arxiv},
	eprint = {1903.07435},
	keywords = {Computer Science - Computation and Language},
}

@article{srivastava_highway_2015,
	title = {Highway Networks},
	url = {http://arxiv.org/abs/1505.00387},
	abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
	journaltitle = {{arXiv}:1505.00387 [cs]},
	author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
	urldate = {2019-03-20},
	date = {2015-05-02},
	eprinttype = {arxiv},
	eprint = {1505.00387},
	keywords = {68T01, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, G.1.6, I.2.6},
}

@article{peters_tune_2019,
	title = {To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks},
	url = {http://arxiv.org/abs/1903.05987},
	shorttitle = {To Tune or Not to Tune?},
	abstract = {While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse {NLP} tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the {NLP} practitioner.},
	journaltitle = {{arXiv}:1903.05987 [cs]},
	author = {Peters, Matthew and Ruder, Sebastian and Smith, Noah A.},
	urldate = {2019-03-20},
	date = {2019-03-14},
	eprinttype = {arxiv},
	eprint = {1903.05987},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{jain_attention_2019,
	title = {Attention is not Explanation},
	url = {http://arxiv.org/abs/1902.10186},
	abstract = {Attention mechanisms have seen wide adoption in neural {NLP} models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of {NLP} tasks that aim to assess the degree to which attention weights provide meaningful ‘explanations’ for predictions. We ﬁnd that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our ﬁndings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/ successar/{AttentionExplanation}.},
	journaltitle = {{arXiv}:1902.10186 [cs]},
	author = {Jain, Sarthak and Wallace, Byron C.},
	urldate = {2019-02-28},
	date = {2019-02-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1902.10186},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{bang_explaining_2019,
	title = {Explaining a black-box using Deep Variational Information Bottleneck Approach},
	url = {http://arxiv.org/abs/1902.06918},
	abstract = {Briefness and comprehensiveness are necessary in order to give a lot of information concisely in explaining a black-box decision system. However, existing interpretable machine learning methods fail to consider briefness and comprehensiveness simultaneously, which may lead to redundant explanations. We propose a system-agnostic interpretable method that provides a brief but comprehensive explanation by adopting the inspiring information theoretic principle, information bottleneck principle. Using an information theoretic objective, {VIBI} selects instance-wise key features that are maximally compressed about an input (briefness), and informative about a decision made by a black-box on that input (comprehensive). The selected key features act as an information bottleneck that serves as a concise explanation for each black-box decision. We show that {VIBI} outperforms other interpretable machine learning methods in terms of both interpretability and fidelity evaluated by human and quantitative metrics.},
	journaltitle = {{arXiv}:1902.06918 [cs, stat]},
	author = {Bang, Seojin and Xie, Pengtao and Wu, Wei and Xing, Eric},
	urldate = {2019-02-25},
	date = {2019-02-19},
	eprinttype = {arxiv},
	eprint = {1902.06918},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ibrahim_global_2019,
	title = {Global Explanations of Neural Networks: Mapping the Landscape of Predictions},
	url = {http://arxiv.org/abs/1902.02384},
	shorttitle = {Global Explanations of Neural Networks},
	abstract = {A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called {GAM}, which explains the landscape of neural network predictions across subpopulations. {GAM} augments global explanations with the proportion of samples that each attribution best explains and specifies which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that {GAM}'s global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, {GAM} can help ensure neural network decisions are generated for the right reasons.},
	journaltitle = {{arXiv}:1902.02384 [cs, stat]},
	author = {Ibrahim, Mark and Louie, Melissa and Modarres, Ceena and Paisley, John},
	urldate = {2019-02-11},
	date = {2019-02-06},
	eprinttype = {arxiv},
	eprint = {1902.02384},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhang_empirical_2018,
	title = {An Empirical Exploration of Curriculum Learning for Neural Machine Translation},
	url = {http://arxiv.org/abs/1811.00739},
	abstract = {Machine translation systems based on deep neural networks are expensive to train. Curriculum learning aims to address this issue by choosing the order in which samples are presented during training to help train better models faster. We adopt a probabilistic view of curriculum learning, which lets us flexibly evaluate the impact of curricula design, and perform an extensive exploration on a German-English translation task. Results show that it is possible to improve convergence time at no loss in translation quality. However, results are highly sensitive to the choice of sample difficulty criteria, curriculum schedule and other hyperparameters.},
	journaltitle = {{arXiv}:1811.00739 [cs]},
	author = {Zhang, Xuan and Kumar, Gaurav and Khayrallah, Huda and Murray, Kenton and Gwinnup, Jeremy and Martindale, Marianna J. and {McNamee}, Paul and Duh, Kevin and Carpuat, Marine},
	urldate = {2019-02-11},
	date = {2018-11-02},
	eprinttype = {arxiv},
	eprint = {1811.00739},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{tikka_causal_2019,
	title = {Causal Effect Identification from Multiple Incomplete Data Sources: A General Search-based Approach},
	url = {http://arxiv.org/abs/1902.01073},
	shorttitle = {Causal Effect Identification from Multiple Incomplete Data Sources},
	abstract = {Causal effect identification considers whether an interventional probability distribution can be uniquely determined without parametric assumptions from measured source distributions and structural knowledge on the generating system. While complete graphical criteria and procedures exist for many identification problems, there are still challenging but important extensions that have not been considered in the literature. To tackle these new settings, we present a search algorithm directly over the rules of do-calculus. Due to generality of do-calculus, the search is capable of taking more advanced data-generating mechanisms into account along with an arbitrary type of both observational and experimental source distributions. The search is enhanced via a heuristic and search space reduction techniques. The approach, called do-search, is provably sound, and it is complete with respect to identifiability problems that have been shown to be completely characterized by do-calculus. When extended with additional rules, the search is capable of handling missing data problems as well. With the versatile search, we are able to approach new problems such as combined transportability and selection bias, or multiple sources of selection bias. We also perform a systematic analysis of bivariate missing data problems and study causal inference under case-control design.},
	journaltitle = {{arXiv}:1902.01073 [cs, stat]},
	author = {Tikka, Santtu and Hyttinen, Antti and Karvanen, Juha},
	urldate = {2019-02-11},
	date = {2019-02-04},
	eprinttype = {arxiv},
	eprint = {1902.01073},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_understanding_2016,
	title = {Understanding neural networks through representation erasure},
	journaltitle = {{arXiv} preprint {arXiv}:1612.08220},
	author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
	date = {2016},
}

@article{yin_ratio_2014,
	title = {Ratio and difference of l1 and l2 norms and sparse representation with coherent dictionaries},
	volume = {14},
	pages = {87--109},
	number = {2},
	journaltitle = {Commun. Inform. Systems},
	author = {Yin, Penghang and Esser, Ernie and Xin, Jack},
	date = {2014},
}

@report{zhang_theory_2017,
	title = {Theory of deep learning iii: Generalization properties of sgd},
	shorttitle = {Theory of deep learning iii},
	institution = {Center for Brains, Minds and Machines ({CBMM})},
	author = {Zhang, Chiyuan and Liao, Qianli and Rakhlin, Alexander and Sridharan, Karthik and Miranda, Brando and Golowich, Noah and Poggio, Tomaso},
	date = {2017},
}

@article{mandt_stochastic_2017,
	title = {Stochastic gradient descent as approximate Bayesian inference},
	volume = {18},
	pages = {4873--4907},
	number = {1},
	journaltitle = {The Journal of Machine Learning Research},
	author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
	date = {2017},
}

@article{kirov_recurrent_2018,
	title = {Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate},
	shorttitle = {Recurrent Neural Networks in Linguistic Theory},
	journaltitle = {{arXiv} preprint {arXiv}:1807.04783},
	author = {Kirov, Christo and Cotterell, Ryan},
	date = {2018},
}

@article{goldberg_assessing_2019,
	title = {Assessing {BERT}'s Syntactic Abilities},
	url = {http://arxiv.org/abs/1901.05287},
	abstract = {I assess the extent to which the recently introduced {BERT} model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) "coloreless green ideas" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The {BERT} model performs remarkably well on all cases.},
	journaltitle = {{arXiv}:1901.05287 [cs]},
	author = {Goldberg, Yoav},
	urldate = {2019-01-22},
	date = {2019-01-16},
	eprinttype = {arxiv},
	eprint = {1901.05287},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{ancona_towards_2018,
	title = {Towards better understanding of gradient-based attribution methods for Deep Neural Networks},
	rights = {http://rightsstatements.org/page/{InC}-{NC}/1.0/},
	url = {https://www.research-collection.ethz.ch/handle/20.500.11850/249929},
	doi = {10.3929/ethz-b-000249929},
	abstract = {Understanding the flow of information in Deep Neural Networks ({DNNs}) is a challenging problem that has gain increasing attention over the last few years.  While several methods have been proposed to explain network predictions, only a few attempts to analyze them from a theoretical perspective have been made in the past. In this work, we analyze various state-of-the-art attribution methods and prove unexplored connections between them. We also show how some methods can be reformulated and more conveniently implemented.  Finally, we perform an empirical evaluation with six attribution methods on a variety of tasks and architectures and discuss their strengths and limitations.},
	eventtitle = {6th International Conference on Learning Representations ({ICLR} 2018)},
	author = {Ancona, Marco and Ceolini, Enea and Oztireli, Cengiz and Gross, Markus},
	urldate = {2019-01-21},
	date = {2018},
	langid = {english},
}

@article{wolf-sonkin_structured_2018,
	title = {A Structured Variational Autoencoder for Contextual Morphological Inflection},
	url = {http://arxiv.org/abs/1806.03746},
	abstract = {Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10\% absolute accuracy in some cases.},
	journaltitle = {{arXiv}:1806.03746 [cs]},
	author = {Wolf-Sonkin, Lawrence and Naradowsky, Jason and Mielke, Sebastian J. and Cotterell, Ryan},
	urldate = {2019-01-21},
	date = {2018-06-10},
	eprinttype = {arxiv},
	eprint = {1806.03746},
	keywords = {Computer Science - Computation and Language},
}

@article{singh_hierarchical_2018,
	title = {Hierarchical interpretations for neural network predictions},
	url = {http://arxiv.org/abs/1806.05337},
	abstract = {Deep neural networks ({DNNs}) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to {DNNs} being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain {DNN} predictions through our proposed method, agglomerative contextual decomposition ({ACD}). Given a prediction from a trained {DNN}, {ACD} produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the {DNN} learned are predictive. Using examples from Stanford Sentiment Treebank and {ImageNet}, we show that {ACD} is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that {ACD} enables users both to identify the more accurate of two {DNNs} and to better trust a {DNN}'s outputs. We also find that {ACD}'s hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.},
	journaltitle = {{arXiv}:1806.05337 [cs, stat]},
	author = {Singh, Chandan and Murdoch, W. James and Yu, Bin},
	urldate = {2019-01-14},
	date = {2018-06-13},
	eprinttype = {arxiv},
	eprint = {1806.05337},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{murdoch_automatic_2017,
	title = {Automatic Rule Extraction from Long Short Term Memory Networks},
	url = {http://arxiv.org/abs/1702.02540},
	abstract = {Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks ({LSTMs}) and demonstrate a new approach for tracking the importance of a given input to the {LSTM} for a given output. By identifying consistently important patterns of words, we are able to distill state of the art {LSTMs} on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the {LSTM}.},
	journaltitle = {{arXiv}:1702.02540 [cs, stat]},
	author = {Murdoch, W. James and Szlam, Arthur},
	urldate = {2019-01-14},
	date = {2017-02-08},
	eprinttype = {arxiv},
	eprint = {1702.02540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{wang_gate_2017,
	title = {Gate Activation Signal Analysis for Gated Recurrent Neural Networks and Its Correlation with Phoneme Boundaries},
	url = {http://arxiv.org/abs/1703.07588},
	abstract = {In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.},
	journaltitle = {{arXiv}:1703.07588 [cs]},
	author = {Wang, Yu-Hsuan and Chung, Cheng-Tao and Lee, Hung-yi},
	urldate = {2019-01-09},
	date = {2017-03-22},
	eprinttype = {arxiv},
	eprint = {1703.07588},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound},
}

@online{noauthor_[1703.07588]_nodate,
	title = {[1703.07588] Gate Activation Signal Analysis for Gated Recurrent Neural Networks and Its Correlation with Phoneme Boundaries},
	url = {https://arxiv.org/abs/1703.07588},
	urldate = {2019-01-09},
}

@article{kim_tutorial_2018,
	title = {A Tutorial on Deep Latent Variable Models of Natural Language},
	url = {http://arxiv.org/abs/1812.06834},
	abstract = {There has been much recent, exciting work on combining the complementary strengths of latent variable models and deep learning. Latent variable modeling makes it easy to explicitly specify model constraints through conditional independence properties, while deep learning makes it possible to parameterize these conditional likelihoods with powerful function approximators. While these "deep latent variable" models provide a rich, flexible framework for modeling many real-world phenomena, difficulties exist: deep parameterizations of conditional likelihoods usually make posterior inference intractable, and latent variable objectives often complicate backpropagation by introducing points of non-differentiability. This tutorial explores these issues in depth through the lens of variational inference.},
	journaltitle = {{arXiv}:1812.06834 [cs, stat]},
	author = {Kim, Yoon and Wiseman, Sam and Rush, Alexander M.},
	urldate = {2019-01-08},
	date = {2018-12-17},
	eprinttype = {arxiv},
	eprint = {1812.06834},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{suzgun_evaluating_2018,
	title = {On Evaluating the Generalization of {LSTM} Models in Formal Languages},
	url = {http://arxiv.org/abs/1811.01001},
	abstract = {Recurrent Neural Networks ({RNNs}) are theoretically Turing-complete and established themselves as a dominant model for language processing. Yet, there still remains an uncertainty regarding their language learning capabilities. In this paper, we empirically evaluate the inductive learning capabilities of Long Short-Term Memory networks, a popular extension of simple {RNNs}, to learn simple formal languages, in particular \$a{\textasciicircum}nb{\textasciicircum}n\$, \$a{\textasciicircum}nb{\textasciicircum}nc{\textasciicircum}n\$, and \$a{\textasciicircum}nb{\textasciicircum}nc{\textasciicircum}nd{\textasciicircum}n\$. We investigate the influence of various aspects of learning, such as training data regimes and model capacity, on the generalization to unobserved samples. We find striking differences in model performances under different training settings and highlight the need for careful analysis and assessment when making claims about the learning capabilities of neural network models.},
	journaltitle = {{arXiv}:1811.01001 [cs]},
	author = {Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M.},
	urldate = {2019-01-06},
	date = {2018-11-02},
	eprinttype = {arxiv},
	eprint = {1811.01001},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, F.4.3, I.2.6, I.2.7},
}

@article{lu_shared_2018,
	title = {Shared Representational Geometry Across Neural Networks},
	url = {http://arxiv.org/abs/1811.11684},
	abstract = {Different neural networks trained on the same dataset often learn similar input-output mappings with very different weights. Is there some correspondence between these neural network solutions? For linear networks, it has been shown that different instances of the same network architecture encode the same representational similarity matrix, and their neural activity patterns are connected by orthogonal transformations. However, it is unclear if this holds for non-linear networks. Using a shared response model, we show that different neural networks encode the same input examples as different orthogonal transformations of an underlying shared representation. We test this claim using both standard convolutional neural networks and residual networks on {CIFAR}10 and {CIFAR}100.},
	journaltitle = {{arXiv}:1811.11684 [cs, stat]},
	author = {Lu, Qihong and Chen, Po-Hsuan and Pillow, Jonathan W. and Ramadge, Peter J. and Norman, Kenneth A. and Hasson, Uri},
	urldate = {2019-01-06},
	date = {2018-11-28},
	eprinttype = {arxiv},
	eprint = {1811.11684},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}
@incollection{wang_towards_2018,
	title = {Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation},
	url = {http://papers.nips.cc/paper/8167-towards-understanding-learning-representations-to-what-extent-do-different-neural-networks-learn-the-same-representation.pdf},
	shorttitle = {Towards Understanding Learning Representations},
	pages = {9607--9616},
	booktitle = {Advances in Neural Information Processing Systems 31},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Liwei and Hu, Lunjia and Gu, Jiayuan and Hu, Zhiqiang and Wu, Yue and He, Kun and Hopcroft, John},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	urldate = {2019-01-06},
	date = {2018},
}

@article{dalvi_what_2018,
	title = {What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep {NLP} Models},
	url = {http://arxiv.org/abs/1812.09355},
	shorttitle = {What Is One Grain of Sand in the Desert?},
	abstract = {Despite the remarkable evolution of deep neural networks in natural language processing ({NLP}), their interpretability remains a challenge. Previous work largely focused on what these models learn at the representation level. We break this analysis down further and study individual dimensions (neurons) in the vector representation learned by end-to-end neural models in {NLP} tasks. We propose two methods: Linguistic Correlation Analysis, based on a supervised method to extract the most relevant neurons with respect to an extrinsic task, and Cross-model Correlation Analysis, an unsupervised method to extract salient neurons w.r.t. the model itself. We evaluate the effectiveness of our techniques by ablating the identified neurons and reevaluating the network's performance for two tasks: neural machine translation ({NMT}) and neural language modeling ({NLM}). We further present a comprehensive analysis of neurons with the aim to address the following questions: i) how localized or distributed are different linguistic properties in the models? ii) are certain neurons exclusive to some properties and not others? iii) is the information more or less distributed in {NMT} vs. {NLM}? and iv) how important are the neurons identified through the linguistic correlation method to the overall task? Our code is publicly available as part of the {NeuroX} toolkit (Dalvi et al. 2019).},
	journaltitle = {{arXiv}:1812.09355 [cs]},
	author = {Dalvi, Fahim and Durrani, Nadir and Sajjad, Hassan and Belinkov, Yonatan and Bau, Anthony and Glass, James},
	urldate = {2019-01-05},
	date = {2018-12-21},
	eprinttype = {arxiv},
	eprint = {1812.09355},
	keywords = {Computer Science - Computation and Language},
}

@article{liu_towards_2018,
	title = {Towards Explainable {NLP}: A Generative Explanation Framework for Text Classification},
	shorttitle = {Towards Explainable {NLP}},
	journaltitle = {{arXiv} preprint {arXiv}:1811.00196},
	author = {Liu, Hui and Yin, Qingyu and Wang, William Yang},
	date = {2018},
}

@online{boston_causal_2012,
	title = {Causal Inference Book},
	url = {https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/},
	abstract = {My colleague Jamie Robins and I are working on a book that provides a cohesive presentation of concepts of, and methods for, causal inference. Much of this material is currently scattered across jo…},
	titleaddon = {Miguel Hernan},
	author = {Boston, 677 Huntington Avenue and Ma 02115 +1495‑1000},
	urldate = {2019-01-04},
	date = {2012-10-19},
	langid = {english},
}

@article{bjerva_tracking_2017,
	title = {Tracking typological traits of uralic languages in distributed language representations},
	journaltitle = {{arXiv} preprint {arXiv}:1711.05468},
	author = {Bjerva, Johannes and Augenstein, Isabelle},
	date = {2017},
}

@article{bjerva_phonology_2018,
	title = {From Phonology to Syntax: Unsupervised Linguistic Typology at Different Levels with Language Embeddings},
	shorttitle = {From Phonology to Syntax},
	journaltitle = {{arXiv} preprint {arXiv}:1802.09375},
	author = {Bjerva, Johannes and Augenstein, Isabelle},
	date = {2018},
}

@article{besserve_counterfactuals_2018,
	title = {Counterfactuals uncover the modular structure of deep generative models},
	journaltitle = {{arXiv} preprint {arXiv}:1812.03253},
	author = {Besserve, Michel and Sun, Rémy and Schölkopf, Bernhard},
	date = {2018},
}

@article{belinkov_analysis_2018,
	title = {Analysis Methods in Neural Language Processing: A Survey},
	shorttitle = {Analysis Methods in Neural Language Processing},
	journaltitle = {{arXiv} preprint {arXiv}:1812.08951},
	author = {Belinkov, Yonatan and Glass, James},
	date = {2018},
}

@article{zhou_non-vacuous_2018,
	title = {Non-vacuous Generalization Bounds at the {ImageNet} Scale: a {PAC}-Bayesian Compression Approach},
	shorttitle = {Non-vacuous Generalization Bounds at the {ImageNet} Scale},
	author = {Zhou, Wenda and Veitch, Victor and Austern, Morgane and Adams, Ryan P. and Orbanz, Peter},
	date = {2018},
}

@article{bahdanau_systematic_2018,
	title = {Systematic Generalization: What Is Required and Can It Be Learned?},
	shorttitle = {Systematic Generalization},
	journaltitle = {{arXiv} preprint {arXiv}:1811.12889},
	author = {Bahdanau, Dzmitry and Murty, Shikhar and Noukhovitch, Michael and Nguyen, Thien Huu and de Vries, Harm and Courville, Aaron},
	date = {2018},
}

@inproceedings{yeh_representer_2018,
	title = {Representer Point Selection for Explaining Deep Neural Networks},
	pages = {9311--9321},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Yeh, Chih-Kuan and Kim, Joon and Yen, Ian En-Hsu and Ravikumar, Pradeep K.},
	date = {2018},
}

@article{noshad_scalable_2018,
	title = {Scalable Mutual Information Estimation using Dependence Graphs},
	journaltitle = {{arXiv} preprint {arXiv}:1801.09125},
	author = {Noshad, Morteza and Hero {III}, Alfred O.},
	date = {2018},
}

@article{burda_exploration_2018,
	title = {Exploration by Random Network Distillation},
	url = {http://arxiv.org/abs/1810.12894},
	abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation ({RND}) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
	journaltitle = {{arXiv}:1810.12894 [cs, stat]},
	author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
	urldate = {2018-12-18},
	date = {2018-10-30},
	eprinttype = {arxiv},
	eprint = {1810.12894},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{press_you_2018,
	title = {You May Not Need Attention},
	url = {http://arxiv.org/abs/1810.13409},
	abstract = {In {NMT}, how far can we get without attention and without separate encoding and decoding? To answer that question, we introduce a recurrent neural translation model that does not use attention and does not have a separate encoder and decoder. Our eager translation model is low-latency, writing target tokens as soon as it reads the first source token, and uses constant memory during decoding. It performs on par with the standard attention-based model of Bahdanau et al. (2014), and better on long sentences.},
	journaltitle = {{arXiv}:1810.13409 [cs]},
	author = {Press, Ofir and Smith, Noah A.},
	urldate = {2018-12-18},
	date = {2018-10-31},
	eprinttype = {arxiv},
	eprint = {1810.13409},
	keywords = {Computer Science - Computation and Language},
}

@article{gotmare_closer_2018,
	title = {A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation},
	url = {http://arxiv.org/abs/1810.13243},
	shorttitle = {A Closer Look at Deep Learning Heuristics},
	abstract = {The convergence rate and final performance of common deep learning models have significantly benefited from heuristics such as learning rate schedules, knowledge distillation, skip connections, and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit such analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz., mode connectivity and canonical correlation analysis ({CCA}), and hypothesize reasons for the success of the heuristics. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectivity and {CCA}. Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed to the deeper layers.},
	journaltitle = {{arXiv}:1810.13243 [cs, stat]},
	author = {Gotmare, Akhilesh and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
	urldate = {2018-12-17},
	date = {2018-10-29},
	eprinttype = {arxiv},
	eprint = {1810.13243},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kementchedjhieva_indicatements_2018,
	title = {Indicatements that character language models learn English morpho-syntactic units and regularities},
	journaltitle = {{arXiv} preprint {arXiv}:1809.00066},
	author = {Kementchedjhieva, Yova and Lopez, Adam},
	date = {2018},
}

@article{verwimp_state_2018,
	title = {State Gradients for {RNN} Memory Analysis},
	url = {http://arxiv.org/abs/1805.04264},
	abstract = {We present a framework for analyzing what the state in {RNNs} remembers from its input embeddings. Our approach is inspired by backpropagation, in the sense that we compute the gradients of the states with respect to the input embeddings. The gradient matrix is decomposed with Singular Value Decomposition to analyze which directions in the embedding space are best transferred to the hidden state space, characterized by the largest singular values. We apply our approach to {LSTM} language models and investigate to what extent and for how long certain classes of words are remembered on average for a certain corpus. Additionally, the extent to which a specific property or relationship is remembered by the {RNN} can be tracked by comparing a vector characterizing that property with the direction(s) in embedding space that are best preserved in hidden state space.},
	journaltitle = {{arXiv}:1805.04264 [cs]},
	author = {Verwimp, Lyan and Van hamme, Hugo and Renkens, Vincent and Wambacq, Patrick},
	urldate = {2018-12-12},
	date = {2018-05-11},
	eprinttype = {arxiv},
	eprint = {1805.04264},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@article{hutchinson_50_2018,
	title = {50 Years of Test (Un)fairness: Lessons for Machine Learning},
	url = {http://arxiv.org/abs/1811.10104},
	doi = {10.1145/3287560.3287600},
	shorttitle = {50 Years of Test (Un)fairness},
	abstract = {Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.},
	journaltitle = {{arXiv}:1811.10104 [cs]},
	author = {Hutchinson, Ben and Mitchell, Margaret},
	urldate = {2018-12-11},
	date = {2018-11-25},
	eprinttype = {arxiv},
	eprint = {1811.10104},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{bau_identifying_2018,
	title = {Identifying and Controlling Important Neurons in Neural Machine Translation},
	url = {http://arxiv.org/abs/1811.01157},
	abstract = {Neural machine translation ({NMT}) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in {NMT} models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control {NMT} translations in predictable ways, by modifying activations of individual neurons.},
	journaltitle = {{arXiv}:1811.01157 [cs]},
	author = {Bau, Anthony and Belinkov, Yonatan and Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Glass, James},
	urldate = {2018-12-04},
	date = {2018-11-03},
	eprinttype = {arxiv},
	eprint = {1811.01157},
	keywords = {Computer Science - Computation and Language, I.2.7},
}

@article{zaslavsky_efficient_2018,
	title = {Efficient compression in color naming and its evolution},
	volume = {115},
	pages = {7937--7942},
	number = {31},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and Tishby, Naftali},
	date = {2018},
}

@article{jitkrittum_informative_2018,
	title = {Informative Features for Model Comparison},
	url = {http://arxiv.org/abs/1810.11630},
	abstract = {Given two candidate models, and a set of target observations, we address the problem of measuring the relative goodness of fit of the two models. We propose two new statistical tests which are nonparametric, computationally efficient (runtime complexity is linear in the sample size), and interpretable. As a unique advantage, our tests can produce a set of examples (informative features) indicating the regions in the data domain where one model fits significantly better than the other. In a real-world problem of comparing {GAN} models, the test power of our new test matches that of the state-of-the-art test of relative goodness of fit, while being one order of magnitude faster.},
	journaltitle = {{arXiv}:1810.11630 [cs, stat]},
	author = {Jitkrittum, Wittawat and Kanagawa, Heishiro and Sangkloy, Patsorn and Hays, James and Schölkopf, Bernhard and Gretton, Arthur},
	urldate = {2018-11-08},
	date = {2018-10-27},
	eprinttype = {arxiv},
	eprint = {1810.11630},
	keywords = {46E22, 62G10, Computer Science - Machine Learning, G.3, I.2.6, Statistics - Machine Learning},
}

@article{choi_learning_2017,
	title = {Learning to Compose Task-Specific Tree Structures},
	url = {http://arxiv.org/abs/1707.02786},
	abstract = {For years, recursive neural networks ({RvNNs}) have been shown to be suitable for representing text into fixed-length vectors and achieved good performance on several natural language processing tasks. However, the main drawback of {RvNNs} is that they require structured input, which makes data preparation and model implementation hard. In this paper, we propose Gumbel Tree-{LSTM}, a novel tree-structured long short-term memory architecture that learns how to compose task-specific tree structures only from plain text data efficiently. Our model uses Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically and to calculate gradients of the discrete decision. We evaluate the proposed model on natural language inference and sentiment analysis, and show that our model outperforms or is at least comparable to previous models. We also find that our model converges significantly faster than other models.},
	journaltitle = {{arXiv}:1707.02786 [cs]},
	author = {Choi, Jihun and Yoo, Kang Min and Lee, Sang-goo},
	urldate = {2018-10-29},
	date = {2017-07-10},
	eprinttype = {arxiv},
	eprint = {1707.02786},
	keywords = {Computer Science - Computation and Language},
}

@article{chevalier-boisvert_babyai:_2018,
	title = {{BabyAI}: First Steps Towards Grounded Language Learning With a Human In the Loop},
	url = {http://arxiv.org/abs/1810.08272},
	shorttitle = {{BabyAI}},
	abstract = {Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the {BabyAI} research platform to support investigations towards including humans in the loop for grounded language learning. The {BabyAI} platform comprises an extensible suite of 19 levels of increasing difficulty. The levels gradually lead the agent towards acquiring a combinatorially rich synthetic language which is a proper subset of English. The platform also provides a heuristic expert agent for the purpose of simulating a human teacher. We report baseline results and estimate the amount of human involvement that would be required to train a neural network-based agent on some of the {BabyAI} levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample efficient when it comes to learning a language with compositional properties.},
	journaltitle = {{arXiv}:1810.08272 [cs]},
	author = {Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Lahlou, Salem and Willems, Lucas and Saharia, Chitwan and Nguyen, Thien Huu and Bengio, Yoshua},
	urldate = {2018-10-28},
	date = {2018-10-18},
	eprinttype = {arxiv},
	eprint = {1810.08272},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{ribeiro_semantically_2018,
	title = {Semantically equivalent adversarial rules for debugging nlp models},
	volume = {1},
	pages = {856--865},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	date = {2018},
}

@article{anonymous_uncertainty-guided_2018,
	title = {Uncertainty-guided Lifelong Learning in Bayesian Networks},
	url = {https://openreview.net/forum?id=SJMBM2RqKQ},
	abstract = {The ability to learn in a setting where tasks arrive in a sequence without access to previous task data is difficult for learning algorithms when restricted in capacity.  In this lifelong learning...},
	author = {Anonymous},
	urldate = {2018-10-03},
	date = {2018-09-27},
}

@article{futrell_rnns_2018,
	title = {{RNNs} as psycholinguistic subjects: Syntactic state and grammatical dependency},
	url = {http://arxiv.org/abs/1809.01329},
	shorttitle = {{RNNs} as psycholinguistic subjects},
	abstract = {Recurrent neural networks ({RNNs}) are the state of the art in sequence modeling for natural language. However, it remains poorly understood what grammatical characteristics of natural language they implicitly learn and represent as a consequence of optimizing the language modeling objective. Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent {RNN} behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior. We broadly test two publicly available long short-term memory ({LSTM}) English sequence models, and learn and test a new Japanese {LSTM}. We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans. Furthermore, none of our models learn the appropriate grammatical dependency configurations licensing reflexive pronouns or negative polarity items.},
	journaltitle = {{arXiv}:1809.01329 [cs]},
	author = {Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Levy, Roger},
	urldate = {2018-10-03},
	date = {2018-09-05},
	eprinttype = {arxiv},
	eprint = {1809.01329},
	keywords = {Computer Science - Computation and Language},
}

@article{linzen_what_2018,
	title = {What can linguistics and deep learning contribute to each other?},
	url = {http://arxiv.org/abs/1809.04179},
	abstract = {Joe Pater's target article calls for greater interaction between neural network research and linguistics. I expand on this call and show how such interaction can benefit both fields. Linguists can contribute to research on neural networks for language technologies by clearly delineating the linguistic capabilities that can be expected of such systems, and by constructing controlled experimental paradigms that can determine whether those desiderata have been met. In the other direction, neural networks can benefit the scientific study of language by providing infrastructure for modeling human sentence processing and for evaluating the necessity of particular innate constraints on language acquisition.},
	journaltitle = {{arXiv}:1809.04179 [cs]},
	author = {Linzen, Tal},
	urldate = {2018-10-03},
	date = {2018-09-11},
	eprinttype = {arxiv},
	eprint = {1809.04179},
	keywords = {Computer Science - Computation and Language},
}

@article{poerner_interpretable_2018,
	title = {Interpretable Textual Neuron Representations for {NLP}},
	url = {http://arxiv.org/abs/1809.07291},
	abstract = {Input optimization methods, such as Google Deep Dream, create interpretable representations of neurons for computer vision {DNNs}. We propose and evaluate ways of transferring this technology to {NLP}. Our results suggest that gradient ascent with a gumbel softmax layer produces n-gram representations that outperform naive corpus search in terms of target neuron activation. The representations highlight differences in syntax awareness between the language and visual models of the Imaginet architecture.},
	journaltitle = {{arXiv}:1809.07291 [cs]},
	author = {Poerner, Nina and Roth, Benjamin and Schütze, Hinrich},
	urldate = {2018-10-03},
	date = {2018-09-19},
	eprinttype = {arxiv},
	eprint = {1809.07291},
	keywords = {Computer Science - Computation and Language},
}

@article{htut_grammar_2018,
	title = {Grammar Induction with Neural Language Models: An Unusual Replication},
	url = {http://arxiv.org/abs/1808.10000},
	shorttitle = {Grammar Induction with Neural Language Models},
	abstract = {A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report near-state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction.},
	journaltitle = {{arXiv}:1808.10000 [cs]},
	author = {Htut, Phu Mon and Cho, Kyunghyun and Bowman, Samuel R.},
	urldate = {2018-10-03},
	date = {2018-08-29},
	eprinttype = {arxiv},
	eprint = {1808.10000},
	keywords = {Computer Science - Computation and Language},
}

@article{anonymous_visualizing_2018,
	title = {Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae},
	url = {https://openreview.net/forum?id=Skz3Q2CcFX},
	abstract = {Embeddings are a fundamental component of many modern machine learning and natural language processing models.
  Understanding them and visualizing them is essential for gathering insights about the...},
	author = {Anonymous},
	urldate = {2018-10-03},
	date = {2018-09-27},
}

@article{anonymous_systematic_2018,
	title = {Systematic Generalization: What Is Required and Can It Be Learned?},
	url = {https://openreview.net/forum?id=HkezXnA9YX},
	shorttitle = {Systematic Generalization},
	abstract = {Numerous models for grounded language understanding have been recently proposed, including (i) generic modules that can be used easily adapted to any given task with little adaptation and (ii)...},
	author = {Anonymous},
	urldate = {2018-10-03},
	date = {2018-09-27},
}

@article{anonymous_proposed_2018,
	title = {A Proposed Hierarchy of Deep Learning Tasks},
	url = {https://openreview.net/forum?id=B1g-X3RqKm},
	abstract = {As the pace of deep learning innovation accelerates, it becomes increasingly important to organize the space of problems by relative difficultly.  Looking to other fields for inspiration, we see...},
	author = {Anonymous},
	urldate = {2018-10-03},
	date = {2018-09-27},
}

@article{anonymous_empirical_2018,
	title = {An Empirical Study of Example Forgetting during Deep Neural Network Learning},
	url = {https://openreview.net/forum?id=BJlxm30cKm},
	abstract = {Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a...},
	author = {Anonymous},
	urldate = {2018-10-03},
	date = {2018-09-27},
}
@article{schrimpf_flexible_2018,
	title = {A Flexible Approach to Automated {RNN} Architecture Generation},
	url = {https://openreview.net/forum?id=BJDCPSJPM},
	abstract = {The process of designing neural architectures requires expert knowledge and extensive trial and error.
  While automated architecture search may simplify these requirements, the recurrent neural...},
	author = {Schrimpf, Martin and Merity, Stephen and Bradbury, James and Socher, Richard},
	urldate = {2018-10-03},
	date = {2018-02-12},
}

@article{anonymous_looking_2018,
	title = {Looking for {ELMo}'s friends: Sentence-Level Pretraining Beyond Language Modeling},
	url = {https://openreview.net/forum?id=Bkl87h09FX},
	shorttitle = {Looking for {ELMo}'s friends},
	abstract = {Work on the problem of contextualized word representation—the development of reusable neural network components for sentence understanding—has recently seen a  surge of progress centered on the...},
	author = {Anonymous},
	urldate = {2018-10-03},
	date = {2018-09-27},
}

@article{anonymous_learning_2018,
	title = {Learning More Interpretable, Backpropagation-Free Deep Architectures with Kernels},
	url = {https://openreview.net/forum?id=H1GLm2R9Km},
	abstract = {One can substitute each neuron in any neural network with a kernel machine and obtain a counterpart powered by kernel machines. The new network inherits the expressive power and architecture of the...},
	author = {Anonymous},
	urldate = {2018-10-03},
	date = {2018-09-27},
}

@article{zhang_language_2018,
	title = {Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis},
	url = {http://arxiv.org/abs/1809.10040},
	shorttitle = {Language Modeling Teaches You More Syntax than Translation Does},
	abstract = {Recent work using auxiliary prediction task classifiers to investigate the properties of {LSTM} representations has begun to shed light on why pretrained representations, like {ELMo} (Peters et al., 2018) and {CoVe} ({McCann} et al., 2017), are so beneficial for neural language understanding models. We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn. With this in mind, we compare four objectives---language modeling, translation, skip-thought, and autoencoding---on their ability to induce syntactic and part-of-speech information. We make a fair comparison between the tasks by holding constant the quantity and genre of the training data, as well as the {LSTM} architecture. We find that representations from language models consistently perform best on our syntactic auxiliary prediction tasks, even when trained on relatively small amounts of data. These results suggest that language modeling may be the best data-rich pretraining task for transfer learning applications requiring syntactic information. We also find that the representations from randomly-initialized, frozen {LSTMs} perform strikingly well on our syntactic auxiliary tasks, but this effect disappears when the amount of training data for the auxiliary tasks is reduced.},
	journaltitle = {{arXiv}:1809.10040 [cs]},
	author = {Zhang, Kelly W. and Bowman, Samuel R.},
	urldate = {2018-10-02},
	date = {2018-09-26},
	eprinttype = {arxiv},
	eprint = {1809.10040},
	keywords = {Computer Science - Computation and Language},
}

@article{wang_identifying_2018,
	title = {Identifying Generalization Properties in Neural Networks},
	url = {http://arxiv.org/abs/1809.07402},
	abstract = {While it has not yet been proven, empirical evidence suggests that model generalization is related to local properties of the optima which can be described via the Hessian. We connect model generalization with the local property of a solution under the {PAC}-Bayes paradigm. In particular, we prove that model generalization ability is related to the Hessian, the higher-order "smoothness" terms characterized by the Lipschitz constant of the Hessian, and the scales of the parameters. Guided by the proof, we propose a metric to score the generalization capability of the model, as well as an algorithm that optimizes the perturbed model accordingly.},
	journaltitle = {{arXiv}:1809.07402 [cs, stat]},
	author = {Wang, Huan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
	urldate = {2018-09-24},
	date = {2018-09-19},
	eprinttype = {arxiv},
	eprint = {1809.07402},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{gal_theoretically_2016,
	title = {A Theoretically Grounded Application of Dropout in Recurrent Neural Networks},
	url = {http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf},
	pages = {1019--1027},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2018-09-17},
	date = {2016},
}

@article{giulianelli_under_2018,
	title = {Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information},
	url = {http://arxiv.org/abs/1808.08079},
	shorttitle = {Under the Hood},
	abstract = {How do neural language models keep track of number agreement between subject and verb? We show that `diagnostic classifiers', trained to predict number from the internal states of a language model, provide a detailed understanding of how, when, and where this information is represented. Moreover, they give us insight into when and where number information is corrupted in cases where the language model ends up making agreement errors. To demonstrate the causal role played by the representations we find, we then use agreement information to influence the course of the {LSTM} during the processing of difficult sentences. Results from such an intervention reveal a large increase in the language model's accuracy. Together, these results show that diagnostic classifiers give us an unrivalled detailed look into the representation of linguistic information in neural models, and demonstrate that this knowledge can be used to improve their performance.},
	journaltitle = {{arXiv}:1808.08079 [cs]},
	author = {Giulianelli, Mario and Harding, Jack and Mohnert, Florian and Hupkes, Dieuwke and Zuidema, Willem},
	urldate = {2018-08-28},
	date = {2018-08-24},
	eprinttype = {arxiv},
	eprint = {1808.08079},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{feng_pathologies_2018,
	title = {Pathologies of Neural Models Make Interpretations Difficult},
	url = {http://arxiv.org/abs/1804.07781},
	abstract = {Model interpretability is a crucial problem in neural networks. Existing interpretation methods highlight salient input features, often determining each feature's importance based on gradient information from the model. We instead remove the least influential words, one at a time, from language inputs. This exposes pathological model behavior on language tasks: models produce high confidence values for reduced inputs, even when humans find them nonsensical. We examine the reasons for this behavior and suggest methods of mitigation. Our results have implications for gradient-based interpretation methods, showing that determining word importance using a model's gradient often does not align with humans' perceived importance of that word. We propose a simple entropy regularization technique that mitigates these issues without affecting performance on clean examples.},
	journaltitle = {{arXiv}:1804.07781 [cs]},
	author = {Feng, Shi and Wallace, Eric and Grissom {II}, Alvin and Iyyer, Mohit and Rodriguez, Pedro and Boyd-Graber, Jordan},
	urldate = {2018-08-22},
	date = {2018-04-20},
	eprinttype = {arxiv},
	eprint = {1804.07781},
	keywords = {Computer Science - Computation and Language},
}

@article{liao_surprising_2018,
	title = {A Surprising Linear Relationship Predicts Test Performance in Deep Networks},
	url = {http://arxiv.org/abs/1807.09659},
	abstract = {Given two networks with the same training loss on a dataset, when would they have drastically different test losses and errors? Better understanding of this question of generalization may improve practical applications of deep networks. In this paper we show that with cross-entropy loss it is surprisingly simple to induce significantly different generalization performances for two networks that have the same architecture, the same meta parameters and the same training error: one can either pretrain the networks with different levels of "corrupted" data or simply initialize the networks with weights of different Gaussian standard deviations. A corollary of recent theoretical results on overfitting shows that these effects are due to an intrinsic problem of measuring test performance with a cross-entropy/exponential-type loss, which can be decomposed into two components both minimized by {SGD} -- one of which is not related to expected classification performance. However, if we factor out this component of the loss, a linear relationship emerges between training and test losses. Under this transformation, classical generalization bounds are surprisingly tight: the empirical/training loss is very close to the expected/test loss. Furthermore, the empirical relation between classification error and normalized cross-entropy loss seem to be approximately monotonic},
	journaltitle = {{arXiv}:1807.09659 [cs, stat]},
	author = {Liao, Qianli and Miranda, Brando and Banburski, Andrzej and Hidary, Jack and Poggio, Tomaso},
	urldate = {2018-08-14},
	date = {2018-07-25},
	eprinttype = {arxiv},
	eprint = {1807.09659},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chen_l-shapley_2018,
	title = {L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data},
	url = {http://arxiv.org/abs/1808.02610},
	shorttitle = {L-Shapley and C-Shapley},
	abstract = {We study instancewise feature importance scoring as a method for model interpretation. Any such method yields, for each predicted instance, a vector of importance scores associated with the feature vector. Methods based on the Shapley score have been proposed as a fair way of computing feature attributions of this kind, but incur an exponential complexity in the number of features. This combinatorial explosion arises from the definition of the Shapley value and prevents these methods from being scalable to large data sets and complex models. We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization. In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring. We establish the relationship of our methods to the Shapley value and another closely related concept known as the Myerson value from cooperative game theory. We demonstrate on both language and image data that our algorithms compare favorably with other methods for model interpretation.},
	journaltitle = {{arXiv}:1808.02610 [cs, stat]},
	author = {Chen, Jianbo and Song, Le and Wainwright, Martin J. and Jordan, Michael I.},
	urldate = {2018-08-14},
	date = {2018-08-07},
	eprinttype = {arxiv},
	eprint = {1808.02610},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{selvaraju_choose_2018,
	title = {Choose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance},
	url = {http://arxiv.org/abs/1808.02861},
	shorttitle = {Choose Your Neuron},
	abstract = {Individual neurons in convolutional neural networks supervised for image-level classification tasks have been shown to implicitly learn semantically meaningful concepts ranging from simple textures and shapes to whole or partial objects - forming a "dictionary" of concepts acquired through the learning process. In this work we introduce a simple, efficient zero-shot learning approach based on this observation. Our approach, which we call Neuron Importance-{AwareWeight} Transfer ({NIWT}), learns to map domain knowledge about novel "unseen" classes onto this dictionary of learned concepts and then optimizes for network parameters that can effectively combine these concepts - essentially learning classifiers by discovering and composing learned semantic concepts in deep networks. Our approach shows improvements over previous approaches on the {CUBirds} and {AWA}2 generalized zero-shot learning benchmarks. We demonstrate our approach on a diverse set of semantic inputs as external domain knowledge including attributes and natural language captions. Moreover by learning inverse mappings, {NIWT} can provide visual and textual explanations for the predictions made by the newly learned classifiers and provide neuron names. Our code is available at https://github.com/ramprs/neuron-importance-zsl.},
	journaltitle = {{arXiv}:1808.02861 [cs]},
	author = {Selvaraju, Ramprasaath R. and Chattopadhyay, Prithvijit and Elhoseiny, Mohamed and Sharma, Tilak and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
	urldate = {2018-08-09},
	date = {2018-08-08},
	eprinttype = {arxiv},
	eprint = {1808.02861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{williams_latent_2018,
	title = {Do latent tree learning models identify meaningful structure in sentences?},
	volume = {6},
	pages = {253--267},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	author = {Williams, Adina and Drozdov, Andrew and Bowman, Samuel R.},
	date = {2018},
}

@article{cotterell_are_2018,
	title = {Are All Languages Equally Hard to Language-Model?},
	journaltitle = {{arXiv} preprint {arXiv}:1806.03743},
	author = {Cotterell, Ryan and Mielke, Sebastian J. and Eisner, Jason and Roark, Brian},
	date = {2018},
}

@inproceedings{deutsch_distributional_2018,
	title = {A Distributional and Orthographic Aggregation Model for English Derivational Morphology},
	volume = {1},
	pages = {1938--1947},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	author = {Deutsch, Daniel and Hewitt, John and Roth, Dan},
	date = {2018},
}

@article{novak_sensitivity_2018,
	title = {Sensitivity and Generalization in Neural Networks: an Empirical Study},
	url = {http://arxiv.org/abs/1802.08760},
	shorttitle = {Sensitivity and Generalization in Neural Networks},
	abstract = {In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with various fully-connected architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets. We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the norm of the input-output Jacobian of the network, and that it correlates well with generalization. We further establish that factors associated with poor generalization \$-\$ such as full-batch training or using random labels \$-\$ correspond to lower robustness, while factors associated with good generalization \$-\$ such as data augmentation and {ReLU} non-linearities \$-\$ give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points.},
	journaltitle = {{arXiv}:1802.08760 [cs, stat]},
	author = {Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	urldate = {2018-07-09},
	date = {2018-02-23},
	eprinttype = {arxiv},
	eprint = {1802.08760},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{morcos_insights_2018,
	title = {Insights on representational similarity in neural networks with canonical correlation},
	url = {http://arxiv.org/abs/1806.05759},
	abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted {CCA} (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of {SVCCA}, a recently proposed method. We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of {CNNs}, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of {RNNs}, across both training and sequential timesteps, finding that {RNNs} converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of {CNNs} and {RNNs}, and demonstrate the utility of using {CCA} to understand representations.},
	journaltitle = {{arXiv}:1806.05759 [cs, stat]},
	author = {Morcos, Ari S. and Raghu, Maithra and Bengio, Samy},
	urldate = {2018-06-20},
	date = {2018-06-14},
	eprinttype = {arxiv},
	eprint = {1806.05759},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{adi_fine-grained_2016,
	title = {Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks},
	url = {http://arxiv.org/abs/1608.04207},
	abstract = {There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as {LSTMs}. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations.},
	journaltitle = {{arXiv}:1608.04207 [cs]},
	author = {Adi, Yossi and Kermany, Einat and Belinkov, Yonatan and Lavi, Ofer and Goldberg, Yoav},
	urldate = {2018-06-12},
	date = {2016-08-15},
	eprinttype = {arxiv},
	eprint = {1608.04207},
	keywords = {Computer Science - Computation and Language},
}

@article{li_convergent_2015,
	title = {Convergent Learning: Do different neural networks learn the same representations?},
	url = {http://arxiv.org/abs/1511.07543},
	shorttitle = {Convergent Learning},
	abstract = {Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units.},
	journaltitle = {{arXiv}:1511.07543 [cs]},
	author = {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
	urldate = {2018-06-11},
	date = {2015-11-23},
	eprinttype = {arxiv},
	eprint = {1511.07543},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{raghu_svcca:_2017,
	title = {{SVCCA}: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability},
	url = {http://arxiv.org/abs/1706.05806},
	shorttitle = {{SVCCA}},
	abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis ({SVCCA}), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less. Code: https://github.com/google/svcca/},
	journaltitle = {{arXiv}:1706.05806 [cs, stat]},
	author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
	urldate = {2018-06-08},
	date = {2017-06-19},
	eprinttype = {arxiv},
	eprint = {1706.05806},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{liu_lstms_2018,
	title = {{LSTMs} Exploit Linguistic Attributes of Data},
	url = {http://arxiv.org/abs/1805.11653},
	abstract = {While recurrent neural networks have found success in a variety of natural language processing applications, they are general models of sequential data. We investigate how the properties of natural language data affect an {LSTM}'s ability to learn a nonlinguistic task: recalling elements from its input. We find that models trained on natural language data are able to recall tokens from much longer sequences than models trained on non-language sequential data. Furthermore, we show that the {LSTM} learns to solve the memorization task by explicitly using a subset of its neurons to count timesteps in the input. We hypothesize that the patterns and structure in natural language data enable {LSTMs} to learn by providing approximate ways of reducing loss, but understanding the effect of different training data on the learnability of {LSTMs} remains an open question.},
	journaltitle = {{arXiv}:1805.11653 [cs]},
	author = {Liu, Nelson F. and Levy, Omer and Schwartz, Roy and Tan, Chenhao and Smith, Noah A.},
	urldate = {2018-06-04},
	date = {2018-05-29},
	eprinttype = {arxiv},
	eprint = {1805.11653},
	keywords = {Computer Science - Computation and Language},
}

@article{levy_long_2018,
	title = {Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum},
	url = {http://arxiv.org/abs/1805.03716},
	abstract = {{LSTMs} were introduced to combat vanishing gradients in simple {RNNs} by augmenting them with gated additive recurrent connections. We present an alternative view to explain the success of {LSTMs}: the gates themselves are versatile recurrent models that provide more representational power than previously appreciated. We do this by decoupling the {LSTM}'s gates from the embedded simple {RNN}, producing a new class of {RNNs} where the recurrence computes an element-wise weighted sum of context-independent functions of the input. Ablations on a range of problems demonstrate that the gating mechanism alone performs as well as an {LSTM} in most settings, strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients.},
	journaltitle = {{arXiv}:1805.03716 [cs, stat]},
	author = {Levy, Omer and Lee, Kenton and {FitzGerald}, Nicholas and Zettlemoyer, Luke},
	urldate = {2018-06-04},
	date = {2018-05-09},
	eprinttype = {arxiv},
	eprint = {1805.03716},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning},
}

@article{gaddy_whats_2018,
	title = {What's Going On in Neural Constituency Parsers? An Analysis},
	url = {http://arxiv.org/abs/1804.07853},
	shorttitle = {What's Going On in Neural Constituency Parsers?},
	abstract = {A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on {PTB}) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.},
	journaltitle = {{arXiv}:1804.07853 [cs]},
	author = {Gaddy, David and Stern, Mitchell and Klein, Dan},
	urldate = {2018-06-04},
	date = {2018-04-20},
	eprinttype = {arxiv},
	eprint = {1804.07853},
	keywords = {Computer Science - Computation and Language},
}

@article{khandelwal_sharp_2018,
	title = {Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context},
	url = {http://arxiv.org/abs/1805.04623},
	shorttitle = {Sharp Nearby, Fuzzy Far Away},
	abstract = {We know very little about how neural language models ({LM}) use prior linguistic context. In this paper, we investigate the role of context in an {LSTM} {LM}, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and {WikiText}-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the {LSTM} to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural {LMs} use their context, but also sheds light on recent success from cache-based models.},
	journaltitle = {{arXiv}:1805.04623 [cs]},
	author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
	urldate = {2018-06-04},
	date = {2018-05-11},
	eprinttype = {arxiv},
	eprint = {1805.04623},
	keywords = {Computer Science - Computation and Language},
}

@article{santurkar_how_2018,
	title = {How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)},
	url = {http://arxiv.org/abs/1805.11604},
	shorttitle = {How Does Batch Normalization Help Optimization?},
	abstract = {Batch Normalization ({BatchNorm}) is a widely adopted technique that enables faster and more stable training of deep neural networks ({DNNs}). Despite its pervasiveness, the exact reasons for {BatchNorm}'s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of {BatchNorm}. Instead, we uncover a more fundamental impact of {BatchNorm} on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. These findings bring us closer to a true understanding of our {DNN} training toolkit.},
	journaltitle = {{arXiv}:1805.11604 [cs, stat]},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
	urldate = {2018-06-04},
	date = {2018-05-29},
	eprinttype = {arxiv},
	eprint = {1805.11604},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{furlanello_born_2018,
	title = {Born Again Neural Networks},
	url = {http://arxiv.org/abs/1805.04770},
	abstract = {Knowledge distillation ({KD}) consists of transferring knowledge from one machine learning model (the teacher\}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. \%we desire a compact model with performance close to the teacher's. We study {KD} from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these \{Born-Again Networks ({BANs}), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with {BANs} based on {DenseNets} demonstrate state-of-the-art performance on the {CIFAR}-10 (3.5\%) and {CIFAR}-100 (15.5\%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max ({CWTM}) and (ii) Dark Knowledge with Permuted Predictions ({DKPP}). Both methods elucidate the essential components of {KD}, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between {DenseNets} and {ResNets} in either direction.},
	journaltitle = {{arXiv}:1805.04770 [cs, stat]},
	author = {Furlanello, Tommaso and Lipton, Zachary C. and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
	urldate = {2018-05-29},
	date = {2018-05-12},
	eprinttype = {arxiv},
	eprint = {1805.04770},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
}

@article{liang_adding_2018,
	title = {Adding One Neuron Can Eliminate All Bad Local Minima},
	url = {http://arxiv.org/abs/1805.08671},
	abstract = {One of the main difficulties in analyzing neural networks is the non-convexity of the loss function which may have many bad local minima. In this paper, we study the landscape of neural networks for binary classification tasks. Under mild assumptions, we prove that after adding one special neuron with a skip connection to the output, or one special neuron per layer, every local minimum is a global minimum.},
	journaltitle = {{arXiv}:1805.08671 [cs, stat]},
	author = {Liang, Shiyu and Sun, Ruoyu and Lee, Jason D. and Srikant, R.},
	urldate = {2018-05-23},
	date = {2018-05-22},
	eprinttype = {arxiv},
	eprint = {1805.08671},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{buckman_neural_2018,
	title = {Neural Lattice Language Models},
	url = {http://arxiv.org/abs/1803.05071},
	abstract = {In this work, we propose a new language modeling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities: neural lattice language models. These models construct a lattice of possible paths through a sentence and marginalize across this lattice to calculate sequence probabilities or optimize parameters. This approach allows us to seamlessly incorporate linguistic intuitions - including polysemy and existence of multi-word lexical items - into our language model. Experiments on multiple language modeling tasks show that English neural lattice language models that utilize polysemous embeddings are able to improve perplexity by 9.95\% relative to a word-level baseline, and that a Chinese model that handles multi-character tokens is able to improve perplexity by 20.94\% relative to a character-level baseline.},
	journaltitle = {{arXiv}:1803.05071 [cs]},
	author = {Buckman, Jacob and Neubig, Graham},
	urldate = {2018-05-17},
	date = {2018-03-13},
	eprinttype = {arxiv},
	eprint = {1803.05071},
	keywords = {Computer Science - Computation and Language},
}

@article{blevins_deep_2018,
	title = {Deep {RNNs} Encode Soft Hierarchical Syntax},
	url = {http://arxiv.org/abs/1805.04218},
	abstract = {We present a set of experiments to demonstrate that deep recurrent neural networks ({RNNs}) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision.},
	journaltitle = {{arXiv}:1805.04218 [cs]},
	author = {Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
	urldate = {2018-05-16},
	date = {2018-05-10},
	eprinttype = {arxiv},
	eprint = {1805.04218},
	keywords = {Computer Science - Computation and Language},
}

@article{weiss_practical_2018,
	title = {On the Practical Computational Power of Finite Precision {RNNs} for Language Recognition},
	url = {http://arxiv.org/abs/1805.04908},
	abstract = {While Recurrent Neural Networks ({RNNs}) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of {RNNs} with finite precision whose computation time is linear in the input length. Under these limitations, we show that different {RNN} variants have different computational power. In particular, we show that the {LSTM} and the Elman-{RNN} with {ReLU} activation are strictly stronger than the {RNN} with a squashing activation and the {GRU}. This is achieved because {LSTMs} and {ReLU}-{RNNs} can easily implement counting behavior. We show empirically that the {LSTM} does indeed learn to effectively use the counting mechanism.},
	journaltitle = {{arXiv}:1805.04908 [cs, stat]},
	author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
	urldate = {2018-05-15},
	date = {2018-05-13},
	eprinttype = {arxiv},
	eprint = {1805.04908},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning},
}

@article{camacho-collados_word_2018,
	title = {From Word to Sense Embeddings: A Survey on Vector Representations of Meaning},
	url = {http://arxiv.org/abs/1805.04032},
	shorttitle = {From Word to Sense Embeddings},
	abstract = {Over the past years, distributed representations have proven effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey is focused on semantic representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their main limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and provides an analysis of five important aspects: interpretability, sense granularity, adaptability to different domains, compositionality and integration into downstream applications.},
	journaltitle = {{arXiv}:1805.04032 [cs]},
	author = {Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
	urldate = {2018-05-15},
	date = {2018-05-10},
	eprinttype = {arxiv},
	eprint = {1805.04032},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{mensch_differentiable_2018,
	title = {Differentiable Dynamic Programming for Structured Prediction and Attention},
	url = {http://arxiv.org/abs/1802.03676},
	abstract = {Dynamic programming ({DP}) solves a variety of structured combinatorial problems by iteratively breaking them down into smaller subproblems. In spite of their versatility, {DP} algorithms are usually non-differentiable, which hampers their use as a layer in neural networks trained by backpropagation. To address this issue, we propose to smooth the max operator in the dynamic programming recursion, using a strongly convex regularizer. This allows to relax both the optimal value and solution of the original combinatorial problem, and turns a broad class of {DP} algorithms into differentiable operators. Theoretically, we provide a new probabilistic perspective on backpropagating through these {DP} operators, and relate them to inference in graphical models. We derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed {DTW} algorithm for time-series alignment. We showcase these instantiations on two structured prediction tasks and on structured and sparse attention for neural machine translation.},
	journaltitle = {{arXiv}:1802.03676 [cs, stat]},
	author = {Mensch, Arthur and Blondel, Mathieu},
	urldate = {2018-05-15},
	date = {2018-02-10},
	eprinttype = {arxiv},
	eprint = {1802.03676},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{radford_learning_2017,
	title = {Learning to Generate Reviews and Discovering Sentiment},
	url = {http://arxiv.org/abs/1704.01444},
	abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
	journaltitle = {{arXiv}:1704.01444 [cs]},
	author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
	urldate = {2018-05-14},
	date = {2017-04-05},
	eprinttype = {arxiv},
	eprint = {1704.01444},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}
@article{raghu_expressive_2016,
	title = {On the Expressive Power of Deep Neural Networks},
	url = {http://arxiv.org/abs/1606.05336},
	abstract = {We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings can be summarized as follows: (1) The complexity of the computed function grows exponentially with depth. (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights. (3) Regularizing on trajectory length (trajectory regularization) is a simpler alternative to batch normalization, with the same performance.},
	journaltitle = {{arXiv}:1606.05336 [cs, stat]},
	author = {Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
	urldate = {2018-05-14},
	date = {2016-06-16},
	eprinttype = {arxiv},
	eprint = {1606.05336},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
}

@article{shwartz-ziv_opening_2017,
	title = {Opening the Black Box of Deep Neural Networks via Information},
	url = {http://arxiv.org/abs/1703.00810},
	abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks ({DNNs}) or their inner organization. Previous work proposed to analyze {DNNs} in the {\textbackslash}textit\{Information Plane\}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck ({IB}) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of {DNNs}. Our main results are: (i) most of the training epochs in standard {DL} are spent on \{{\textbackslash}emph compression\} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent ({SGD}) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck ({IB}) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the {IB} self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
	journaltitle = {{arXiv}:1703.00810 [cs]},
	author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
	urldate = {2018-05-14},
	date = {2017-03-02},
	eprinttype = {arxiv},
	eprint = {1703.00810},
	keywords = {Computer Science - Learning},
}

@article{arpit_closer_2017,
	title = {A Closer Look at Memorization in Deep Networks},
	url = {http://arxiv.org/abs/1706.05394},
	abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks ({DNNs}) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade {DNN} training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
	journaltitle = {{arXiv}:1706.05394 [cs, stat]},
	author = {Arpit, Devansh and Jastrzębski, Stanisław and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
	urldate = {2018-05-10},
	date = {2017-06-16},
	eprinttype = {arxiv},
	eprint = {1706.05394},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{gulordava_colorless_2018,
	title = {Colorless green recurrent networks dream hierarchically},
	url = {http://arxiv.org/abs/1803.11138},
	abstract = {Recurrent neural networks ({RNNs}) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate here to what extent {RNNs} learn to track abstract hierarchical syntactic structure. We test whether {RNNs} trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where {RNNs} cannot rely on semantic or lexical cues ("The colorless green ideas I ate with the chair sleep furiously"), and, for Italian, we compare model performance to human intuitions. Our language-model-trained {RNNs} make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that {RNNs} are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.},
	journaltitle = {{arXiv}:1803.11138 [cs]},
	author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
	urldate = {2018-05-10},
	date = {2018-03-29},
	eprinttype = {arxiv},
	eprint = {1803.11138},
	keywords = {Computer Science - Computation and Language},
}

@article{conneau_what_2018,
	title = {What you can cram into a single vector: Probing sentence embeddings for linguistic properties},
	url = {http://arxiv.org/abs/1805.01070},
	shorttitle = {What you can cram into a single vector},
	abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. "Downstream" tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
	journaltitle = {{arXiv}:1805.01070 [cs]},
	author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Loïc and Baroni, Marco},
	urldate = {2018-05-08},
	date = {2018-05-02},
	eprinttype = {arxiv},
	eprint = {1805.01070},
	keywords = {Computer Science - Computation and Language},
}

@article{morcos_importance_2018,
	title = {On the importance of single directions for generalization},
	url = {http://arxiv.org/abs/1803.06959},
	abstract = {Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.},
	journaltitle = {{arXiv}:1803.06959 [cs, stat]},
	author = {Morcos, Ari S. and Barrett, David G. T. and Rabinowitz, Neil C. and Botvinick, Matthew},
	urldate = {2018-05-04},
	date = {2018-03-19},
	eprinttype = {arxiv},
	eprint = {1803.06959},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@online{noauthor_understanding_nodate,
	title = {Understanding Deep Learning through Neuron Deletion},
	url = {https://deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/},
	abstract = {Deep neural networks are composed of many individual neurons, which combine in complex and counterintuitive ways to solve challenging tasks, ranging from machine translation to Go. This complexity grants neural networks their power but also earns them their reputation as confusing and opaque black boxes. Understanding how deep neural networks function is critical for explaining their decisions and enabling us to build more powerful systems. For instance, imagine the difficulty of trying to build a clock without understanding how individual gears fit together. One approach to understanding neural networks, both in neuroscience and deep learning, is to investigate the role of individual neurons, especially those which are easily interpretable.},
	titleaddon = {{DeepMind}},
	urldate = {2018-04-30},
}

@article{ren_learning_2018,
	title = {Learning to Reweight Examples for Robust Deep Learning},
	url = {http://arxiv.org/abs/1803.09050},
	abstract = {Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.},
	journaltitle = {{arXiv}:1803.09050 [cs, stat]},
	author = {Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
	urldate = {2018-04-27},
	date = {2018-03-23},
	eprinttype = {arxiv},
	eprint = {1803.09050},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{frankle_lottery_2018,
	title = {The Lottery Ticket Hypothesis: Training Pruned Neural Networks},
	url = {http://arxiv.org/abs/1803.03635},
	shorttitle = {The Lottery Ticket Hypothesis},
	abstract = {Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the "lottery ticket hypothesis," proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these "lottery tickets," meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization. This paper conducts a series of experiments with {XOR} and {MNIST} that support the lottery ticket hypothesis. In particular, we identify these fortuitously-initialized subcomponents by pruning low-magnitude weights from trained networks. We then demonstrate that these subcomponents can be successfully retrained in isolation so long as the subnetworks are given the same initializations as they had at the beginning of the training process. Initialized as such, these small networks reliably converge successfully, often faster than the original network at the same level of accuracy. However, when these subcomponents are randomly reinitialized or rearranged, they perform worse than the original network. In other words, large networks that train successfully contain small subnetworks with initializations conducive to optimization. The lottery ticket hypothesis and its connection to pruning are a step toward developing architectures, initializations, and training strategies that make it possible to solve the same problems with much smaller networks.},
	journaltitle = {{arXiv}:1803.03635 [cs]},
	author = {Frankle, Jonathan and Carbin, Michael},
	urldate = {2018-04-23},
	date = {2018-03-09},
	eprinttype = {arxiv},
	eprint = {1803.03635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{rabinowitz_machine_2018,
	title = {Machine Theory of Mind},
	url = {http://arxiv.org/abs/1802.07740},
	abstract = {Theory of mind ({ToM}; Premack \& Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a {ToMnet} -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the {ToMnet} to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic {ToM} tasks such as the "Sally-Anne" test (Wimmer \& Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent {AI} systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable {AI}.},
	journaltitle = {{arXiv}:1802.07740 [cs]},
	author = {Rabinowitz, Neil C. and Perbet, Frank and Song, H. Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
	urldate = {2018-04-23},
	date = {2018-02-21},
	eprinttype = {arxiv},
	eprint = {1802.07740},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{kottur_natural_2017,
	title = {Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog},
	url = {http://arxiv.org/abs/1706.08502},
	abstract = {A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision! In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.},
	journaltitle = {{arXiv}:1706.08502 [cs]},
	author = {Kottur, Satwik and Moura, José M. F. and Lee, Stefan and Batra, Dhruv},
	urldate = {2018-04-23},
	date = {2017-06-26},
	eprinttype = {arxiv},
	eprint = {1706.08502},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@article{lazaridou_multi-agent_2016,
	title = {Multi-Agent Cooperation and the Emergence of (Natural) Language},
	url = {http://arxiv.org/abs/1612.07182},
	abstract = {The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the "word meanings" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively.},
	journaltitle = {{arXiv}:1612.07182 [cs]},
	author = {Lazaridou, Angeliki and Peysakhovich, Alexander and Baroni, Marco},
	urldate = {2018-04-23},
	date = {2016-12-21},
	eprinttype = {arxiv},
	eprint = {1612.07182},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Science and Game Theory, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Multiagent Systems},
}

@article{he_learning_2017,
	title = {Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings},
	url = {http://arxiv.org/abs/1704.07130},
	abstract = {We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.},
	journaltitle = {{arXiv}:1704.07130 [cs]},
	author = {He, He and Balakrishnan, Anusha and Eric, Mihail and Liang, Percy},
	urldate = {2018-04-23},
	date = {2017-04-24},
	eprinttype = {arxiv},
	eprint = {1704.07130},
	keywords = {Computer Science - Computation and Language},
}

@article{dreossi_semantic_2018,
	title = {Semantic Adversarial Deep Learning},
	url = {http://arxiv.org/abs/1804.07045},
	abstract = {Fueled by massive amounts of data, models produced by machine-learning ({ML}) algorithms, especially deep neural networks, are being used in diverse domains where trustworthiness is a concern, including automotive systems, finance, health care, natural language processing, and malware detection. Of particular concern is the use of {ML} algorithms in cyber-physical systems ({CPS}), such as self-driving cars and aviation, where an adversary can cause serious consequences. However, existing approaches to generating adversarial examples and devising robust {ML} algorithms mostly ignore the semantics and context of the overall system containing the {ML} component. For example, in an autonomous vehicle using deep learning for perception, not every adversarial example for the neural network might lead to a harmful consequence. Moreover, one may want to prioritize the search for adversarial examples towards those that significantly modify the desired semantics of the overall system. Along the same lines, existing algorithms for constructing robust {ML} algorithms ignore the specification of the overall system. In this paper, we argue that the semantics and specification of the overall system has a crucial role to play in this line of research. We present preliminary research results that support this claim.},
	journaltitle = {{arXiv}:1804.07045 [cs, stat]},
	author = {Dreossi, Tommaso and Jha, Somesh and Seshia, Sanjit A.},
	urldate = {2018-04-21},
	date = {2018-04-19},
	eprinttype = {arxiv},
	eprint = {1804.07045},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{zhou_compressibility_2018,
	title = {Compressibility and Generalization in Large-Scale Deep Learning},
	url = {https://arxiv.org/abs/1804.05862v1},
	author = {Zhou, Wenda and Veitch, Victor and Austern, Morgane and Adams, Ryan P. and Orbanz, Peter},
	urldate = {2018-04-19},
	date = {2018-04-16},
	langid = {english},
}

@article{ma_teacher_2018,
	title = {Teacher Improves Learning by Selecting a Training Subset},
	url = {http://arxiv.org/abs/1802.08946},
	abstract = {We call a learner super-teachable if a teacher can trim down an iid training set while making the learner learn even better. We provide sharp super-teaching guarantees on two learners: the maximum likelihood estimator for the mean of a Gaussian, and the large margin classifier in 1D. For general learners, we provide a mixed-integer nonlinear programming-based algorithm to find a super teaching set. Empirical experiments show that our algorithm is able to find good super-teaching sets for both regression and classification problems.},
	journaltitle = {{arXiv}:1802.08946 [cs, stat]},
	author = {Ma, Yuzhe and Nowak, Robert and Rigollet, Philippe and Zhang, Xuezhou and Zhu, Xiaojin},
	urldate = {2018-04-15},
	date = {2018-02-24},
	eprinttype = {arxiv},
	eprint = {1802.08946},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
}

@article{takada_independently_2017,
	title = {Independently Interpretable Lasso: A New Regularizer for Sparse Regression with Uncorrelated Variables},
	url = {http://arxiv.org/abs/1711.01796},
	shorttitle = {Independently Interpretable Lasso},
	abstract = {Sparse regularization such as \${\textbackslash}ell\_1\$ regularization is a quite powerful and widely used strategy for high dimensional learning problems. The effectiveness of sparse regularization has been supported practically and theoretically by several studies. However, one of the biggest issues in sparse regularization is that its performance is quite sensitive to correlations between features. Ordinary \${\textbackslash}ell\_1\$ regularization can select variables correlated with each other, which results in deterioration of not only its generalization error but also interpretability. In this paper, we propose a new regularization method, "Independently Interpretable Lasso" ({IILasso}). Our proposed regularizer suppresses selecting correlated variables, and thus each active variable independently affects the objective variable in the model. Hence, we can interpret regression coefficients intuitively and also improve the performance by avoiding overfitting. We analyze theoretical property of {IILasso} and show that the proposed method is much advantageous for its sign recovery and achieves almost minimax optimal convergence rate. Synthetic and real data analyses also indicate the effectiveness of {IILasso}.},
	journaltitle = {{arXiv}:1711.01796 [stat]},
	author = {Takada, Masaaki and Suzuki, Taiji and Fujisawa, Hironori},
	urldate = {2018-04-15},
	date = {2017-11-06},
	eprinttype = {arxiv},
	eprint = {1711.01796},
	keywords = {Statistics - Machine Learning},
}

@article{graves_adaptive_2016,
	title = {Adaptive Computation Time for Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1603.08983},
	abstract = {This paper introduces Adaptive Computation Time ({ACT}), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. {ACT} requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of {ACT}, which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case {ACT} does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that {ACT} or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.},
	journaltitle = {{arXiv}:1603.08983 [cs]},
	author = {Graves, Alex},
	urldate = {2018-03-28},
	date = {2016-03-29},
	eprinttype = {arxiv},
	eprint = {1603.08983},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{fojo_comparing_2018,
	title = {Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1803.08165},
	abstract = {Adaptive Computation Time for Recurrent Neural Networks ({ACT}) is one of the most promising architectures for variable computation. {ACT} adapts to the input sequence by being able to look at each sample more than once, and learn how many times it should do it. In this paper, we compare {ACT} to Repeat-{RNN}, a novel architecture based on repeating each sample a fixed number of times. We found surprising results, where Repeat-{RNN} performs as good as {ACT} in the selected tasks. Source code in {TensorFlow} and {PyTorch} is publicly available at https://imatge-upc.github.io/danifojo-2018-repeatrnn/},
	journaltitle = {{arXiv}:1803.08165 [cs]},
	author = {Fojo, Daniel and Campos, Víctor and Giro-i-Nieto, Xavier},
	urldate = {2018-03-28},
	date = {2018-03-21},
	eprinttype = {arxiv},
	eprint = {1803.08165},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{yang_breaking_2017,
	title = {Breaking the Softmax Bottleneck: A High-Rank {RNN} Language Model},
	url = {http://arxiv.org/abs/1711.03953},
	shorttitle = {Breaking the Softmax Bottleneck},
	abstract = {We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and {WikiText}-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.},
	journaltitle = {{arXiv}:1711.03953 [cs]},
	author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.},
	urldate = {2018-03-27},
	date = {2017-11-10},
	eprinttype = {arxiv},
	eprint = {1711.03953},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
}

@article{greff_lstm:_2017,
	title = {{LSTM}: A Search Space Odyssey},
	volume = {28},
	issn = {2162-237X},
	doi = {10.1109/TNNLS.2016.2582924},
	shorttitle = {{LSTM}},
	abstract = {Several variants of the long short-term memory ({LSTM}) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical {LSTM} variants. In this paper, we present the first large-scale analysis of eight {LSTM} variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all {LSTM} variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional {ANalysis} Of {VAriance} framework. In total, we summarize the results of 5400 experimental runs (≈15 years of {CPU} time), which makes our study the largest of its kind on {LSTM} networks. Our results show that none of the variants can improve upon the standard {LSTM} architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
	pages = {2222--2232},
	number = {10},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	author = {Greff, K. and Srivastava, R. K. and Koutník, J. and Steunebrink, B. R. and Schmidhuber, J.},
	date = {2017-10},
	keywords = {Computer architecture, Functional {ANalysis} Of {VAriance} ({fANOVA}), Handwriting recognition, {LSTM}, Logic gates, Microprocessors, Recurrent neural networks, Speech recognition, Training, analysis of variance framework, handwriting recognition, learning (artificial intelligence), long short-term memory ({LSTM}), long short-term memory architecture, machine learning problems, polyphonic music modeling, random search, recurrent neural nets, recurrent neural networks, search problems, search space odyssey, sequence learning, speech recognition, statistical analysis},
}

@article{bock_syntactic_1986,
	title = {Syntactic persistence in language production},
	volume = {18},
	issn = {0010-0285},
	url = {http://www.sciencedirect.com/science/article/pii/0010028586900046},
	doi = {10.1016/0010-0285(86)90004-6},
	abstract = {Activation processes appear to have an important impact on the mechanisms of language use, including those responsible for syntactic structure in speech. Some implications of this claim for theories of language performance were examined with a syntactic priming procedure. On each priming trial, subjects produced a priming sentence in one of several syntactic forms. They then viewed an unrelated event in a picture and described it in one sentence. The probability of a particular syntactic form being used in the description increased when that form had occurred in the prime, under presentation conditions that minimized subjects' attention to their speech, to the syntactic features of the priming sentences, and to connections between the priming sentences and the subsequent pictures. This syntactic repetition effect suggests that sentence formulation processes are somewhat inertial and subject to such probabilistic factors as the frequency or recency of use of particular structural forms. Two further experiments showed that this effect was not appreciably modified by variations in certain conceptual characteristics of sentences, and all three experiments found evidence that the effects of priming were specific to features of sentence form, independent of sentence content. The empirical isolability of structural features from conceptual characteristics of successive utterances is consistent with the assumption that some syntactic processes are organized into a functionally independent subsystem.},
	pages = {355--387},
	number = {3},
	journaltitle = {Cognitive Psychology},
	shortjournal = {Cognitive Psychology},
	author = {Bock, J. Kathryn},
	urldate = {2018-03-07},
	date = {1986-07-01},
}

@article{dai_syntax-directed_2018,
	title = {Syntax-Directed Variational Autoencoder for Structured Data},
	url = {https://openreview.net/forum?id=SyqShMZRb},
	abstract = {Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and...},
	author = {Dai, Hanjun and Tian, Yingtao and Dai, Bo and Skiena, Steven and Song, Le},
	urldate = {2018-03-06},
	date = {2018-02-15},
}

@article{kirby_cumulative_2008,
	title = {Cumulative cultural evolution in the laboratory: An experimental approach to the origins of structure in human language},
	volume = {105},
	rights = {© 2008 by The National Academy of Sciences of the {USA}},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/105/31/10681},
	doi = {10.1073/pnas.0707835105},
	shorttitle = {Cumulative cultural evolution in the laboratory},
	abstract = {We introduce an experimental paradigm for studying the cumulative cultural evolution of language. In doing so we provide the first experimental validation for the idea that cultural transmission can lead to the appearance of design without a designer. Our experiments involve the iterated learning of artificial languages by human participants. We show that languages transmitted culturally evolve in such a way as to maximize their own transmissibility: over time, the languages in our experiments become easier to learn and increasingly structured. Furthermore, this structure emerges purely as a consequence of the transmission of language over generations, without any intentional design on the part of individual language learners. Previous computational and mathematical models suggest that iterated learning provides an explanation for the structure of human language and link particular aspects of linguistic structure with particular constraints acting on language during its transmission. The experimental work presented here shows that the predictions of these models, and models of cultural evolution more generally, can be tested in the laboratory.},
	pages = {10681--10686},
	number = {31},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Kirby, Simon and Cornish, Hannah and Smith, Kenny},
	urldate = {2018-03-05},
	date = {2008-08-05},
	langid = {english},
	pmid = {18667697},
	keywords = {cultural transmission, iterated learning, language evolution},
}

@online{noauthor_interpretable_2018,
	title = {Interpretable Machine Learning through Teaching},
	url = {https://blog.openai.com/interpretable-machine-learning-through-teaching/},
	abstract = {We've designed a method that encourages {AIs} to teach each other with examples that also make sense to humans. Our approach automatically selects the most informative examples to teach a concept — for instance, the best images to describe the concept of dogs — and experimentally we found our approach to be},
	titleaddon = {{OpenAI} Blog},
	urldate = {2018-02-26},
	date = {2018-02-15},
}

@article{ross_improving_2017,
	title = {Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients},
	url = {http://arxiv.org/abs/1711.09404},
	abstract = {Deep neural networks have proven remarkably effective at solving many classification problems, but have been criticized recently for two major weaknesses: the reasons behind their predictions are uninterpretable, and the predictions themselves can often be fooled by small adversarial perturbations. These problems pose major obstacles for the adoption of neural networks in domains that require security or transparency. In this work, we evaluate the effectiveness of defenses that differentiably penalize the degree to which small changes in inputs can alter model predictions. Across multiple attacks, architectures, defenses, and datasets, we find that neural networks trained with this input gradient regularization exhibit robustness to transferred adversarial examples generated to fool all of the other models. We also find that adversarial examples generated to fool gradient-regularized models fool all other models equally well, and actually lead to more "legitimate," interpretable misclassifications as rated by people (which we confirm in a human subject experiment). Finally, we demonstrate that regularizing input gradients makes them more naturally interpretable as rationales for model predictions. We conclude by discussing this relationship between interpretability and robustness in deep neural networks.},
	journaltitle = {{arXiv}:1711.09404 [cs]},
	author = {Ross, Andrew Slavin and Doshi-Velez, Finale},
	urldate = {2018-02-26},
	date = {2017-11-26},
	eprinttype = {arxiv},
	eprint = {1711.09404},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Learning},
}

@article{neto_detecting_2018,
	title = {Detecting Learning vs Memorization in Deep Neural Networks using Shared Structure Validation Sets},
	url = {http://arxiv.org/abs/1802.07714},
	abstract = {The roles played by learning and memorization represent an important topic in deep learning research. Recent work on this subject has shown that the optimization behavior of {DNNs} trained on shuffled labels is qualitatively different from {DNNs} trained with real labels. Here, we propose a novel permutation approach that can differentiate memorization from learning in deep neural networks ({DNNs}) trained as usual (i.e., using the real labels to guide the learning, rather than shuffled labels). The evaluation of weather the {DNN} has learned and/or memorized, happens in a separate step where we compare the predictive performance of a shallow classifier trained with the features learned by the {DNN}, against multiple instances of the same classifier, trained on the same input, but using shuffled labels as outputs. By evaluating these shallow classifiers in validation sets that share structure with the training set, we are able to tell apart learning from memorization. Application of our permutation approach to multi-layer perceptrons and convolutional neural networks trained on image data corroborated many findings from other groups. Most importantly, our illustrations also uncovered interesting dynamic patterns about how {DNNs} memorize over increasing numbers of training epochs, and support the surprising result that {DNNs} are still able to learn, rather than only memorize, when trained with pure Gaussian noise as input.},
	journaltitle = {{arXiv}:1802.07714 [cs, stat]},
	author = {Neto, Elias Chaibub},
	urldate = {2018-02-26},
	date = {2018-02-21},
	eprinttype = {arxiv},
	eprint = {1802.07714},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{parisi_continual_2018,
	title = {Continual Lifelong Learning with Neural Networks: A Review},
	url = {http://arxiv.org/abs/1802.07569},
	shorttitle = {Continual Lifelong Learning with Neural Networks},
	abstract = {Humans and animals have the ability to continually acquire and fine-tune knowledge throughout their lifespan. This ability is mediated by a rich set of neurocognitive functions that together contribute to the early development and experience-driven specialization of our sensorimotor skills. Consequently, the ability to learn from continuous streams of information is crucial for computational learning systems and autonomous agents (inter)acting in the real world. However, continual lifelong learning remains a long-standing challenge for machine learning and neural network models since the incremental acquisition of new skills from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback also for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which the number of tasks is not known a priori and the information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to continual lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic interference. Although significant advances have been made in domain-specific continual lifelong learning with neural networks, extensive research efforts are required for the development of general-purpose artificial intelligence and autonomous agents. We discuss well-established research and recent methodological trends motivated by experimentally observed lifelong learning factors in biological systems. Such factors include principles of neurosynaptic stability-plasticity, critical developmental stages, intrinsically motivated exploration, transfer learning, and crossmodal integration.},
	journaltitle = {{arXiv}:1802.07569 [cs, q-bio, stat]},
	author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
	urldate = {2018-02-26},
	date = {2018-02-21},
	eprinttype = {arxiv},
	eprint = {1802.07569},
	keywords = {Computer Science - Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{kochurov_bayesian_2018,
	title = {Bayesian Incremental Learning for Deep Neural Networks},
	url = {http://arxiv.org/abs/1802.07329},
	abstract = {In industrial machine learning pipelines, data often arrive in parts. Particularly in the case of deep neural networks, it may be too expensive to train the model from scratch each time, so one would rather use a previously learned model and the new data to improve performance. However, deep neural networks are prone to getting stuck in a suboptimal solution when trained on only new data as compared to the full dataset. Our work focuses on a continuous learning setup where the task is always the same and new parts of data arrive sequentially. We apply a Bayesian approach to update the posterior approximation with each new piece of data and find this method to outperform the traditional approach in our experiments.},
	journaltitle = {{arXiv}:1802.07329 [cs, stat]},
	author = {Kochurov, Max and Garipov, Timur and Podoprikhin, Dmitry and Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
	urldate = {2018-02-26},
	date = {2018-02-20},
	eprinttype = {arxiv},
	eprint = {1802.07329},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{wu_beyond_2017,
	title = {Beyond Sparsity: Tree Regularization of Deep Models for Interpretability},
	url = {http://arxiv.org/abs/1711.06178},
	shorttitle = {Beyond Sparsity},
	abstract = {The lack of interpretability remains a key barrier to the adoption of deep models in many applications. In this work, we explicitly regularize deep models so human users might step through the process behind their predictions in little time. Specifically, we train deep time-series models so their class-probability predictions have high accuracy while being closely modeled by decision trees with few nodes. Using intuitive toy examples as well as medical tasks for treating sepsis and {HIV}, we demonstrate that this new tree regularization yields models that are easier for humans to simulate than simpler L1 or L2 penalties without sacrificing predictive power.},
	journaltitle = {{arXiv}:1711.06178 [cs, stat]},
	author = {Wu, Mike and Hughes, Michael C. and Parbhoo, Sonali and Zazzi, Maurizio and Roth, Volker and Doshi-Velez, Finale},
	urldate = {2018-02-26},
	date = {2017-11-16},
	eprinttype = {arxiv},
	eprint = {1711.06178},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{li_adaptive_2018,
	title = {Adaptive Memory Networks},
	url = {http://arxiv.org/abs/1802.00510},
	abstract = {We present Adaptive Memory Networks ({AMN}) that processes input-question pairs to dynamically construct a network architecture optimized for lower inference times for Question Answering ({QA}) tasks. {AMN} processes the input story to extract entities and stores them in memory banks. Starting from a single bank, as the number of input entities increases, {AMN} learns to create new banks as the entropy in a single bank becomes too high. Hence, after processing an input-question(s) pair, the resulting network represents a hierarchical structure where entities are stored in different banks, distanced by question relevance. At inference, one or few banks are used, creating a tradeoff between accuracy and performance. {AMN} is enabled by dynamic networks that allow input dependent network creation and efficiency in dynamic mini-batching as well as our novel bank controller that allows learning discrete decision making with high accuracy. In our results, we demonstrate that {AMN} learns to create variable depth networks depending on task complexity and reduces inference times for {QA} tasks.},
	journaltitle = {{arXiv}:1802.00510 [cs]},
	author = {Li, Daniel and Kadav, Asim},
	urldate = {2018-02-20},
	date = {2018-02-01},
	eprinttype = {arxiv},
	eprint = {1802.00510},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{berg-kirkpatrick_empirical_2012,
	location = {Stroudsburg, {PA}, {USA}},
	title = {An Empirical Investigation of Statistical Significance in {NLP}},
	url = {http://dl.acm.org/citation.cfm?id=2390948.2391058},
	series = {{EMNLP}-{CoNLL} '12},
	abstract = {We investigate two aspects of the empirical behavior of paired significance tests for {NLP} systems. First, when one system appears to outperform another, how does significance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on? Is it true that for each task there is a gain which roughly implies significance? We explore these issues across a range of {NLP} tasks using both large collections of past systems' outputs and variants of single systems. Next, once significance levels are computed, how well does the standard i.i.d. notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing.},
	pages = {995--1005},
	booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
	publisher = {Association for Computational Linguistics},
	author = {Berg-Kirkpatrick, Taylor and Burkett, David and Klein, Dan},
	urldate = {2018-02-20},
	date = {2012},
}

@inproceedings{philipp_nonparametric_2017,
	title = {Nonparametric Neural Networks},
	url = {http://arxiv.org/abs/1712.05440},
	abstract = {Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce *nonparametric neural networks*, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an L\_p penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an L\_2 penalty. We employ a novel optimization algorithm, which we term *adaptive radial-angular gradient descent* or *{AdaRad}*, and obtain promising results.},
	eventtitle = {{ICLR}},
	booktitle = {{arXiv}:1712.05440 [cs]},
	author = {Philipp, George and Carbonell, Jaime G.},
	urldate = {2018-02-18},
	date = {2017-12-14},
	eprinttype = {arxiv},
	eprint = {1712.05440},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Learning},
}

@article{niculae_sparsemap:_2018,
	title = {{SparseMAP}: Differentiable Sparse Structured Inference},
	url = {http://arxiv.org/abs/1802.04223},
	shorttitle = {{SparseMAP}},
	abstract = {Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce {SparseMAP}, a new method for sparse structured inference, together with corresponding loss functions. {SparseMAP} inference is able to automatically select only a few global structures: it is situated between {MAP} inference, which picks a single structure, and marginal inference, which assigns probability mass to all structures, including implausible ones. Importantly, {SparseMAP} can be computed using only calls to a {MAP} oracle, hence it is applicable even to problems where marginal inference is intractable, such as linear assignment. Moreover, thanks to the solution sparsity, gradient backpropagation is efficient regardless of the structure. {SparseMAP} thus enables us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.},
	journaltitle = {{arXiv}:1802.04223 [cs, stat]},
	author = {Niculae, Vlad and Martins, André F. T. and Blondel, Mathieu and Cardie, Claire},
	urldate = {2018-02-17},
	date = {2018-02-12},
	eprinttype = {arxiv},
	eprint = {1802.04223},
	keywords = {68T50, Computer Science - Computation and Language, Computer Science - Learning, I.2.6, Statistics - Machine Learning},
}

@inproceedings{bowman_tree-structured_2015,
	title = {Tree-structured composition in neural networks without tree-structured architectures},
	url = {http://arxiv.org/abs/1506.04834},
	abstract = {Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models. We hypothesize that neural sequence models like {LSTMs} are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data. We demonstrate this possibility using an artificial data task for which recursive compositional structure is crucial, and find an {LSTM}-based sequence model can indeed learn to exploit the underlying tree structure. However, its performance consistently lags behind that of tree models, even on large training sets, suggesting that tree-structured models are more effective at exploiting recursive structure.},
	eventtitle = {{NIPS} Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches},
	booktitle = {{arXiv}:1506.04834 [cs]},
	author = {Bowman, Samuel R. and Manning, Christopher D. and Potts, Christopher},
	urldate = {2018-02-17},
	date = {2015-06-16},
	eprinttype = {arxiv},
	eprint = {1506.04834},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
}
@inproceedings{hashimoto_joint_2017,
	title = {A Joint Many-Task Model: Growing a Neural Network for Multiple {NLP} Tasks},
	url = {http://arxiv.org/abs/1611.01587},
	shorttitle = {A Joint Many-Task Model},
	abstract = {Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on {POS} tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.},
	eventtitle = {{EMNLP}},
	booktitle = {{arXiv}:1611.01587 [cs]},
	author = {Hashimoto, Kazuma and Xiong, Caiming and Tsuruoka, Yoshimasa and Socher, Richard},
	urldate = {2016-11-11},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1611.01587},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@incollection{bengio_scheduled_2015,
	title = {Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks},
	url = {http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf},
	pages = {1171--1179},
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	urldate = {2018-02-16},
	date = {2015},
}

@article{weinstein_teaching_2018,
	title = {Teaching the science of learning},
	volume = {3},
	issn = {2365-7464},
	url = {https://doi.org/10.1186/s41235-017-0087-y},
	doi = {10.1186/s41235-017-0087-y},
	abstract = {The science of learning has made a considerable contribution to our understanding of effective teaching and learning strategies. However, few instructors outside of the field are privy to this research. In this tutorial review, we focus on six specific cognitive strategies that have received robust support from decades of research: spaced practice, interleaving, retrieval practice, elaboration, concrete examples, and dual coding. We describe the basic research behind each strategy and relevant applied research, present examples of existing and suggested implementation, and make recommendations for further research that would broaden the reach of these strategies.},
	pages = {2},
	journaltitle = {Cognitive Research: Principles and Implications},
	shortjournal = {Cognitive Research: Principles and Implications},
	author = {Weinstein, Yana and Madan, Christopher R. and Sumeracki, Megan A.},
	urldate = {2018-02-14},
	date = {2018-01-24},
	keywords = {Education, Learning, Memory, Teaching},
}

@article{christodouloupoulos_massively_2015,
	title = {A massively parallel corpus: the Bible in 100 languages},
	volume = {49},
	issn = {1574-0218},
	url = {https://doi.org/10.1007/s10579-014-9287-y},
	doi = {10.1007/s10579-014-9287-y},
	abstract = {We describe the creation of a massively parallel corpus based on 100 translations of the Bible. We discuss some of the difficulties in acquiring and processing the raw material as well as the potential of the Bible as a corpus for natural language processing. Finally we present a statistical analysis of the corpora collected and a detailed comparison between the English translation and other English corpora.},
	pages = {375--395},
	number = {2},
	journaltitle = {Language Resources and Evaluation},
	author = {Christodouloupoulos, Christos and Steedman, Mark},
	date = {2015-06},
}

@article{de_vivo_ultrastructural_2017,
	title = {Ultrastructural evidence for synaptic scaling across the wake/sleep cycle},
	volume = {355},
	pages = {507--510},
	number = {6324},
	journaltitle = {Science},
	author = {De Vivo, Luisa and Bellesi, Michele and Marshall, William and Bushong, Eric A. and Ellisman, Mark H. and Tononi, Giulio and Cirelli, Chiara},
	date = {2017},
}

@inproceedings{sachan_easy_2016,
	title = {Easy questions first? a case study on curriculum learning for question answering},
	volume = {1},
	pages = {453--463},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	author = {Sachan, Mrinmaya and Xing, Eric},
	date = {2016},
}

@article{zaremba_learning_2014,
	title = {Learning to Execute},
	volume = {abs/1410.4615},
	journaltitle = {{CoRR}},
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	date = {2014},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{tsvetkov_learning_2016,
	title = {Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning},
	volume = {abs/1605.03852},
	journaltitle = {{CoRR}},
	author = {Tsvetkov, Yulia and Faruqui, Manaal and Ling, Wang and {MacWhinney}, Brian and Dyer, Chris},
	date = {2016},
}

@article{he_reshaping_2014,
	title = {Reshaping deep neural network for fast decoding by node-pruning},
	pages = {245--249},
	journaltitle = {2014 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {He, Tianxing and Fan, Yuchen and Qian, Yanmin and Tan, Tian and Yu, Kai},
	date = {2014},
}

@inproceedings{mozer_using_1989,
	title = {Using Relevance to Reduce Network Size Automatically},
	author = {Mozer, Michael C. and Smolensky, Paul},
	date = {1989},
}

@article{mariet_diversity_2015,
	title = {Diversity Networks: Neural Network Compression Using Determinantal Point Processes},
	url = {http://arxiv.org/abs/1511.05077},
	shorttitle = {Diversity Networks},
	abstract = {We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process ({DPP}) over neurons in a given layer. It uses this {DPP} to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.},
	journaltitle = {{arXiv}:1511.05077 [cs]},
	author = {Mariet, Zelda and Sra, Suvrit},
	urldate = {2018-02-09},
	date = {2015-11-16},
	eprinttype = {arxiv},
	eprint = {1511.05077},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{sahil_garg_neurogenesis-inspired_2017,
	title = {{NEUROGENESIS}-{INSPIRED} {DICTIONARY} {LEARNING}: {ONLINE} {MODEL} {ADAPTION} {IN} A {CHANGING} {WORLD}},
	url = {https://openreview.net/forum?id=S1dJ1smFg&noteId=S1dJ1smFg},
	author = {{Sahil Garg} and {Irina Rish} and {Guillermo Cecchi}},
	urldate = {2017-02-22},
	date = {2017},
}

@inproceedings{nikolas_wolfe_incredible_2017,
	title = {The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning},
	eventtitle = {{ICLR}},
	author = {{Nikolas Wolfe} and {Aditya Sharma} and {Lukas Drude} and {Bhiksha Raj}},
	date = {2017},
}

@inproceedings{hadi_amiri_repeat_2017,
	title = {Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks},
	url = {https://scholar.google.com/scholar?hl=en&q=Repeat+before+Forgetting%3A+Spaced+Repetition+for+Efficient+and+Effective+Training+of+Neural+Networks&btnG=&as_sdt=1%2C33&as_sdtp=},
	eventtitle = {{EMNLP}},
	author = {{Hadi Amiri} and {Timothy A. Miller} and {Guergana Savova}},
	urldate = {2017-10-01},
	date = {2017},
}

@article{tang_pruning_2015,
	title = {A pruning based method to learn both weights and connections for {LSTM}},
	url = {https://pdfs.semanticscholar.org/bd00/be89b0e987b21fe3b5145d8030f85a8f075f.pdf},
	author = {Tang, Shijian and Han, Jiang},
	urldate = {2016-12-12},
	date = {2015},
}

@article{miranda_reducing_2015,
	title = {Reducing the Training Time of Neural Networks by Partitioning},
	url = {http://arxiv.org/abs/1511.02954},
	abstract = {This paper presents a new method for pre-training neural networks that can decrease the total training time for a neural network while maintaining the final performance, which motivates its use on deep neural networks. By partitioning the training task in multiple training subtasks with sub-models, which can be performed independently and in parallel, it is shown that the size of the sub-models reduces almost quadratically with the number of subtasks created, quickly scaling down the sub-models used for the pre-training. The sub-models are then merged to provide a pre-trained initial set of weights for the original model. The proposed method is independent of the other aspects of the training, such as architecture of the neural network, training method, and objective, making it compatible with a wide range of existing approaches. The speedup without loss of performance is validated experimentally on {MNIST} and on {CIFAR}10 data sets, also showing that even performing the subtasks sequentially can decrease the training time. Moreover, we show that larger models may present higher speedups and conjecture about the benefits of the method in distributed learning systems.},
	journaltitle = {{arXiv}:1511.02954 [cs]},
	author = {Miranda, Conrado S. and Von Zuben, Fernando J.},
	urldate = {2018-02-08},
	date = {2015-11-09},
	eprinttype = {arxiv},
	eprint = {1511.02954},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{kaiser_neural_2015,
	title = {Neural {GPUs} Learn Algorithms},
	url = {http://arxiv.org/abs/1511.08228},
	abstract = {Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines ({NTMs}). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal {NTMs} have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural {GPU}. It is based on a type of convolutional gated recurrent unit and, like the {NTM}, is computationally universal. Unlike the {NTM}, the Neural {GPU} is highly parallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural {GPU} can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural {GPU} on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.},
	journaltitle = {{arXiv}:1511.08228 [cs]},
	author = {Kaiser, Łukasz and Sutskever, Ilya},
	urldate = {2018-02-08},
	date = {2015-11-25},
	eprinttype = {arxiv},
	eprint = {1511.08228},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{chen_net2net:_2015,
	title = {Net2Net: Accelerating Learning via Knowledge Transfer},
	url = {http://arxiv.org/abs/1511.05641},
	shorttitle = {Net2Net},
	abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the {ImageNet} dataset.},
	journaltitle = {{arXiv}:1511.05641 [cs]},
	author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
	urldate = {2018-02-08},
	date = {2015-11-17},
	eprinttype = {arxiv},
	eprint = {1511.05641},
	keywords = {Computer Science - Learning},
}

@article{han_deep_2015,
	title = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
	url = {http://arxiv.org/abs/1510.00149},
	shorttitle = {Deep Compression},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the {ImageNet} dataset, our method reduced the storage required by {AlexNet} by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of {VGG}-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip {SRAM} cache rather than off-chip {DRAM} memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on {CPU}, {GPU} and mobile {GPU}, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	journaltitle = {{arXiv}:1510.00149 [cs]},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	urldate = {2018-02-08},
	date = {2015-10-01},
	eprinttype = {arxiv},
	eprint = {1510.00149},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
}

@article{srinivas_learning_2015,
	title = {Learning Neural Network Architectures using Backpropagation},
	url = {http://arxiv.org/abs/1511.05497},
	abstract = {Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state {ReLU}, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.},
	journaltitle = {{arXiv}:1511.05497 [cs]},
	author = {Srinivas, Suraj and Babu, R. Venkatesh},
	urldate = {2018-02-08},
	date = {2015-11-17},
	eprinttype = {arxiv},
	eprint = {1511.05497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{lecun_optimal_1990,
	title = {Optimal brain damage},
	pages = {598--605},
	booktitle = {Advances in neural information processing systems},
	author = {{LeCun}, Yann and Denker, John S. and Solla, Sara A.},
	date = {1990},
}

@inproceedings{hassibi_optimal_1993,
	title = {Optimal brain surgeon and general network pruning},
	pages = {293--299},
	booktitle = {Neural Networks, 1993., {IEEE} International Conference on},
	publisher = {{IEEE}},
	author = {Hassibi, Babak and Stork, David G. and Wolff, Gregory J.},
	date = {1993},
}

@article{cote_infinite_2015,
	title = {An Infinite Restricted Boltzmann Machine},
	url = {http://arxiv.org/abs/1502.02476},
	abstract = {We present a mathematical construction for the restricted Boltzmann machine ({RBM}) that doesn't require specifying the number of hidden units. In fact, the hidden layer size is adaptive and can grow during training. This is obtained by first extending the {RBM} to be sensitive to the ordering of its hidden units. Then, thanks to a carefully chosen definition of the energy function, we show that the limit of infinitely many hidden units is well defined. As with {RBM}, approximate maximum likelihood training can be performed, resulting in an algorithm that naturally and adaptively adds trained hidden units during learning. We empirically study the behaviour of this infinite {RBM}, showing that its performance is competitive to that of the {RBM}, while not requiring the tuning of a hidden layer size.},
	journaltitle = {{arXiv}:1502.02476 [cs]},
	author = {Côté, Marc-Alexandre and Larochelle, Hugo},
	urldate = {2018-02-08},
	date = {2015-02-09},
	eprinttype = {arxiv},
	eprint = {1502.02476},
	keywords = {Computer Science - Learning},
}

@inproceedings{ash_dynamic_1989,
	title = {Dynamic node creation in backpropagation networks},
	doi = {10.1109/IJCNN.1989.118509},
	abstract = {Summary form only given. A novel method called dynamic node creation ({DNC}) that attacks issues of training large networks and of testing networks with different numbers of hidden layer units is presented. {DNC} sequentially adds nodes one at a time to the hidden layer(s) of the network until the desired approximation accuracy is achieved. Simulation results for parity, symmetry, binary addition, and the encoder problem are presented. The procedure was capable of finding known minimal topologies in many cases, and was always within three nodes of the minimum. Computational expense for finding the solutions was comparable to training normal backpropagation ({BP}) networks with the same final topologies. Starting out with fewer nodes than needed to solve the problem actually seems to help find a solution. The method yielded a solution for every problem tried. {BP} applied to the same large networks with randomized initial weights was unable, after repeated attempts, to replicate some minimum solutions found by {DNC}.{\textless}{\textgreater}},
	eventtitle = {International 1989 Joint Conference on Neural Networks},
	pages = {623 vol.2--},
	booktitle = {International 1989 Joint Conference on Neural Networks},
	author = {{Ash}},
	date = {1989},
	keywords = {Encoding, Learning systems, Neural networks, Topology, backpropagation networks, dynamic node creation, encoder, encoding, hidden layer, learning systems, neural nets, topology},
}

@online{noauthor_pattern.en_2010,
	title = {pattern.en {\textbar} {CLiPS}},
	url = {http://www.clips.ua.ac.be/pages/pattern-en},
	urldate = {2018-02-07},
	date = {2010-12-26},
	langid = {english},
}

@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	volume = {abs/1207.0580},
	journaltitle = {{CoRR}},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	date = {2012},
}

@article{tarullo_sleep_2011,
	title = {Sleep and infant learning},
	volume = {20},
	pages = {35--46},
	number = {1},
	journaltitle = {Infant and child development},
	author = {Tarullo, Amanda R and Balsam, Peter D and Fifer, William P},
	date = {2011},
}

@article{yordanova_shifting_2008,
	title = {Shifting from implicit to explicit knowledge: different roles of early-and late-night sleep},
	volume = {15},
	pages = {508--515},
	number = {7},
	journaltitle = {Learning \& Memory},
	author = {Yordanova, Juliana and Kolev, Vasil and Verleger, Rolf and Bataghva, Zhamak and Born, Jan and Wagner, Ullrich},
	date = {2008},
}

@inproceedings{smith_what_2017,
	title = {What Do Recurrent Neural Network Grammars Learn About Syntax?},
	booktitle = {{EACL}},
	author = {Smith, Noah A. and Dyer, Chris and Ballesteros, Miguel and Neubig, Graham and Kong, Lingpeng and Kuncoro, Adhiguna},
	date = {2017},
}

@inproceedings{schohn_less_2000,
	title = {Less is More: Active Learning with Support Vector Machines},
	booktitle = {{ICML}},
	author = {Schohn, Greg and Cohn, David},
	date = {2000},
}

@article{rosch_principles_1999,
	title = {Principles of categorization},
	volume = {189},
	journaltitle = {Concepts: core readings},
	author = {Rosch, Eleanor},
	date = {1999},
}

@article{belinkov_evaluating_2018,
	title = {Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks},
	volume = {abs/1801.07772},
	url = {http://arxiv.org/abs/1801.07772},
	journaltitle = {{CoRR}},
	author = {Belinkov, Yonatan and Màrquez, Lluís and Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Glass, James R.},
	date = {2018},
}

@article{williams_learning_2017,
	title = {Learning to parse from a semantic objective: It works. Is it syntax?},
	url = {http://arxiv.org/abs/1709.01121},
	shorttitle = {Learning to parse from a semantic objective},
	abstract = {Recent work on reinforcement learning and other gradient estimators for latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to ground-truth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) they do outperform baselines on sentence classification, but that (ii) their parsing strategies are not especially consistent across random restarts, (iii) the parses they produce tend to be shallower than {PTB} parses, and (iv) these do not resemble those of {PTB} or of any other recognizable semantic or syntactic grammar formalism.},
	journaltitle = {{arXiv}:1709.01121 [cs]},
	author = {Williams, Adina and Drozdov, Andrew and Bowman, Samuel R.},
	urldate = {2018-02-04},
	date = {2017-09-04},
	eprinttype = {arxiv},
	eprint = {1709.01121},
	keywords = {Computer Science - Computation and Language},
}

@online{noauthor_[1801.09808]_nodate,
	title = {[1801.09808] The Intriguing Properties of Model Explanations},
	url = {https://arxiv.org/abs/1801.09808},
	urldate = {2018-02-04},
}

@article{castelli_pruning_2018,
	title = {Pruning Techniques for Mixed Ensembles of Genetic Programming Models},
	url = {http://arxiv.org/abs/1801.07668},
	abstract = {The objective of this paper is to define an effective strategy for building an ensemble of Genetic Programming ({GP}) models. Ensemble methods are widely used in machine learning due to their features: they average out biases, they reduce the variance and they usually generalize better than single models. Despite these advantages, building ensemble of {GP} models is not a well-developed topic in the evolutionary computation community. To fill this gap, we propose a strategy that blends individuals produced by standard syntax-based {GP} and individuals produced by geometric semantic genetic programming, one of the newest semantics-based method developed in {GP}. In fact, recent literature showed that combining syntax and semantics could improve the generalization ability of a {GP} model. Additionally, to improve the diversity of the {GP} models used to build up the ensemble, we propose different pruning criteria that are based on correlation and entropy, a commonly used measure in information theory. Experimental results,obtained over different complex problems, suggest that the pruning criteria based on correlation and entropy could be effective in improving the generalization ability of the ensemble model and in reducing the computational burden required to build it.},
	journaltitle = {{arXiv}:1801.07668 [cs, stat]},
	author = {Castelli, Mauro and Gonçalves, Ivo and Manzoni, Luca and Vanneschi, Leonardo},
	urldate = {2018-02-02},
	date = {2018-01-23},
	eprinttype = {arxiv},
	eprint = {1801.07668},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{shirakawa_dynamic_2018,
	title = {Dynamic Optimization of Neural Network Structures Using Probabilistic Modeling},
	url = {http://arxiv.org/abs/1801.07650},
	abstract = {Deep neural networks ({DNNs}) are powerful machine learning models and have succeeded in various artificial intelligence tasks. Although various architectures and modules for the {DNNs} have been proposed, selecting and designing the appropriate network structure for a target problem is a challenging task. In this paper, we propose a method to simultaneously optimize the network structure and weight parameters during neural network training. We consider a probability distribution that generates network structures, and optimize the parameters of the distribution instead of directly optimizing the network structure. The proposed method can apply to the various network structure optimization problems under the same framework. We apply the proposed method to several structure optimization problems such as selection of layers, selection of unit types, and selection of connections using the {MNIST}, {CIFAR}-10, and {CIFAR}-100 datasets. The experimental results show that the proposed method can find the appropriate and competitive network structures.},
	journaltitle = {{arXiv}:1801.07650 [cs, stat]},
	author = {Shirakawa, Shinichi and Iwata, Yasushi and Akimoto, Youhei},
	urldate = {2018-02-02},
	date = {2018-01-23},
	eprinttype = {arxiv},
	eprint = {1801.07650},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{zhang_mitigating_2018,
	title = {Mitigating Unwanted Biases with Adversarial Learning},
	url = {http://arxiv.org/abs/1801.07593},
	abstract = {Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the {UCI} Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.},
	journaltitle = {{arXiv}:1801.07593 [cs]},
	author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
	urldate = {2018-02-02},
	date = {2018-01-22},
	eprinttype = {arxiv},
	eprint = {1801.07593},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Learning},
}

@article{gelderloos_phonemes_2016,
	title = {From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning},
	shorttitle = {From phonemes to images},
	journaltitle = {{arXiv} preprint {arXiv}:1610.03342},
	author = {Gelderloos, Lieke and Chrupa\{{\textbackslash}textbackslash\}la, Grzegorz},
	date = {2016},
}

@inproceedings{bjerva_semantic_2016,
	title = {Semantic Tagging with Deep Residual Networks},
	booktitle = {{COLING}},
	author = {Bjerva, Johannes and Plank, Barbara and Bos, Johan},
	date = {2016},
}

@article{zolna_fraternal_2017,
	title = {Fraternal Dropout},
	url = {http://arxiv.org/abs/1711.00066},
	abstract = {Recurrent neural networks ({RNNs}) are important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing {RNNs} is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an {RNN} (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of {RNNs} to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft {COCO}) and semi-supervised ({CIFAR}-10) tasks.},
	journaltitle = {{arXiv}:1711.00066 [cs, stat]},
	author = {Zolna, Konrad and Arpit, Devansh and Suhubdy, Dendi and Bengio, Yoshua},
	urldate = {2018-02-01},
	date = {2017-10-31},
	eprinttype = {arxiv},
	eprint = {1711.00066},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
}

@article{zhu_sparsely_2018,
	title = {Sparsely Connected Convolutional Networks},
	url = {http://arxiv.org/abs/1801.05895},
	abstract = {Residual learning with skip connections permits training ultra-deep neural networks and obtains superb performance. Building in this direction, {DenseNets} proposed a dense connection structure where each layer is directly connected to all of its predecessors. The densely connected structure leads to better information flow and feature reuse. However, the overly dense skip connections also bring about the problems of potential risk of overfitting, parameter redundancy and large memory consumption. In this work, we analyze the feature aggregation patterns of {ResNets} and {DenseNets} under a uniform aggregation view framework. We show that both structures densely gather features from previous layers in the network but combine them in their respective ways: summation ({ResNets}) or concatenation ({DenseNets}). We compare the strengths and drawbacks of these two aggregation methods and analyze their potential effects on the networks' performance. Based on our analysis, we propose a new structure named {SparseNets} which achieves better performance with fewer parameters than {DenseNets} and {ResNets}.},
	journaltitle = {{arXiv}:1801.05895 [cs]},
	author = {Zhu, Ligeng and Deng, Ruizhi and Deng, Zhiwei and Mori, Greg and Tan, Ping},
	urldate = {2018-01-22},
	date = {2018-01-17},
	eprinttype = {arxiv},
	eprint = {1801.05895},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
@article{ruder_learning_2017,
	title = {Learning what to share between loosely related tasks},
	url = {http://arxiv.org/abs/1705.08142},
	abstract = {Multi-task learning is motivated by the observation that humans bring to bear what they know about related problems when solving new ones. Similarly, deep neural networks can profit from related tasks by sharing parameters with other networks. However, humans do not consciously decide to transfer knowledge between tasks. In Natural Language Processing ({NLP}), it is hard to predict if sharing will lead to improvements, particularly if tasks are only loosely related. To overcome this, we introduce Sluice Networks, a general framework for multi-task learning where trainable parameters control the amount of sharing. Our framework generalizes previous proposals in enabling sharing of all combinations of subspaces, layers, and skip connections. We perform experiments on three task pairs, and across seven different domains, using data from {OntoNotes} 5.0, and achieve up to 15\% average error reductions over common approaches to multi-task learning. We show that a) label entropy is predictive of gains in sluice networks, confirming findings for hard parameter sharing and b) while sluice networks easily fit noise, they are robust across domains in practice.},
	journaltitle = {{arXiv}:1705.08142 [cs, stat]},
	author = {Ruder, Sebastian and Bingel, Joachim and Augenstein, Isabelle and Søgaard, Anders},
	urldate = {2018-01-22},
	date = {2017-05-23},
	eprinttype = {arxiv},
	eprint = {1705.08142},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{bennani-smires_gitgraph_2018,
	title = {{GitGraph} - Architecture Search Space Creation through Frequent Computational Subgraph Mining},
	url = {http://arxiv.org/abs/1801.05159},
	abstract = {The dramatic success of deep neural networks across multiple application areas often relies on experts painstakingly designing a network architecture specific to each task. To simplify this process and make it more accessible, an emerging research effort seeks to automate the design of neural network architectures, using e.g. evolutionary algorithms or reinforcement learning or simple search in a constrained space of neural modules. Considering the typical size of the search space (e.g. \$10{\textasciicircum}\{10\}\$ candidates for a \$10\$-layer network) and the cost of evaluating a single candidate, current architecture search methods are very restricted. They either rely on static pre-built modules to be recombined for the task at hand, or they define a static hand-crafted framework within which they can generate new architectures from the simplest possible operations. In this paper, we relax these restrictions, by capitalizing on the collective wisdom contained in the plethora of neural networks published in online code repositories. Concretely, we (a) extract and publish {GitGraph}, a corpus of neural architectures and their descriptions; (b) we create problem-specific neural architecture search spaces, implemented as a textual search mechanism over {GitGraph}; (c) we propose a method of identifying unique common subgraphs within the architectures solving each problem (e.g., image processing, reinforcement learning), that can then serve as modules in the newly created problem specific neural search space.},
	journaltitle = {{arXiv}:1801.05159 [cs]},
	author = {Bennani-Smires, Kamil and Musat, Claudiu and Hossmann, Andreea and Baeriswyl, Michael},
	urldate = {2018-01-22},
	date = {2018-01-16},
	eprinttype = {arxiv},
	eprint = {1801.05159},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning},
}

@article{mohseni_human-grounded_2018,
	title = {A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning},
	url = {http://arxiv.org/abs/1801.05075},
	abstract = {In order for people to be able to trust and take advantage of the results of advanced machine learning and artificial intelligence solutions for real decision making, people need to be able to understand the machine rationale for given output. Research in explain artificial intelligence ({XAI}) addresses the aim, but there is a need for evaluation of human relevance and understandability of explanations. Our work contributes a novel methodology for evaluating the quality or human interpretability of explanations for machine learning models. We present an evaluation benchmark for instance explanations from text and image classifiers. The explanation meta-data in this benchmark is generated from user annotations of image and text samples. We describe the benchmark and demonstrate its utility by a quantitative evaluation on explanations generated from a recent machine learning algorithm. This research demonstrates how human-grounded evaluation could be used as a measure to qualify local machine-learning explanations.},
	journaltitle = {{arXiv}:1801.05075 [cs]},
	author = {Mohseni, Sina and Ragan, Eric D.},
	urldate = {2018-01-22},
	date = {2018-01-15},
	eprinttype = {arxiv},
	eprint = {1801.05075},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@article{shalev-shwartz_failures_2017,
	title = {Failures of Gradient-Based Deep Learning},
	url = {http://arxiv.org/abs/1703.07950},
	abstract = {In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradient-based algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.},
	journaltitle = {{arXiv}:1703.07950 [cs, stat]},
	author = {Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
	urldate = {2018-01-22},
	date = {2017-03-23},
	eprinttype = {arxiv},
	eprint = {1703.07950},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2018-01-22},
	date = {2017-06-12},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
}

@article{hu_overcoming_2018,
	title = {Overcoming the vanishing gradient problem in plain recurrent networks},
	url = {http://arxiv.org/abs/1801.06105},
	abstract = {Plain recurrent networks greatly suffer from the vanishing gradient problem while Gated Neural Networks ({GNNs}) such as Long-short Term Memory ({LSTM}) and Gated Recurrent Unit ({GRU}) deliver promising results in many sequence learning tasks through sophisticated network designs. This paper shows how we can address this problem in a plain recurrent network by analyzing the gating mechanisms in {GNNs}. We propose a novel network called the Recurrent Identity Network ({RIN}) which allows a plain recurrent network to overcome the vanishing gradient problem while training very deep models without the use of gates. We compare this model with {IRNNs} and {LSTMs} on multiple sequence modeling benchmarks. The {RINs} demonstrate competitive performance and converge faster in all tasks. Notably, small {RIN} models produce 12\%--67\% higher accuracy on the Sequential and Permuted {MNIST} datasets and reach state-of-the-art performance on the {bAbI} question answering dataset.},
	journaltitle = {{arXiv}:1801.06105 [cs]},
	author = {Hu, Yuhuang and Huber, Adrian and Anumula, Jithendar and Liu, Shih-Chii},
	urldate = {2018-01-22},
	date = {2018-01-18},
	eprinttype = {arxiv},
	eprint = {1801.06105},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{wang_comparison_2018,
	title = {A Comparison of Rule Extraction for Different Recurrent Neural Network Models and Grammatical Complexity},
	url = {http://arxiv.org/abs/1801.05420},
	abstract = {It has been shown that rules can be extracted from highly non-linear, recursive models such as recurrent neural networks ({RNNs}). The {RNN} models mostly investigated include both Elman networks and second-order recurrent networks. Recently, new types of {RNNs} have demonstrated superior power in handling many machine learning tasks, especially when structural data is involved such as language modeling. Here, we empirically evaluate different recurrent models on the task of learning deterministic finite automata ({DFA}), the seven Tomita grammars. We are interested in the capability of recurrent models with different architectures in learning and expressing regular grammars, which can be the building blocks for many applications dealing with structural data. Our experiments show that a second-order {RNN} provides the best and stablest performance of extracting {DFA} over all Tomita grammars and that other {RNN} models are greatly influenced by different Tomita grammars. To better understand these results, we provide a theoretical analysis of the "complexity" of different grammars, by introducing the entropy and the averaged edit distance of regular grammars defined in this paper. Through our analysis, we categorize all Tomita grammars into different classes, which explains the inconsistency in the performance of extraction observed across all {RNN} models.},
	journaltitle = {{arXiv}:1801.05420 [cs]},
	author = {Wang, Qinglong and Zhang, Kaixuan and Ororbia {II}, Alexander G. and Xing, Xinyu and Liu, Xue and Giles, C. Lee},
	urldate = {2018-01-22},
	date = {2018-01-15},
	eprinttype = {arxiv},
	eprint = {1801.05420},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
}

@article{sachan_investigating_2018,
	title = {Investigating the Working of Text Classifiers},
	url = {http://arxiv.org/abs/1801.06261},
	abstract = {Text classification is one of the most widely studied task in natural language processing. Recently, larger and larger multilayer neural network models are employed for the task motivated by the principle of compositionality. Almost all of the methods reported use discriminative approaches for the task. Discriminative approaches come with a caveat that if there is no proper capacity control, it might latch on to any signal even though it might not generalize. With use of various state-of-the-art approaches for text classifiers, we want to explore if the models actually learn to compose meaning of the sentences or still just use some key lexicons. To test our hypothesis, we construct datasets where the train and test split have no direct overlap of such lexicons. We study various text classifiers and observe that there is a big performance drop on these datasets. Finally, we show that even simple regularization techniques can improve performance on these datasets.},
	journaltitle = {{arXiv}:1801.06261 [cs]},
	author = {Sachan, Devendra Singh and Zaheer, Manzil and Salakhutdinov, Ruslan},
	urldate = {2018-01-22},
	date = {2018-01-18},
	eprinttype = {arxiv},
	eprint = {1801.06261},
	keywords = {Computer Science - Computation and Language},
}

@article{lee_emergent_2017,
	title = {Emergent translation in multi-agent communication},
	journaltitle = {{arXiv} preprint {arXiv}:1710.06922},
	author = {Lee, Jason and Cho, Kyunghyun and Weston, Jason and Kiela, Douwe},
	date = {2017},
}

@article{poerner_evaluating_2018,
	title = {Evaluating neural network explanation methods using hybrid documents and morphological prediction},
	url = {http://arxiv.org/abs/1801.06422},
	abstract = {We propose two novel paradigms for evaluating neural network explanations in {NLP}. The first paradigm works on hybrid documents, the second exploits morphosyntactic agreements. Neither paradigm requires manual annotations; instead, a relevance ground truth is generated automatically. In our experiments, successful explanations for Long Short Term Memory networks ({LSTMs}) were produced by a decomposition of memory cells (Murdoch \& Szlam, 2017), while for convolutional neural networks, a gradient-based method by (Denil et al., 2014) works well. We also introduce {LIMSSE}, a substring-based extension of {LIME} (Ribeiro et al., 2016) that produces the most successful explanations in the hybrid document experiment.},
	journaltitle = {{arXiv}:1801.06422 [cs]},
	author = {Poerner, Nina and Schütze, Hinrich and Roth, Benjamin},
	urldate = {2018-01-22},
	date = {2018-01-19},
	eprinttype = {arxiv},
	eprint = {1801.06422},
	keywords = {Computer Science - Computation and Language},
}

@article{howard_fine-tuned_2018,
	title = {Fine-tuned Language Models for Text Classification},
	url = {http://arxiv.org/abs/1801.06146},
	abstract = {Transfer learning has revolutionized computer vision, but existing approaches in {NLP} still require task-specific modifications and training from scratch. We propose Fine-tuned Language Models ({FitLaM}), an effective transfer learning method that can be applied to any task in {NLP}, and introduce techniques that are key for fine-tuning a state-of-the-art language model. Our method significantly outperforms the state-of-the-art on five text classification tasks, reducing the error by 18-24\% on the majority of datasets. We open-source our pretrained models and code to enable adoption by the community.},
	journaltitle = {{arXiv}:1801.06146 [cs, stat]},
	author = {Howard, Jeremy and Ruder, Sebastian},
	urldate = {2018-01-22},
	date = {2018-01-18},
	eprinttype = {arxiv},
	eprint = {1801.06146},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning},
}

@article{andreas_analogs_2017,
	title = {Analogs of Linguistic Structure in Deep Representations},
	url = {http://arxiv.org/abs/1707.08139},
	abstract = {We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a "syntax" with functional analogues to qualitative properties of natural language.},
	journaltitle = {{arXiv}:1707.08139 [cs]},
	author = {Andreas, Jacob and Klein, Dan},
	urldate = {2018-01-19},
	date = {2017-07-25},
	eprinttype = {arxiv},
	eprint = {1707.08139},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@book{david_barber_bayesian_nodate,
	title = {Bayesian Reasoning and Machine Learning},
	url = {http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090310.pdf},
	author = {{David Barber}},
	urldate = {2017-01-31},
}

@article{chang_becoming_2006,
	title = {Becoming syntactic.},
	volume = {113},
	pages = {234},
	number = {2},
	journaltitle = {Psychological review},
	author = {Chang, Franklin and Dell, Gary S. and Bock, Kathryn},
	date = {2006},
}

@incollection{franklin_chang_computational_nodate,
	title = {Computational Models of Sentence Production: A Dual-Path Approach},
	booktitle = {The Oxford Handbook of Language Production},
	author = {{Franklin Chang} and {Hartmut Fitz}},
}

@article{chang_learning_2015,
	title = {Learning to Search Better Than Your Teacher},
	url = {http://arxiv.org/abs/1502.02206},
	abstract = {Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, {LOLS}, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently, {LOLS} can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications.},
	journaltitle = {{arXiv}:1502.02206 [cs, stat]},
	author = {Chang, Kai-Wei and Krishnamurthy, Akshay and Agarwal, Alekh and Daumé {III}, Hal and Langford, John},
	date = {2015-02-07},
	eprinttype = {arxiv},
	eprint = {1502.02206},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{lundberg_unified_2017,
	title = {A Unified Approach to Interpreting Model Predictions},
	url = {http://arxiv.org/abs/1705.07874},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, {SHAP} ({SHapley} Additive {exPlanations}). {SHAP} assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	journaltitle = {{arXiv}:1705.07874 [cs, stat]},
	author = {Lundberg, Scott and Lee, Su-In},
	date = {2017-05-22},
	eprinttype = {arxiv},
	eprint = {1705.07874},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
}

@incollection{kusner_counterfactual_2017,
	title = {Counterfactual Fairness},
	url = {http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf},
	pages = {4069--4079},
	booktitle = {Advances in Neural Information Processing Systems 30},
	publisher = {Curran Associates, Inc.},
	author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	date = {2017},
}

@article{dong_learning_2017,
	title = {Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon},
	url = {http://arxiv.org/abs/1705.07565},
	abstract = {How to develop slim and accurate deep neural networks has become crucial for real- world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. Therefore, there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods.},
	journaltitle = {{arXiv}:1705.07565 [cs]},
	author = {Dong, Xin and Chen, Shangyu and Pan, Sinno Jialin},
	date = {2017-05-22},
	eprinttype = {arxiv},
	eprint = {1705.07565},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{draelos_neurogenesis_2016,
	title = {Neurogenesis Deep Learning},
	url = {http://arxiv.org/abs/1612.03770},
	abstract = {Neural machine learning methods, such as deep neural networks ({DNN}), have achieved remarkable success in a number of complex data processing tasks. These methods have arguably had their strongest impact on tasks such as image and audio processing - data processing domains in which humans have long held clear advantages over conventional algorithms. In contrast to biological neural systems, which are capable of learning continuously, deep artificial networks have a limited ability for incorporating new information in an already trained network. As a result, methods for continuous learning are potentially highly impactful in enabling the application of deep networks to dynamic data sets. Here, inspired by the process of adult neurogenesis in the hippocampus, we explore the potential for adding new neurons to deep layers of artificial neural networks in order to facilitate their acquisition of novel information while preserving previously trained data representations. Our results on the {MNIST} handwritten digit dataset and the {NIST} {SD} 19 dataset, which includes lower and upper case letters and digits, demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms.},
	journaltitle = {{arXiv}:1612.03770 [cs, stat]},
	author = {Draelos, Timothy J. and Miner, Nadine E. and Lamb, Christopher C. and Vineyard, Craig M. and Carlson, Kristofor D. and James, Conrad D. and Aimone, James B.},
	urldate = {2017-02-27},
	date = {2016-12-12},
	eprinttype = {arxiv},
	eprint = {1612.03770},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{andreas_neural_2016,
	title = {Neural module networks},
	url = {http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Andreas_Neural_Module_Networks_CVPR_2016_paper.html},
	pages = {39--48},
	booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
	urldate = {2017-03-06},
	date = {2016},
}

@article{miikkulainen_evolving_2017,
	title = {Evolving Deep Neural Networks},
	url = {http://arxiv.org/abs/1703.00548},
	abstract = {The success of deep learning depends on finding an architecture to fit the task. As deep learning has scaled up to more challenging tasks, the architectures have become difficult to design by hand. This paper proposes an automated method, {CoDeepNEAT}, for optimizing deep learning architectures through evolution. By extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling. It also supports building a real-world application of automated image captioning on a magazine website. Given the anticipated increases in available computing power, evolution of deep networks is promising approach to constructing deep learning applications in the future.},
	journaltitle = {{arXiv}:1703.00548 [cs]},
	author = {Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Dan and Francon, Olivier and Raju, Bala and Navruzyan, Arshak and Duffy, Nigel and Hodjat, Babak},
	urldate = {2017-03-03},
	date = {2017-03-01},
	eprinttype = {arxiv},
	eprint = {1703.00548},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@article{morerio_curriculum_2017,
	title = {Curriculum Dropout},
	url = {http://arxiv.org/abs/1703.06229},
	abstract = {Dropout is a very effective way of regularizing neural networks. Stochastically "dropping out" units with a certain probability discourages over-specific co-adaptations of feature detectors, preventing overfitting and improving network generalization. Besides, Dropout can be interpreted as an approximate model aggregation technique, where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. In this paper, we show that using a fixed dropout probability during training is a suboptimal choice. We thus propose a time scheduling for the probability of retaining neurons in the network. This induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem. This idea of "starting easy" and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models. Indeed, we prove that our optimization strategy implements a very general curriculum scheme, by gradually adding noise to both the input and intermediate feature representations within the network architecture. Experiments on seven image classification datasets and different network architectures show that our method, named Curriculum Dropout, frequently yields to better generalization and, at worst, performs just as well as the standard Dropout method.},
	journaltitle = {{arXiv}:1703.06229 [cs, stat]},
	author = {Morerio, Pietro and Cavazza, Jacopo and Volpi, Riccardo and Vidal, Rene and Murino, Vittorio},
	urldate = {2017-03-22},
	date = {2017-03-17},
	eprinttype = {arxiv},
	eprint = {1703.06229},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{celikyilmaz_scaffolding_2017,
	title = {Scaffolding Networks for Teaching and Learning to Comprehend},
	url = {http://arxiv.org/abs/1702.08653},
	abstract = {In scaffolding teaching, students are gradually asked questions to build background knowledge, clear up confusions, learn to be attentive, and improve comprehension. Inspired by this approach, we explore methods for teaching machines to learn to reason over text documents through asking questions about the past information. We address three key challenges in teaching and learning to reason: 1) the need for an effective architecture that learns from the information in text and keeps it in memory; 2) the difficulty of self-assessing what is learned at any given point and what is left to be learned; 3) the difficulty of teaching reasoning in a scalable way. To address the first challenge, we present the Scaffolding Network, an attention-based neural network agent that can reason over a dynamic memory. It learns a policy using reinforcement learning to incrementally register new information about concepts and their relations. For the second challenge, we describe a question simulator as part of the scaffolding network that learns to continuously question the agent about the information processed so far. Through questioning, the agent learns to correctly answer as many questions as possible. For the last challenge, we explore training with reduced annotated data. We evaluate on synthetic and real datasets, demonstrating that our model competes well with the state-of-the-art methods, especially when less supervision is used.},
	journaltitle = {{arXiv}:1702.08653 [cs]},
	author = {Celikyilmaz, Asli and Deng, Li and Li, Lihong and Wang, Chong},
	urldate = {2017-03-02},
	date = {2017-02-28},
	eprinttype = {arxiv},
	eprint = {1702.08653},
	keywords = {Computer Science - Computation and Language},
}

@article{graves_automated_2017,
	title = {Automated Curriculum Learning for Neural Networks},
	url = {https://arxiv.org/abs/1704.03003},
	journaltitle = {{arXiv} preprint {arXiv}:1704.03003},
	author = {Graves, Alex and Bellemare, Marc G. and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
	urldate = {2017-04-15},
	date = {2017},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@incollection{jitkrittum_linear-time_2017,
	title = {A Linear-Time Kernel Goodness-of-Fit Test},
	url = {http://papers.nips.cc/paper/6630-a-linear-time-kernel-goodness-of-fit-test.pdf},
	pages = {261--270},
	booktitle = {Advances in Neural Information Processing Systems 30},
	publisher = {Curran Associates, Inc.},
	author = {Jitkrittum, Wittawat and Xu, Wenkai and Szabo, Zoltan and Fukumizu, Kenji and Gretton, Arthur},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	date = {2017},
}

@incollection{tucker_rebar:_2017,
	title = {{REBAR}: Low-variance, unbiased gradient estimates for discrete latent variable models},
	url = {http://papers.nips.cc/paper/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models.pdf},
	shorttitle = {{REBAR}},
	pages = {2624--2633},
	booktitle = {Advances in Neural Information Processing Systems 30},
	publisher = {Curran Associates, Inc.},
	author = {Tucker, George and Mnih, Andriy and Maddison, Chris J and Lawson, John and Sohl-Dickstein, Jascha},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	date = {2017},
}

@incollection{remes_non-stationary_2017,
	title = {Non-Stationary Spectral Kernels},
	url = {http://papers.nips.cc/paper/7050-non-stationary-spectral-kernels.pdf},
	pages = {4645--4654},
	booktitle = {Advances in Neural Information Processing Systems 30},
	publisher = {Curran Associates, Inc.},
	author = {Remes, Sami and Heinonen, Markus and Kaski, Samuel},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	date = {2017},
}

@article{nickel_poincar$backslash$e_2017,
	title = {Poincar\${\textbackslash}backslash\$'e Embeddings for Learning Hierarchical Representations},
	journaltitle = {{arXiv} preprint {arXiv}:1705.08039},
	author = {Nickel, Maximilian and Kiela, Douwe},
	date = {2017},
}

@article{achille_emergence_2017,
	title = {Emergence of Invariance and Disentangling in Deep Representations},
	url = {http://arxiv.org/abs/1706.01350},
	abstract = {Using established principles from Information Theory and Statistics, we show that in a deep neural network invariance to nuisance factors is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then show that, in order to avoid memorization, we need to limit the quantity of information stored in the weights, which leads to a novel usage of the Information Bottleneck Lagrangian on the weights as a learning criterion. This also has an alternative interpretation as minimizing a {PAC}-Bayesian bound on the test error. Finally, we exploit a duality between weights and activations induced by the architecture, to show that the information in the weights bounds the minimality and Total Correlation of the layers, therefore showing that regularizing the weights explicitly or implicitly, using {SGD}, not only helps avoid overfitting, but also fosters invariance and disentangling of the learned representation. The theory also enables predicting sharp phase transitions between underfitting and overfitting random labels at precise information values, and sheds light on the relation between the geometry of the loss function, in particular so-called "flat minima," and generalization.},
	journaltitle = {{arXiv}:1706.01350 [cs, stat]},
	author = {Achille, Alessandro and Soatto, Stefano},
	date = {2017-06-05},
	eprinttype = {arxiv},
	eprint = {1706.01350},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
}

@article{botha_natural_2017,
	title = {Natural Language Processing with Small Feed-Forward Networks},
	url = {http://arxiv.org/abs/1708.00214},
	abstract = {We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.},
	journaltitle = {{arXiv}:1708.00214 [cs]},
	author = {Botha, Jan A. and Pitler, Emily and Ma, Ji and Bakalov, Anton and Salcianu, Alex and Weiss, David and {McDonald}, Ryan and Petrov, Slav},
	date = {2017-08-01},
	eprinttype = {arxiv},
	eprint = {1708.00214},
	keywords = {68T50, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, I.2.7},
}

@article{matiisen_teacher-student_2017,
	title = {Teacher-Student Curriculum Learning},
	url = {http://arxiv.org/abs/1707.00183},
	abstract = {We propose Teacher-Student Curriculum Learning ({TSCL}), a framework for automatic curriculum learning, where the Student tries to learn a complex task and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e. where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that {TSCL} matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with {LSTM} and navigation in Minecraft. Using our automatically generated curriculum enabled to solve a Minecraft maze that could not be solved at all when training directly on solving the maze, and the learning was an order of magnitude faster than uniform sampling of subtasks.},
	journaltitle = {{arXiv}:1707.00183 [cs]},
	author = {Matiisen, Tambet and Oliver, Avital and Cohen, Taco and Schulman, John},
	date = {2017-07-01},
	eprinttype = {arxiv},
	eprint = {1707.00183},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning},
}

@article{harrison_guiding_2017,
	title = {Guiding Reinforcement Learning Exploration Using Natural Language},
	url = {http://arxiv.org/abs/1707.08616},
	abstract = {In this work we present a technique to use natural language to help reinforcement learning generalize to unseen environments. This technique uses neural machine translation, specifically the use of encoder-decoder networks, to learn associations between natural language behavior descriptions and state-action information. We then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effective at learning in unseen environments. We evaluate this technique using the popular arcade game, Frogger, under ideal and non-ideal conditions. This evaluation shows that our modified policy shaping algorithm improves over a Q-learning agent as well as a baseline version of policy shaping.},
	journaltitle = {{arXiv}:1707.08616 [cs, stat]},
	author = {Harrison, Brent and Ehsan, Upol and Riedl, Mark O.},
	date = {2017-07-26},
	eprinttype = {arxiv},
	eprint = {1707.08616},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning},
}
@article{potapenko_interpretable_2017,
	title = {Interpretable probabilistic embeddings: bridging the gap between topic models and neural networks},
	url = {http://arxiv.org/abs/1711.04154},
	shorttitle = {Interpretable probabilistic embeddings},
	abstract = {We consider probabilistic topic models and more recent word embedding techniques from a perspective of learning hidden semantic representations. Inspired by a striking similarity of the two approaches, we merge them and learn probabilistic embeddings with online {EM}-algorithm on word co-occurrence data. The resulting embeddings perform on par with Skip-Gram Negative Sampling ({SGNS}) on word similarity tasks and benefit in the interpretability of the components. Next, we learn probabilistic document embeddings that outperform paragraph2vec on a document similarity task and require less memory and time for training. Finally, we employ multimodal Additive Regularization of Topic Models ({ARTM}) to obtain a high sparsity and learn embeddings for other modalities, such as timestamps and categories. We observe further improvement of word similarity performance and meaningful inter-modality similarities.},
	journaltitle = {{arXiv}:1711.04154 [cs]},
	author = {Potapenko, Anna and Popov, Artem and Vorontsov, Konstantin},
	date = {2017-11-11},
	eprinttype = {arxiv},
	eprint = {1711.04154},
	keywords = {Computer Science - Computation and Language},
}

@inreference{noauthor_neuroconstructivism_2017,
	title = {Neuroconstructivism},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Neuroconstructivism&oldid=805918247},
	abstract = {Neuroconstructivism is a theory that states that gene–gene interaction, gene–environment interaction and, crucially, ontogeny are all considered to play a vital role in how the brain progressively sculpts itself and how it gradually becomes specialized over developmental time.
Supporters of neuroconstructivism, such as Annette Karmiloff-Smith, argue against innate modularity of mind, the notion that a brain is composed of innate neural structures or modules which have distinct established evolutionarily developed functions. Instead, emphasis is put on innate domain relevant biases. These biases are understood as aiding learning and directing attention. Module-like structures are therefore the product of both experience and these innate biases. Neuroconstructivism can therefore be seen as a bridge between Jerry Fodor's psychological nativism and Jean Piaget's theory of cognitive development.},
	booktitle = {Wikipedia},
	date = {2017-10-18},
	langid = {english},
	note = {Page Version {ID}: 805918247},
}

@article{milli_interpretable_2017,
	title = {Interpretable and Pedagogical Examples},
	url = {http://arxiv.org/abs/1711.00694},
	abstract = {Teachers intentionally pick the most informative examples to show their students. However, if the teacher and student are neural networks, the examples that the teacher network learns to give, although effective at teaching the student, are typically uninterpretable. We show that training the student and teacher iteratively, rather than jointly, can produce interpretable teaching strategies. We evaluate interpretability by (1) measuring the similarity of the teacher's emergent strategies to intuitive strategies in each domain and (2) conducting human experiments to evaluate how effective the teacher's strategies are at teaching humans. We show that the teacher network learns to select or generate interpretable, pedagogical examples to teach rule-based, probabilistic, boolean, and hierarchical concepts.},
	journaltitle = {{arXiv}:1711.00694 [cs]},
	author = {Milli, Smitha and Abbeel, Pieter and Mordatch, Igor},
	date = {2017-11-02},
	eprinttype = {arxiv},
	eprint = {1711.00694},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{taghvaei_how_2017,
	title = {How regularization affects the critical points in linear networks},
	url = {http://arxiv.org/abs/1709.09625},
	abstract = {This paper is concerned with the problem of representing and learning a linear transformation using a linear neural network. In recent years, there has been a growing interest in the study of such networks in part due to the successes of deep learning. The main question of this body of research and also of this paper pertains to the existence and optimality properties of the critical points of the mean-squared loss function. The primary concern here is the robustness of the critical points with regularization of the loss function. An optimal control model is introduced for this purpose and a learning algorithm (regularized form of backprop) derived for the same using the Hamilton's formulation of optimal control. The formulation is used to provide a complete characterization of the critical points in terms of the solutions of a nonlinear matrix-valued equation, referred to as the characteristic equation. Analytical and numerical tools from bifurcation theory are used to compute the critical points via the solutions of the characteristic equation. The main conclusion is that the critical point diagram can be fundamentally different even with arbitrary small amounts of regularization.},
	journaltitle = {{arXiv}:1709.09625 [cs, math]},
	author = {Taghvaei, Amirhossein and Kim, Jin W. and Mehta, Prashant G.},
	date = {2017-09-27},
	eprinttype = {arxiv},
	eprint = {1709.09625},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control},
}

@online{noauthor_[1705.09280]_nodate,
	title = {[1705.09280] Implicit Regularization in Matrix Factorization},
	url = {https://arxiv.org/abs/1705.09280},
	urldate = {2017-11-09},
}

@article{song_complexity_2017,
	title = {On the Complexity of Learning Neural Networks},
	url = {http://arxiv.org/abs/1707.04615},
	abstract = {The stunning empirical successes of neural networks currently lack rigorous theoretical explanation. What form would such an explanation take, in the face of existing complexity-theoretic lower bounds? A first step might be to show that data generated by neural networks with a single hidden layer, smooth activation functions and benign input distributions can be learned efficiently. We demonstrate here a comprehensive lower bound ruling out this possibility: for a wide class of activation functions (including all currently used), and inputs drawn from any logconcave distribution, there is a family of one-hidden-layer functions whose output is a sum gate, that are hard to learn in a precise sense: any statistical query algorithm (which includes all known variants of stochastic gradient descent with any loss function) needs an exponential number of queries even using tolerance inversely proportional to the input dimensionality. Moreover, this hard family of functions is realizable with a small (sublinear in dimension) number of activation units in the single hidden layer. The lower bound is also robust to small perturbations of the true weights. Systematic experiments illustrate a phase transition in the training error as predicted by the analysis.},
	journaltitle = {{arXiv}:1707.04615 [cs]},
	author = {Song, Le and Vempala, Santosh and Wilmes, John and Xie, Bo},
	date = {2017-07-14},
	eprinttype = {arxiv},
	eprint = {1707.04615},
	keywords = {Computer Science - Computational Complexity, Computer Science - Learning},
}

@article{aghasi_net-trim:_2016,
	title = {Net-Trim: A Layer-wise Convex Pruning of Deep Neural Networks},
	url = {http://arxiv.org/abs/1611.05162},
	shorttitle = {Net-Trim},
	abstract = {Model reduction is a highly desirable process for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance. Net-Trim is a layer-wise convex framework to prune (sparsify) deep neural networks. The method is applicable to neural networks operating with the rectified linear unit ({ReLU}) as the nonlinear activation. The basic idea is to retrain the network layer by layer keeping the layer inputs and outputs close to the originally trained model, while seeking a sparse transform matrix. We present both the parallel and cascade versions of the algorithm. While the former enjoys computational distributability, the latter is capable of achieving simpler models. In both cases, we mathematically show a consistency between the retrained model and the initial trained network. We also derive the general sufficient conditions for the recovery of a sparse transform matrix. In the case of standard Gaussian training samples of dimension \$N\$ being fed to a layer, and \$s\$ being the maximum number of nonzero terms across all columns of the transform matrix, we show that \${\textbackslash}mathcal\{O\}(s{\textbackslash}log N)\$ samples are enough to accurately learn the layer model.},
	journaltitle = {{arXiv}:1611.05162 [cs, stat]},
	author = {Aghasi, Alireza and Nguyen, Nam and Romberg, Justin},
	date = {2016-11-16},
	eprinttype = {arxiv},
	eprint = {1611.05162},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{ming_understanding_2017,
	title = {Understanding Hidden Memories of Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1710.10777},
	abstract = {Recurrent neural networks ({RNNs}) have been successfully applied to various natural language processing ({NLP}) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing {RNN} models for {NLP} tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on {RNNs}' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an {RNN}'s hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.},
	journaltitle = {{arXiv}:1710.10777 [cs]},
	author = {Ming, Yao and Cao, Shaozu and Zhang, Ruixiang and Li, Zhen and Chen, Yuanzhe and Song, Yangqiu and Qu, Huamin},
	date = {2017-10-30},
	eprinttype = {arxiv},
	eprint = {1710.10777},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{hill_understanding_2017,
	title = {Understanding Grounded Language Learning Agents},
	url = {http://arxiv.org/abs/1710.09867},
	abstract = {Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds. To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words. While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, {AI} researchers and practitioners currently lack a clear understanding of exactly how they do so. Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions. For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world. We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge. We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.},
	journaltitle = {{arXiv}:1710.09867 [cs]},
	author = {Hill, Felix and Hermann, Karl Moritz and Blunsom, Phil and Clark, Stephen},
	date = {2017-10-26},
	eprinttype = {arxiv},
	eprint = {1710.09867},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@article{neklyudov_structured_2017,
	title = {Structured Bayesian Pruning via Log-Normal Multiplicative Noise},
	url = {http://arxiv.org/abs/1705.07283},
	abstract = {Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves generalization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However, this sparsity can hardly be used for acceleration since it is unstructured. In the paper, we propose a new Bayesian model that takes into account the computational structure of neural networks and provides structured sparsity, e.g. removes neurons and/or convolutional channels in {CNNs}. To do this, we inject noise to the neurons outputs while keeping the weights unregularized. We established the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the {KL}-term in the evidence lower bound is computed in closed-form. The model leads to structured sparsity by removing elements with a low {SNR} from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is very easy to implement as it only corresponds to the addition of one dropout-like layer in computation graph.},
	journaltitle = {{arXiv}:1705.07283 [stat]},
	author = {Neklyudov, Kirill and Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
	date = {2017-05-20},
	eprinttype = {arxiv},
	eprint = {1705.07283},
	keywords = {Statistics - Machine Learning},
}

@article{louizos_bayesian_2017,
	title = {Bayesian Compression for Deep Learning},
	url = {http://arxiv.org/abs/1705.08665},
	abstract = {Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by taking a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.},
	journaltitle = {{arXiv}:1705.08665 [cs, stat]},
	author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
	date = {2017-05-24},
	eprinttype = {arxiv},
	eprint = {1705.08665},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{zagoruyko_paying_2016,
	title = {Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer},
	url = {http://arxiv.org/abs/1612.03928},
	shorttitle = {Paying More Attention to Attention},
	abstract = {Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and {NLP}. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student {CNN} network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at https://github.com/szagoruyko/attention-transfer},
	journaltitle = {{arXiv}:1612.03928 [cs]},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	date = {2016-12-12},
	eprinttype = {arxiv},
	eprint = {1612.03928},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{andreas_translating_2017,
	title = {Translating Neuralese},
	url = {http://arxiv.org/abs/1704.06960},
	abstract = {Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.},
	journaltitle = {{arXiv}:1704.06960 [cs]},
	author = {Andreas, Jacob and Dragan, Anca and Klein, Dan},
	date = {2017-04-23},
	eprinttype = {arxiv},
	eprint = {1704.06960},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@article{neyshabur_geometry_2017,
	title = {Geometry of Optimization and Implicit Regularization in Deep Learning},
	url = {http://arxiv.org/abs/1705.03071},
	abstract = {We argue that the optimization plays a crucial role in generalization of deep learning models through implicit regularization. We do this by demonstrating that generalization ability is not controlled by network size but rather by some other implicit control. We then demonstrate how changing the empirical optimization procedure can improve generalization, even if actual optimization quality is not affected. We do so by studying the geometry of the parameter space of deep networks, and devising an optimization algorithm attuned to this geometry.},
	journaltitle = {{arXiv}:1705.03071 [cs]},
	author = {Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov, Ruslan and Srebro, Nathan},
	date = {2017-05-08},
	eprinttype = {arxiv},
	eprint = {1705.03071},
	keywords = {Computer Science - Learning},
}

@article{wei_learning_2017,
	title = {Learning to Transfer},
	url = {http://arxiv.org/abs/1708.05629},
	abstract = {Transfer learning borrows knowledge from a source domain to facilitate learning in a target domain. Two primary issues to be addressed in transfer learning are what and how to transfer. For a pair of domains, adopting different transfer learning algorithms results in different knowledge transferred between them. To discover the optimal transfer learning algorithm that maximally improves the learning performance in the target domain, researchers have to exhaustively explore all existing transfer learning algorithms, which is computationally intractable. As a trade-off, a sub-optimal algorithm is selected, which requires considerable expertise in an ad-hoc way. Meanwhile, it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices. Motivated by this, we propose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences. We establish the L2T framework in two stages: 1) we first learn a reflection function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer for a newly arrived pair of domains by optimizing the reflection function. Extensive experiments demonstrate the L2T's superiority over several state-of-the-art transfer learning algorithms and its effectiveness on discovering more transferable knowledge.},
	journaltitle = {{arXiv}:1708.05629 [cs, stat]},
	author = {Wei, Ying and Zhang, Yu and Yang, Qiang},
	urldate = {2017-08-26},
	date = {2017-08-18},
	eprinttype = {arxiv},
	eprint = {1708.05629},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
}

@article{li_large-scale_2017,
	title = {Large-Scale Domain Adaptation via Teacher-Student Learning},
	url = {http://arxiv.org/abs/1708.05466},
	abstract = {High accuracy speech recognition requires a large amount of transcribed data for supervised training. In the absence of such data, domain adaptation of a well-trained acoustic model can be performed, but even here, high accuracy usually requires significant labeled data from the target domain. In this work, we propose an approach to domain adaptation that does not require transcriptions but instead uses a corpus of unlabeled parallel data, consisting of pairs of samples from the source domain of the well-trained model and the desired target domain. To perform adaptation, we employ teacher/student (T/S) learning, in which the posterior probabilities generated by the source-domain model can be used in lieu of labels to train the target-domain model. We evaluate the proposed approach in two scenarios, adapting a clean acoustic model to noisy speech and adapting an adults speech acoustic model to children speech. Significant improvements in accuracy are obtained, with reductions in word error rate of up to 44\% over the original source model without the need for transcribed data in the target domain. Moreover, we show that increasing the amount of unlabeled data results in additional model robustness, which is particularly beneficial when using simulated training data in the target-domain.},
	journaltitle = {{arXiv}:1708.05466 [cs]},
	author = {Li, Jinyu and Seltzer, Michael L. and Wang, Xi and Zhao, Rui and Gong, Yifan},
	urldate = {2017-08-26},
	date = {2017-08-17},
	eprinttype = {arxiv},
	eprint = {1708.05466},
	keywords = {Computer Science - Computation and Language},
}

@article{brock_smash:_2017,
	title = {{SMASH}: One-Shot Model Architecture Search through {HyperNetworks}},
	url = {http://arxiv.org/abs/1708.05344},
	shorttitle = {{SMASH}},
	abstract = {Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary {HyperNet} that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with {HyperNet}-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with {ResNet}, {DenseNet}, and {FractalNet} blocks as special cases. We validate our method ({SMASH}) on {CIFAR}-10 and {CIFAR}-100, {STL}-10, {ModelNet}10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks. Our code is available at https://github.com/ajbrock/{SMASH}},
	journaltitle = {{arXiv}:1708.05344 [cs]},
	author = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
	urldate = {2017-08-21},
	date = {2017-08-17},
	eprinttype = {arxiv},
	eprint = {1708.05344},
	keywords = {Computer Science - Learning},
}

@inproceedings{nema_diversity_2017,
	location = {Vancouver, Canada},
	title = {Diversity driven attention model for query-based abstractive summarization},
	url = {http://aclweb.org/anthology/P17-1098},
	abstract = {Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28\% (absolute) in {ROUGE}-L scores.},
	pages = {1063--1072},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Nema, Preksha and Khapra, Mitesh M. and Laha, Anirban and Ravindran, Balaraman},
	urldate = {2017-08-14},
	date = {2017-07},
}

@inproceedings{tan_abstractive_2017,
	location = {Vancouver, Canada},
	title = {Abstractive Document Summarization with a Graph-Based Attentional Neural Model},
	url = {http://aclweb.org/anthology/P17-1108},
	abstract = {Abstractive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization, which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.},
	pages = {1171--1181},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Tan, Jiwei and Wan, Xiaojun and Xiao, Jianguo},
	urldate = {2017-08-14},
	date = {2017-07},
}

@inproceedings{cotterell_probabilistic_2017,
	location = {Vancouver, Canada},
	title = {Probabilistic Typology: Deep Generative Models of Vowel Inventories},
	url = {http://aclweb.org/anthology/P17-1109},
	shorttitle = {Probabilistic Typology},
	abstract = {Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most—but not all—languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.},
	pages = {1182--1192},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Cotterell, Ryan and Eisner, Jason},
	urldate = {2017-08-14},
	date = {2017-07},
}

@inproceedings{ji_neural_2017,
	location = {Vancouver, Canada},
	title = {Neural Discourse Structure for Text Categorization},
	url = {http://aclweb.org/anthology/P17-1092},
	abstract = {We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both {RST} and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.},
	pages = {996--1005},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Ji, Yangfeng and Smith, Noah A.},
	urldate = {2017-08-14},
	date = {2017-07},
}

@inproceedings{wang_naturalizing_2017,
	location = {Vancouver, Canada},
	title = {Naturalizing a Programming Language via Interactive Learning},
	url = {http://aclweb.org/anthology/P17-1086},
	abstract = {Our goal is to create a convenient natural language interface for performing well-specified but complex actions such as analyzing data, manipulating text, and querying databases. However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language. To bridge this gap, we start with a core programming language and allow users to “naturalize” the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9\% of the last 10K utterances.},
	pages = {929--938},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Sida I. and Ginn, Samuel and Liang, Percy and Manning, Christopher D.},
	urldate = {2017-08-14},
	date = {2017-07},
}

@inproceedings{preotiuc-pietro_beyond_2017,
	location = {Vancouver, Canada},
	title = {Beyond Binary Labels: Political Ideology Prediction of Twitter Users},
	url = {http://aclweb.org/anthology/P17-1068},
	shorttitle = {Beyond Binary Labels},
	abstract = {Automatic political orientation prediction from social media posts has to date proven successful only in distinguishing between publicly declared liberals and conservatives in the {US}. This study examines users’ political ideology using a seven-point scale which enables us to identify politically moderate and neutral users – groups which are of particular interest to political scientists and pollsters. Using a novel data set with political ideology labels self-reported through surveys, our goal is two-fold: a) to characterize the groups of politically engaged users through language use on Twitter; b) to build a fine-grained model that predicts political ideology of unseen users. Our results identify differences in both political leaning and engagement and the extent to which each group tweets using political keywords. Finally, we demonstrate how to improve ideology prediction accuracy by exploiting the relationships between the user groups.},
	pages = {729--740},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Preoţiuc-Pietro, Daniel and Liu, Ye and Hopkins, Daniel and Ungar, Lyle},
	urldate = {2017-08-14},
	date = {2017-07},
}

@inproceedings{tan_friendships_2017,
	location = {Vancouver, Canada},
	title = {Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts},
	url = {http://aclweb.org/anthology/P17-1072},
	shorttitle = {Friendships, Rivalries, and Trysts},
	abstract = {Understanding how ideas relate to each other is a fundamental question in many domains, ranging from intellectual history to public communication. Because ideas are naturally embedded in texts, we propose the first framework to systematically characterize the relations between ideas based on their occurrence in a corpus of documents, independent of how these ideas are represented. Combining two statistics—cooccurrence within documents and prevalence correlation over time—our approach reveals a number of different ways in which ideas can cooperate and compete. For instance, two ideas can closely track each other’s prevalence over time, and yet rarely cooccur, almost like a “cold war” scenario. We observe that pairwise cooccurrence and prevalence correlation exhibit different distributions. We further demonstrate that our approach is able to uncover intriguing relations between ideas through in-depth case studies on news articles and research papers.},
	pages = {773--783},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Tan, Chenhao and Card, Dallas and Smith, Noah A.},
	urldate = {2017-08-14},
	date = {2017-07},
}

@inproceedings{belinkov_what_2017,
	location = {Vancouver, Canada},
	title = {What do Neural Machine Translation Models Learn about Morphology?},
	url = {http://aclweb.org/anthology/P17-1080},
	abstract = {Neural machine translation ({MT}) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural {MT} models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural {MT} system and its ability to capture word structure.},
	pages = {861--872},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
	urldate = {2017-08-14},
	date = {2017-07},
}

@inproceedings{kim_domain_2017,
	location = {Vancouver, Canada},
	title = {Domain Attention with an Ensemble of Experts},
	url = {http://aclweb.org/anthology/P17-1060},
	abstract = {An important problem in domain adaptation is to quickly generalize to a new domain with limited supervision given K existing domains. One approach is to retrain a global model across all K + 1 domains using standard techniques, for instance Daum´e {III} (2009). However, it is desirable to adapt without having to re-estimate a global model from scratch each time a new domain with potentially new intents and slots is added. We describe a solution based on attending an ensemble of domain experts. We assume K domain specific intent and slot models trained on respective domains. When given domain K + 1, our model uses a weighted combination of the K domain experts’ feedback along with its own opinion to make predictions on the new domain. In experiments, the model significantly outperforms baselines that do not use domain adaptation and also performs better than the full retraining approach.},
	pages = {643--653},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Young-Bum and Stratos, Karl and Kim, Dongchan},
	date = {2017-07},
}

@inproceedings{ma_detect_2017,
	location = {Vancouver, Canada},
	title = {Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning},
	url = {http://aclweb.org/anthology/P17-1066},
	abstract = {How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-of-the-art rumor detection models.},
	pages = {708--717},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Ma, Jing and Gao, Wei and Wong, Kam-Fai},
	date = {2017-07},
}

@inproceedings{bollmann_learning_2017,
	location = {Vancouver, Canada},
	title = {Learning attention for historical text normalization by learning to pronounce},
	url = {http://aclweb.org/anthology/P17-1031},
	abstract = {Automated processing of historical texts often relies on pre-normalization to modern word forms. Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task. We address this problem by using several novel encoder-decoder architectures, including a multi-task learning ({MTL}) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-the-art by an absolute 2\% increase in performance. We analyze the induced models across 44 different texts from Early New High German. Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms. This, we believe, is an important step toward understanding how {MTL} works.},
	pages = {332--344},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Bollmann, Marcel and Bingel, Joachim and Søgaard, Anders},
	date = {2017-07},
}

@inproceedings{abend_state_2017,
	location = {Vancouver, Canada},
	title = {The State of the Art in Semantic Representation},
	url = {http://aclweb.org/anthology/P17-1008},
	abstract = {Semantic representation is receiving growing attention in {NLP} in the past few years, and many proposals for semantic schemes (e.g., {AMR}, {UCCA}, {GMB}, {UDS}) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.},
	pages = {77--89},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Abend, Omri and Rappoport, Ari},
	date = {2017-07},
}

@inproceedings{gittens_skip-gram_2017,
	location = {Vancouver, Canada},
	title = {Skip-Gram - Zipf + Uniform = Vector Additivity},
	url = {http://aclweb.org/anthology/P17-1007},
	abstract = {In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected "side-effect" of such models is that their vectors often exhibit compositionality, i.e., {\textbackslash}emphadding two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., "man" + "royal" = "king". This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction ({SDR}) framework of Globerson and Tishby: the parameters of {SDR} models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit {SDR} models.},
	pages = {69--76},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Gittens, Alex and Achlioptas, Dimitris and Mahoney, Michael W.},
	date = {2017-07},
}

@inproceedings{cheng_learning_2017,
	location = {Vancouver, Canada},
	title = {Learning Structured Natural Language Representations for Semantic Parsing},
	url = {http://aclweb.org/anthology/P17-1005},
	abstract = {We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art on {SPADES} and {GRAPHQUESTIONS} and obtain competitive results on {GEOQUERY} and {WEBQUESTIONS}. The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are dif- ferent from linguistically motivated ones.},
	pages = {44--55},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Cheng, Jianpeng and Reddy, Siva and Saraswat, Vijay and Lapata, Mirella},
	date = {2017-07},
}

@inproceedings{liu_adversarial_2017,
	location = {Vancouver, Canada},
	title = {Adversarial Multi-task Learning for Text Classification},
	url = {http://aclweb.org/anthology/P17-1001},
	abstract = {Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at http://nlp.fudan.edu.cn/data/.},
	pages = {1--10},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
	date = {2017-07},
}

@inproceedings{sasaki_other_2017,
	location = {Vancouver, Canada},
	title = {Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization},
	url = {http://aclweb.org/anthology/P17-1037},
	shorttitle = {Other Topics You May Also Agree or Disagree},
	abstract = {We presents in this paper our approach for modeling inter-topic preferences of Twitter users: for example, "those who agree with the Trans-Pacific Partnership ({TPP}) also agree with free trade". This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion survey, electoral prediction, electoral campaigns, and online debates. In order to extract users' preferences on Twitter, we design linguistic patterns in which people agree and disagree about specific topics (e.g., "A is completely wrong”). By applying these linguistic patterns to a collection of tweets, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization: representing users' preference as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our presented approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences.},
	pages = {398--408},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Sasaki, Akira and Hanawa, Kazuaki and Okazaki, Naoaki and Inui, Kentaro},
	date = {2017-07},
}

@inproceedings{croce_deep_2017,
	location = {Vancouver, Canada},
	title = {Deep Learning in Semantic Kernel Spaces},
	url = {http://aclweb.org/anthology/P17-1032},
	abstract = {Kernel methods enable the direct usage of structured representations of textual data during language learning and inference tasks. Expressive kernels, such as Tree Kernels, achieve excellent performance in {NLP}. On the other side, deep neural networks have been demonstrated effective in automatically learning feature representations during training. However, their input is tensor data, i.e., they can not manage rich structured information. In this paper, we show that expressive kernels and deep neural networks can be combined in a common framework in order to (i) explicitly model structured information and (ii) learn non-linear decision functions. We show that the input layer of a deep architecture can be pre-trained through the application of the Nystrom low-rank approximation of kernel spaces. The resulting “kernelized" neural network achieves state-of-the-art accuracy in three different tasks.},
	pages = {345--354},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Croce, Danilo and Filice, Simone and Castellucci, Giuseppe and Basili, Roberto},
	date = {2017-07},
}

@inproceedings{yin_syntactic_2017,
	location = {Vancouver, Canada},
	title = {A Syntactic Neural Model for General-Purpose Code Generation},
	url = {http://aclweb.org/anthology/P17-1041},
	abstract = {We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.},
	pages = {440--450},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Yin, Pengcheng and Neubig, Graham},
	date = {2017-07},
}

@inproceedings{gan_scalable_2017,
	location = {Vancouver, Canada},
	title = {Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling},
	url = {http://aclweb.org/anthology/P17-1030},
	abstract = {Recurrent neural networks ({RNNs}) have shown promising performance for language modeling. However, traditional training of {RNNs} using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in {RNNs}. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various {RNN} models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.},
	pages = {321--331},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Gan, Zhe and Li, Chunyuan and Chen, Changyou and Pu, Yunchen and Su, Qinliang and Carin, Lawrence},
	date = {2017-07},
}

@inproceedings{liang_neural_2017,
	location = {Vancouver, Canada},
	title = {Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision},
	url = {http://aclweb.org/anthology/P17-1003},
	shorttitle = {Neural Symbolic Machines},
	abstract = {Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural "programmer", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic "computer", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply {REINFORCE} to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of {REINFORCE}, we augment it with an iterative maximum-likelihood training process. {NSM} outperforms the state-of-the-art on the {WebQuestionsSP} dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.},
	pages = {23--33},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Liang, Chen and Berant, Jonathan and Le, Quoc and Forbus, Kenneth D. and Lao, Ni},
	date = {2017-07},
}

@article{glasmachers_limits_2017,
	title = {Limits of End-to-End Learning},
	url = {http://arxiv.org/abs/1704.08305},
	abstract = {End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning system is specifically designed so that all modules are differentiable. In effect, not only a central learning machine, but also all "peripheral" modules like representation learning and memory formation are covered by a holistic learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex. In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of scaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning.},
	journaltitle = {{arXiv}:1704.08305 [cs, stat]},
	author = {Glasmachers, Tobias},
	date = {2017-04-26},
	eprinttype = {arxiv},
	eprint = {1704.08305},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{kahng_activis:_2017,
	title = {{ActiVis}: Visual Exploration of Industry-Scale Deep Neural Network Models},
	url = {http://arxiv.org/abs/1704.01942},
	shorttitle = {{ActiVis}},
	abstract = {While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved {ActiVis}, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance- and subset-level. {ActiVis} has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how {ActiVis} may work with different models.},
	journaltitle = {{arXiv}:1704.01942 [cs, stat]},
	author = {Kahng, Minsuk and Andrews, Pierre and Kalro, Aditya and Chau, Duen Horng},
	urldate = {2017-04-25},
	date = {2017-04-06},
	eprinttype = {arxiv},
	eprint = {1704.01942},
	keywords = {Computer Science - Human-Computer Interaction, Statistics - Machine Learning},
}

@article{balduzzi_shattered_2017,
	title = {The Shattered Gradients Problem: If resnets are the answer, then what is the question?},
	url = {http://arxiv.org/abs/1702.08591},
	shorttitle = {The Shattered Gradients Problem},
	abstract = {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. The problem has largely been overcome through the introduction of carefully constructed initializations and batch normalization. Nevertheless, architectures incorporating skip-connections such as resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise. In contrast, the gradients in architectures with skip-connections are far more resistant to shattering decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new "looks linear" ({LL}) initialization that prevents shattering. Preliminary experiments show the new initialization allows to train very deep networks without the addition of skip-connections.},
	journaltitle = {{arXiv}:1702.08591 [cs, stat]},
	author = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, J. P. and Ma, Kurt Wan-Duo and {McWilliams}, Brian},
	urldate = {2017-04-24},
	date = {2017-02-27},
	eprinttype = {arxiv},
	eprint = {1702.08591},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}
@article{kaplan_beating_2017,
	title = {Beating Atari with Natural Language Guided Reinforcement Learning},
	url = {http://arxiv.org/abs/1704.05539},
	abstract = {We introduce the first deep reinforcement learning agent that learns to beat Atari games with the aid of natural language instructions. The agent uses a multimodal embedding between environment observations and natural language to self-monitor progress through a list of English instructions, granting itself reward for completing instructions in addition to increasing the game score. Our agent significantly outperforms Deep Q-Networks ({DQNs}), Asynchronous Advantage Actor-Critic (A3C) agents, and the best agents posted to {OpenAI} Gym on what is often considered the hardest Atari 2600 environment: Montezuma's Revenge.},
	journaltitle = {{arXiv}:1704.05539 [cs]},
	author = {Kaplan, Russell and Sauer, Christopher and Sosa, Alexander},
	urldate = {2017-04-24},
	date = {2017-04-18},
	eprinttype = {arxiv},
	eprint = {1704.05539},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{gong_adversarial_2017,
	title = {Adversarial and Clean Data Are Not Twins},
	url = {http://arxiv.org/abs/1704.04960},
	abstract = {Adversarial attack has cast a shadow on the massive success of deep neural networks. Despite being almost visually identical to the clean data, the adversarial images can fool deep neural networks into wrong predictions with very high confidence. In this paper, however, we show that we can build a simple binary classifier separating the adversarial apart from the clean data with accuracy over 99\%. We also empirically show that the binary classifier is robust to a second-round adversarial attack. In other words, it is difficult to disguise adversarial samples to bypass the binary classifier. Further more, we empirically investigate the generalization limitation which lingers on all current defensive methods, including the binary classifier approach. And we hypothesize that this is the result of intrinsic property of adversarial crafting algorithms.},
	journaltitle = {{arXiv}:1704.04960 [cs]},
	author = {Gong, Zhitao and Wang, Wenlu and Ku, Wei-Shinn},
	urldate = {2017-04-24},
	date = {2017-04-17},
	eprinttype = {arxiv},
	eprint = {1704.04960},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{jean_does_2017,
	title = {Does Neural Machine Translation Benefit from Larger Context?},
	url = {http://arxiv.org/abs/1704.05135},
	abstract = {We propose a neural machine translation architecture that models the surrounding text in addition to the source sentence. These models lead to better performance, both in terms of general translation quality and pronoun prediction, when trained on small corpora, although this improvement largely disappears when trained with a larger corpus. We also discover that attention-based neural machine translation is well suited for pronoun prediction and compares favorably with other approaches that were specifically designed for this task.},
	journaltitle = {{arXiv}:1704.05135 [cs, stat]},
	author = {Jean, Sebastien and Lauly, Stanislas and Firat, Orhan and Cho, Kyunghyun},
	urldate = {2017-04-24},
	date = {2017-04-17},
	eprinttype = {arxiv},
	eprint = {1704.05135},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning},
}

@article{narang_exploring_2017,
	title = {Exploring Sparsity in Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1704.05119},
	abstract = {Recurrent Neural Networks ({RNN}) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these {RNNs} efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8x and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning {RNNs} reduces the size of the model and can also help achieve significant inference time speed-up using sparse matrix multiply. Benchmarks show that using our technique model size can be reduced by 90\% and speed-up is around 2x to 7x.},
	journaltitle = {{arXiv}:1704.05119 [cs]},
	author = {Narang, Sharan and Diamos, Gregory and Sengupta, Shubho and Elsen, Erich},
	urldate = {2017-04-24},
	date = {2017-04-17},
	eprinttype = {arxiv},
	eprint = {1704.05119},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
}

@article{stahlberg_unfolding_2017,
	title = {Unfolding and Shrinking Neural Machine Translation Ensembles},
	url = {http://arxiv.org/abs/1704.03279},
	abstract = {Ensembling is a well-known technique in neural machine translation ({NMT}). Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the {GPU}. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On Japanese-English we report that the resulting network has the size and decoding speed of a single {NMT} network but performs on the level of a 3-ensemble system.},
	journaltitle = {{arXiv}:1704.03279 [cs]},
	author = {Stahlberg, Felix and Byrne, Bill},
	urldate = {2017-04-24},
	date = {2017-04-11},
	eprinttype = {arxiv},
	eprint = {1704.03279},
	keywords = {Computer Science - Computation and Language},
}

@article{ma_convergence_2017,
	title = {On Convergence Property of Implicit Self-paced Objective},
	url = {http://arxiv.org/abs/1703.09923},
	abstract = {Self-paced learning ({SPL}) is a new methodology that simulates the learning principle of humans/animals to start learning easier aspects of a learning task, and then gradually take more complex examples into training. This new-coming learning regime has been empirically substantiated to be effective in various computer vision and pattern recognition tasks. Recently, it has been proved that the {SPL} regime has a close relationship to a implicit self-paced objective function. While this implicit objective could provide helpful interpretations to the effectiveness, especially the robustness, insights under the {SPL} paradigms, there are still no theoretical results strictly proved to verify such relationship. To this issue, in this paper, we provide some convergence results on this implicit objective of {SPL}. Specifically, we prove that the learning process of {SPL} always converges to critical points of this implicit objective under some mild conditions. This result verifies the intrinsic relationship between {SPL} and this implicit objective, and makes the previous robustness analysis on {SPL} complete and theoretically rational.},
	journaltitle = {{arXiv}:1703.09923 [cs]},
	author = {Ma, Zilu and Liu, Shiqi and Meng, Deyu},
	urldate = {2017-04-03},
	date = {2017-03-29},
	eprinttype = {arxiv},
	eprint = {1703.09923},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{zhang_understanding_2016,
	title = {Understanding deep learning requires rethinking generalization},
	url = {http://arxiv.org/abs/1611.03530},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	journaltitle = {{arXiv}:1611.03530 [cs]},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	urldate = {2017-03-28},
	date = {2016-11-10},
	eprinttype = {arxiv},
	eprint = {1611.03530},
	keywords = {Computer Science - Learning},
}

@article{shalev-shwartz_failures_2017,
	title = {Failures of Deep Learning},
	url = {http://arxiv.org/abs/1703.07950},
	abstract = {In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four families of problems for which some of the commonly used existing algorithms fail or suffer significant difficulty. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.},
	journaltitle = {{arXiv}:1703.07950 [cs]},
	author = {Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
	urldate = {2017-03-28},
	date = {2017-03-23},
	eprinttype = {arxiv},
	eprint = {1703.07950},
	keywords = {Computer Science - Learning},
}

@article{hardt_identity_2016,
	title = {Identity Matters in Deep Learning},
	url = {http://arxiv.org/abs/1611.04231},
	abstract = {An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as {\textbackslash}emph\{batch normalization\}, but was also key to the immense success of {\textbackslash}emph\{residual networks\}. In this work, we put the principle of {\textbackslash}emph\{identity parameterization\} on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with {ReLu} activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and {ReLu} activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the {CIFAR}10, {CIFAR}100, and {ImageNet} classification benchmarks.},
	journaltitle = {{arXiv}:1611.04231 [cs, stat]},
	author = {Hardt, Moritz and Ma, Tengyu},
	urldate = {2017-03-28},
	date = {2016-11-13},
	eprinttype = {arxiv},
	eprint = {1611.04231},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{salimans_evolution_2017,
	title = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
	url = {http://arxiv.org/abs/1703.03864},
	abstract = {We explore the use of Evolution Strategies, a class of black box optimization algorithms, as an alternative to popular {RL} techniques such as Q-learning and Policy Gradients. Experiments on {MuJoCo} and Atari show that {ES} is a viable solution strategy that scales extremely well with the number of {CPUs} available: By using hundreds to thousands of parallel workers, {ES} can solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training time. In addition, we highlight several advantages of {ES} as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
	journaltitle = {{arXiv}:1703.03864 [cs, stat]},
	author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sutskever, Ilya},
	urldate = {2017-03-24},
	date = {2017-03-10},
	eprinttype = {arxiv},
	eprint = {1703.03864},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@online{noauthor_uncertainty_nodate,
	title = {Uncertainty in Deep Learning ({PhD} Thesis) {\textbar} Yarin Gal - Blog {\textbar} Cambridge Machine Learning Group},
	url = {http://mlg.eng.cam.ac.uk/yarin/blog_2248.html},
	abstract = {So I finally submitted my {PhD} thesis, collecting already published results on how to obtain uncertainty in deep learning, and lots of bits and pieces of new research I had lying around...},
	urldate = {2017-03-21},
}

@article{kaiser_can_2016,
	title = {Can Active Memory Replace Attention?},
	url = {http://arxiv.org/abs/1610.08613},
	abstract = {Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.},
	journaltitle = {{arXiv}:1610.08613 [cs]},
	author = {Kaiser, Łukasz and Bengio, Samy},
	urldate = {2017-03-08},
	date = {2016-10-27},
	eprinttype = {arxiv},
	eprint = {1610.08613},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
}

@article{zoph_neural_2016,
	title = {Neural Architecture Search with Reinforcement Learning},
	url = {http://arxiv.org/abs/1611.01578},
	abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this {RNN} with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the {CIFAR}-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our {CIFAR}-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used {LSTM} cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on {PTB} and achieves a state-of-the-art perplexity of 1.214.},
	journaltitle = {{arXiv}:1611.01578 [cs]},
	author = {Zoph, Barret and Le, Quoc V.},
	urldate = {2017-03-07},
	date = {2016-11-04},
	eprinttype = {arxiv},
	eprint = {1611.01578},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{yao_discovery_2017,
	title = {Discovery of Evolving Semantics through Dynamic Word Embedding Learning},
	url = {http://arxiv.org/abs/1703.00607},
	abstract = {During the course of human language evolution, the semantic meanings of words keep evolving with time. The understanding of evolving semantics enables us to capture the true meaning of the words in different usage contexts, and thus is critical for various applications, such as machine translation. While it is naturally promising to study word semantics in a time-aware manner, traditional methods to learn word vector representation do not adequately capture the change over time. To this end, in this paper, we aim at learning time-aware vector representation of words through dynamic word embedding modeling. Specifically, we first propose a method that captures time-specific semantics and across-time alignment simultaneously in a way that is robust to data sparsity. Then, we solve the resulting optimization problem using a scalable coordinate descent method. Finally, we perform the empirical study on New York Times data to learn the temporal embeddings and develop multiple evaluations that illustrate the semantic evolution of words, discovered from news media. Moreover, our qualitative and quantitative tests indicate that the our method not only reliably captures the semantic evolution over time, but also onsistently outperforms state-of-the-art temporal embedding approaches on both semantic accuracy and alignment quality.},
	journaltitle = {{arXiv}:1703.00607 [cs, stat]},
	author = {Yao, Zijun and Sun, Yifan and Ding, Weicong and Rao, Nikhil and Xiong, Hui},
	urldate = {2017-03-03},
	date = {2017-03-01},
	eprinttype = {arxiv},
	eprint = {1703.00607},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
}

@article{doshi-velez_roadmap_2017,
	title = {A Roadmap for a Rigorous Science of Interpretability},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	journaltitle = {{arXiv}:1702.08608 [cs, stat]},
	author = {Doshi-Velez, Finale and Kim, Been},
	urldate = {2017-03-02},
	date = {2017-02-27},
	eprinttype = {arxiv},
	eprint = {1702.08608},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
}

@article{lebedev_fast_2015,
	title = {Fast {ConvNets} Using Group-wise Brain Damage},
	url = {http://arxiv.org/abs/1506.02515},
	abstract = {We revisit the idea of brain damage, i.e. the pruning of the coefficients of a neural network, and suggest how brain damage can be modified and used to speedup convolutional layers. The approach uses the fact that many efficient implementations reduce generalized convolutions to matrix multiplications. The suggested brain damage process prunes the convolutional kernel tensor in a group-wise fashion by adding group-sparsity regularization to the standard training process. After such group-wise pruning, convolutions can be reduced to multiplications of thinned dense matrices, which leads to speedup. In the comparison on {AlexNet}, the method achieves very competitive performance.},
	journaltitle = {{arXiv}:1506.02515 [cs]},
	author = {Lebedev, Vadim and Lempitsky, Victor},
	urldate = {2017-03-02},
	date = {2015-06-08},
	eprinttype = {arxiv},
	eprint = {1506.02515},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{guo_dynamic_2016,
	title = {Dynamic Network Surgery for Efficient {DNNs}},
	url = {http://arxiv.org/abs/1608.04493},
	abstract = {Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in {LeNet}-5 and {AlexNet} by a factor of \${\textbackslash}bm\{108\}{\textbackslash}times\$ and \${\textbackslash}bm\{17.7\}{\textbackslash}times\$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.},
	journaltitle = {{arXiv}:1608.04493 [cs]},
	author = {Guo, Yiwen and Yao, Anbang and Chen, Yurong},
	urldate = {2017-03-02},
	date = {2016-08-16},
	eprinttype = {arxiv},
	eprint = {1608.04493},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{sharma_online_2017,
	title = {Online Multi-Task Learning Using Active Sampling},
	url = {http://arxiv.org/abs/1702.06053},
	abstract = {One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large task-specific (expert) networks which require extensive training. We propose a simple yet efficient multi-task learning framework which solves multiple goal-directed tasks in an online or active learning setup without the need for expert supervision.},
	journaltitle = {{arXiv}:1702.06053 [cs]},
	author = {Sharma, Sahil and Ravindran, Balaraman},
	urldate = {2017-02-23},
	date = {2017-02-20},
	eprinttype = {arxiv},
	eprint = {1702.06053},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{li_data_2017,
	title = {Data Distillation for Controlling Specificity in Dialogue Generation},
	url = {http://arxiv.org/abs/1702.06703},
	abstract = {People speak at different levels of specificity in different situations. Depending on their knowledge, interlocutors, mood, etc.\} A conversational agent should have this ability and know when to be specific and when to be general. We propose an approach that gives a neural network--based conversational agent this ability. Our approach involves alternating between {\textbackslash}emph\{data distillation\} and model training : removing training examples that are closest to the responses most commonly produced by the model trained from the last round and then retrain the model on the remaining dataset. Dialogue generation models trained with different degrees of data distillation manifest different levels of specificity. We then train a reinforcement learning system for selecting among this pool of generation models, to choose the best level of specificity for a given input. Compared to the original generative model trained without distillation, the proposed system is capable of generating more interesting and higher-quality responses, in addition to appropriately adjusting specificity depending on the context. Our research constitutes a specific case of a broader approach involving training multiple subsystems from a single dataset distinguished by differences in a specific property one wishes to model. We show that from such a set of subsystems, one can use reinforcement learning to build a system that tailors its output to different input contexts at test time.},
	journaltitle = {{arXiv}:1702.06703 [cs]},
	author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
	urldate = {2017-02-23},
	date = {2017-02-22},
	eprinttype = {arxiv},
	eprint = {1702.06703},
	keywords = {Computer Science - Computation and Language},
}

@incollection{zhang_architectural_2016,
	title = {Architectural Complexity Measures of Recurrent Neural Networks},
	url = {http://papers.nips.cc/paper/6303-architectural-complexity-measures-of-recurrent-neural-networks.pdf},
	pages = {1822--1830},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Saizheng and Wu, Yuhuai and Che, Tong and Lin, Zhouhan and Memisevic, Roland and Salakhutdinov, Ruslan R and Bengio, Yoshua},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2016-12-12},
	date = {2016},
}

@article{cho_learning_2014,
	title = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical Machine Translation},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called {RNN} Encoder-Decoder that consists of two recurrent neural networks ({RNN}). One {RNN} encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the {RNN} Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	journaltitle = {{arXiv}:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	urldate = {2016-10-05},
	date = {2014-06-03},
	eprinttype = {arxiv},
	eprint = {1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@online{noauthor_interesting_nodate,
	title = {Interesting papers on learning automatically learning neural network topology • /r/{MachineLearning}},
	url = {https://www.reddit.com/r/MachineLearning/comments/44ld5c/interesting_papers_on_learning_automatically/?st=itwy6bs4&sh=95299f3f},
	abstract = {I'm doing some research on this at the moment and has compiled a bunch of interesting research papers in this area. I wanted to present them here...},
	titleaddon = {reddit},
	urldate = {2016-10-05},
}

@article{wong_sequence_2016,
	title = {Sequence Student-Teacher Training of Deep Neural Networks},
	url = {https://www.repository.cam.ac.uk/handle/1810/256846},
	doi = {10.17863/CAM.779},
	abstract = {The performance of automatic speech recognition can often be significantly improved by combining multiple systems together. Though beneficial, ensemble methods can be computationally expensive, often requiring multiple decoding runs. An alternative approach, appropriate for deep learning schemes, is to adopt student-teacher training. Here, a student model is trained to reproduce the outputs of a teacher model, or ensemble of teachers. The standard approach is to train the student model on the frame posterior outputs of the teacher. This paper examines the interaction between student-teacher training schemes and sequence training criteria, which have been shown to yield significant performance gains over frame-level criteria. There are several possible options for integrating sequence training, including training of the ensemble and further training of the student. This paper also proposes an extension to the student-teacher framework, where the student is trained to emulate the hypothesis posterior distribution of the teacher, or ensemble of teachers. This sequence student-teacher training approach allows the benefit of student-teacher training to be directly combined with sequence training schemes. These approaches are evaluated on two speech recognition tasks: a Wall Street Journal based task and a low-resource Tok Pisin conversational telephone speech task from the {IARPA} Babel programme.},
	author = {Wong, Jeremy H. M. and Gales, Mark J. F.},
	urldate = {2017-02-13},
	date = {2016-09-13},
	langid = {english},
}

@article{freitag_ensemble_2017,
	title = {Ensemble Distillation for Neural Machine Translation},
	url = {http://arxiv.org/abs/1702.01802},
	abstract = {Knowledge distillation describes a method for training a student network to perform better by learning from a stronger teacher network. In this work, we run experiments with different kinds of teacher net- works to enhance the translation performance of a student Neural Machine Translation ({NMT}) network. We demonstrate techniques based on an ensemble and a best {BLEU} teacher network. We also show how to benefit from a teacher network that has the same architecture and dimensions of the student network. Further- more, we introduce a data filtering technique based on the dissimilarity between the forward translation (obtained during knowledge distillation) of a given source sentence and its target reference. We use {TER} to measure dissimilarity. Finally, we show that an ensemble teacher model can significantly reduce the student model size while still getting performance improvements compared to the baseline student network.},
	journaltitle = {{arXiv}:1702.01802 [cs]},
	author = {Freitag, Markus and Al-Onaizan, Yaser and Sankaran, Baskaran},
	urldate = {2017-02-09},
	date = {2017-02-06},
	eprinttype = {arxiv},
	eprint = {1702.01802},
	keywords = {Computer Science - Computation and Language},
}

@article{boleda_living_2017,
	title = {Living a discrete life in a continuous world: Reference with distributed representations},
	url = {http://arxiv.org/abs/1702.01815},
	shorttitle = {Living a discrete life in a continuous world},
	abstract = {Reference is the crucial property of language that allows us to connect linguistic expressions to the world. Modeling it requires handling both continuous and discrete aspects of meaning. Data-driven models excel at the former, but struggle with the latter, and the reverse is true for symbolic models. We propose a fully data-driven, end-to-end trainable model that, while operating on continuous multimodal representations, learns to organize them into a discrete-like entity library. We also introduce a referential task to test it, cross-modal tracking. Our model beats standard neural network architectures, but is outperformed by some parametrizations of Memory Networks, another model with external memory.},
	journaltitle = {{arXiv}:1702.01815 [cs]},
	author = {Boleda, Gemma and Padó, Sebastian and Pham, Nghia The and Baroni, Marco},
	urldate = {2017-02-09},
	date = {2017-02-06},
	eprinttype = {arxiv},
	eprint = {1702.01815},
	keywords = {Computer Science - Computation and Language},
}

@article{looks_deep_2017,
	title = {Deep Learning with Dynamic Computation Graphs},
	url = {http://arxiv.org/abs/1702.02181},
	abstract = {Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.},
	journaltitle = {{arXiv}:1702.02181 [cs, stat]},
	author = {Looks, Moshe and Herreshoff, Marcello and Hutchins, {DeLesley} and Norvig, Peter},
	urldate = {2017-02-09},
	date = {2017-02-07},
	eprinttype = {arxiv},
	eprint = {1702.02181},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{baroni_commai:_2017,
	title = {{CommAI}: Evaluating the first steps towards a useful general {AI}},
	url = {http://arxiv.org/abs/1701.08954},
	shorttitle = {{CommAI}},
	abstract = {With machine learning successfully applied to new daunting problems almost every day, general {AI} starts looking like an attainable goal. However, most current research focuses instead on important but narrow applications, such as image classification or machine translation. We believe this to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose here a set of concrete desiderata for general {AI}, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.},
	journaltitle = {{arXiv}:1701.08954 [cs]},
	author = {Baroni, Marco and Joulin, Armand and Jabri, Allan and Kruszewski, Germàn and Lazaridou, Angeliki and Simonic, Klemen and Mikolov, Tomas},
	urldate = {2017-02-07},
	date = {2017-01-31},
	eprinttype = {arxiv},
	eprint = {1701.08954},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning},
}

@article{fernando_convolution_2016,
	title = {Convolution by Evolution: Differentiable Pattern Producing Networks},
	url = {http://arxiv.org/abs/1606.02580},
	shorttitle = {Convolution by Evolution},
	abstract = {In this work we introduce a differentiable version of the Compositional Pattern Producing Network, called the {DPPN}. Unlike a standard {CPPN}, the topology of a {DPPN} is evolved but the weights are learned. A Lamarckian algorithm, that combines evolution and learning, produces {DPPNs} to reconstruct an image. Our main result is that {DPPNs} can be evolved/trained to compress the weights of a denoising autoencoder from 157684 to roughly 200 parameters, while achieving a reconstruction accuracy comparable to a fully connected network with more than two orders of magnitude more parameters. The regularization ability of the {DPPN} allows it to rediscover (approximate) convolutional network architectures embedded within a fully connected architecture. Such convolutional architectures are the current state of the art for many computer vision applications, so it is satisfying that {DPPNs} are capable of discovering this structure rather than having to build it in by design. {DPPNs} exhibit better generalization when tested on the Omniglot dataset after being trained on {MNIST}, than directly encoded fully connected autoencoders. {DPPNs} are therefore a new framework for integrating learning and evolution.},
	journaltitle = {{arXiv}:1606.02580 [cs]},
	author = {Fernando, Chrisantha and Banarse, Dylan and Reynolds, Malcolm and Besse, Frederic and Pfau, David and Jaderberg, Max and Lanctot, Marc and Wierstra, Daan},
	urldate = {2017-01-16},
	date = {2016-06-08},
	eprinttype = {arxiv},
	eprint = {1606.02580},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{cortes_adanet:_2016,
	title = {{AdaNet}: Adaptive Structural Learning of Artificial Neural Networks},
	url = {http://arxiv.org/abs/1607.01097},
	shorttitle = {{AdaNet}},
	abstract = {We present a new theoretical framework for analyzing and learning artificial neural networks. Our approach simultaneously and adaptively learns both the structure of the network as well as its weights. The methodology is based upon and accompanied by strong data-dependent theoretical learning guarantees. We present some preliminary results to show that the final network architecture adapts to the complexity of a given problem.},
	journaltitle = {{arXiv}:1607.01097 [cs]},
	author = {Cortes, Corinna and Gonzalvo, Xavi and Kuznetsov, Vitaly and Mohri, Mehryar and Yang, Scott},
	urldate = {2017-01-09},
	date = {2016-07-04},
	eprinttype = {arxiv},
	eprint = {1607.01097},
	keywords = {Computer Science - Learning},
}

@article{liu_dynamic_2017,
	title = {Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution},
	url = {http://arxiv.org/abs/1701.00299},
	shorttitle = {Dynamic Deep Neural Networks},
	abstract = {We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward deep neural network that allow selective execution. Given an input, only a subset of D2NN neurons are executed, and the particular subset is determined by the D2NN itself. By pruning unnecessary computation depending on input, D2NNs provide a way to improve computational efficiency. To achieve dynamic selective execution, a D2NN augments a regular feed-forward deep neural network (directed acyclic graph of differentiable modules) with one or more controller modules. Each controller module is a sub-network whose output is a decision that controls whether other modules can execute. A D2NN is trained end to end. Both regular modules and controller modules in a D2NN are learnable and are jointly trained to optimize both accuracy and efficiency. Such training is achieved by integrating backpropagation with reinforcement learning. With extensive experiments of various D2NN architectures on image classification tasks, we demonstrate that D2NNs are general and flexible, and can effectively optimize accuracy-efficiency trade-offs.},
	journaltitle = {{arXiv}:1701.00299 [cs, stat]},
	author = {Liu, Lanlan and Deng, Jia},
	urldate = {2017-01-07},
	date = {2017-01-01},
	eprinttype = {arxiv},
	eprint = {1701.00299},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{goldberg_primer_2015,
	title = {A Primer on Neural Network Models for Natural Language Processing},
	url = {http://arxiv.org/abs/1510.00726},
	abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
	journaltitle = {{arXiv}:1510.00726 [cs]},
	author = {Goldberg, Yoav},
	urldate = {2016-12-20},
	date = {2015-10-02},
	eprinttype = {arxiv},
	eprint = {1510.00726},
	keywords = {Computer Science - Computation and Language},
}
@incollection{long_unsupervised_2016,
	title = {Unsupervised Domain Adaptation with Residual Transfer Networks},
	url = {http://papers.nips.cc/paper/6110-unsupervised-domain-adaptation-with-residual-transfer-networks.pdf},
	pages = {136--144},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2016-12-13},
	date = {2016},
}

@incollection{wang_natural-parameter_2016,
	title = {Natural-Parameter Networks: A Class of Probabilistic Neural Networks},
	url = {http://papers.nips.cc/paper/6279-natural-parameter-networks-a-class-of-probabilistic-neural-networks.pdf},
	shorttitle = {Natural-Parameter Networks},
	pages = {118--126},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Hao and {SHI}, Xingjian and Yeung, Dit-Yan},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2016-12-13},
	date = {2016},
}

@incollection{singh_swapout:_2016,
	title = {Swapout: Learning an ensemble of deep architectures},
	url = {http://papers.nips.cc/paper/6205-swapout-learning-an-ensemble-of-deep-architectures.pdf},
	shorttitle = {Swapout},
	pages = {28--36},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Singh, Saurabh and Hoiem, Derek and Forsyth, David},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2016-12-13},
	date = {2016},
}

@incollection{rudolph_exponential_2016,
	title = {Exponential Family Embeddings},
	url = {http://papers.nips.cc/paper/6571-exponential-family-embeddings.pdf},
	pages = {478--486},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Rudolph, Maja and Ruiz, Francisco and Mandt, Stephan and Blei, David},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2016-12-12},
	date = {2016},
}

@incollection{daniely_toward_2016,
	title = {Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity},
	url = {http://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity.pdf},
	shorttitle = {Toward Deeper Understanding of Neural Networks},
	pages = {2253--2261},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Daniely, Amit and Frostig, Roy and Singer, Yoram},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2016-12-12},
	date = {2016},
}

@incollection{alvarez_learning_2016,
	title = {Learning the Number of Neurons in Deep Networks},
	url = {http://papers.nips.cc/paper/6372-learning-the-number-of-neurons-in-deep-networks.pdf},
	pages = {2262--2270},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Alvarez, Jose M and Salzmann, Mathieu},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2016-12-12},
	date = {2016},
}

@incollection{wen_learning_2016,
	title = {Learning Structured Sparsity in Deep Neural Networks},
	url = {http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf},
	pages = {2074--2082},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2016-12-12},
	date = {2016},
}

@incollection{weston_dialog-based_2016,
	title = {Dialog-based Language Learning},
	url = {http://papers.nips.cc/paper/6264-dialog-based-language-learning.pdf},
	pages = {829--837},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Weston, Jason E},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2016-12-12},
	date = {2016},
}

@incollection{gregor_towards_2016,
	title = {Towards Conceptual Compression},
	url = {http://papers.nips.cc/paper/6542-towards-conceptual-compression.pdf},
	pages = {3549--3557},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Gregor, Karol and Besse, Frederic and Jimenez Rezende, Danilo and Danihelka, Ivo and Wierstra, Daan},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2016-12-12},
	date = {2016},
}

@article{see_compression_2016,
	title = {Compression of Neural Machine Translation Models via Pruning},
	url = {https://arxiv.org/abs/1606.09274},
	journaltitle = {{arXiv} preprint {arXiv}:1606.09274},
	author = {See, Abigail and Luong, Minh-Thang and Manning, Christopher D.},
	urldate = {2016-12-12},
	date = {2016},
}

@article{fahlman_cascade-correlation_1989,
	title = {The cascade-correlation learning architecture},
	url = {http://repository.cmu.edu/compsci/1938/},
	author = {Fahlman, Scott E. and Lebiere, Christian},
	urldate = {2016-11-30},
	date = {1989},
}

@article{cirik_visualizing_2016,
	title = {Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks},
	url = {http://arxiv.org/abs/1611.06204},
	abstract = {Curriculum Learning emphasizes the order of training instances in a computational learning setup. The core hypothesis is that simpler instances should be learned early as building blocks to learn more complex ones. Despite its usefulness, it is still unknown how exactly the internal representation of models are affected by curriculum learning. In this paper, we study the effect of curriculum learning on Long Short-Term Memory ({LSTM}) networks, which have shown strong competency in many Natural Language Processing ({NLP}) problems. Our experiments on sentiment analysis task and a synthetic task similar to sequence prediction tasks in {NLP} show that curriculum learning has a positive effect on the {LSTM}'s internal states by biasing the model towards building constructive representations i.e. the internal representation at the previous timesteps are used as building blocks for the final prediction. We also find that smaller models significantly improves when they are trained with curriculum learning. Lastly, we show that curriculum learning helps more when the amount of training data is limited.},
	journaltitle = {{arXiv}:1611.06204 [cs]},
	author = {Cirik, Volkan and Hovy, Eduard and Morency, Louis-Philippe},
	urldate = {2016-11-24},
	date = {2016-11-18},
	eprinttype = {arxiv},
	eprint = {1611.06204},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{jiaqi_mu_suma_bhat_pramod_viswanath_geometry_nodate,
	title = {{GEOMETRY} {OF} {POLYSEMY}},
	author = {{Jiaqi Mu, Suma Bhat, Pramod Viswanath}},
}

@inproceedings{aryk_anderson_beyond_nodate,
	title = {{BEYOND} {FINE} {TUNING}: A {MODULAR} {APPROACH} {TO} {LEARNING} {ON} {SMALL} {DATA}},
	author = {{Aryk Anderson} and {Kyle Shaffer} and {Artem Yankov} and {, Courtney D. Corley†\& Nathan O. Hodas}},
}

@inproceedings{tianxiang_gao_sample_nodate,
	title = {Sample Importance in Training Deep Neural Networks},
	author = {{Tianxiang Gao} and {Vladimir Jojic}},
}

@inproceedings{karen_ullrich_soft_nodate,
	title = {Soft Weight-Sharing for Neural Network Compression},
	author = {{Karen Ullrich} and {Edward Meeds} and {Max Welling}},
}

@article{gauthier_paradigm_2016,
	title = {A Paradigm for Situated and Goal-Driven Language Learning},
	url = {http://arxiv.org/abs/1610.03585},
	abstract = {A distinguishing property of human intelligence is the ability to flexibly use language in order to communicate complex ideas with other humans in a variety of contexts. Research in natural language dialogue should focus on designing communicative agents which can integrate themselves into these contexts and productively collaborate with humans. In this abstract, we propose a general situated language learning paradigm which is designed to bring about robust language agents able to cooperate productively with humans.},
	journaltitle = {{arXiv}:1610.03585 [cs]},
	author = {Gauthier, Jon and Mordatch, Igor},
	urldate = {2016-11-06},
	date = {2016-10-11},
	eprinttype = {arxiv},
	eprint = {1610.03585},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{naomi_saphra_domain_2015,
	title = {Domain Adaptation for Word Embeddings With Matrix Factorization},
	rights = {All rights reserved},
	url = {https://www.overleaf.com/1624982skhqpv},
	eventtitle = {Women in Machine Learning Workshop at {NIPS}},
	booktitle = {Women in Machine Learning Workshop at {NIPS}},
	author = {{Naomi Saphra} and {Raman Arora}},
	urldate = {2015-01-23},
	date = {2015},
}

@inproceedings{saphra_evaluating_2016,
	title = {Evaluating Informal-Domain Word Representations With {UrbanDictionary}},
	rights = {All rights reserved},
	url = {https://arxiv.org/abs/1606.08270},
	eventtitle = {The First Workshop on Evaluating Vector Space Representations for {NLP}},
	booktitle = {The First Workshop on Evaluating Vector Space Representations for {NLP}},
	author = {Saphra, Naomi and Lopez, Adam},
	urldate = {2016-10-28},
	date = {2016},
}

@inproceedings{naomi_saphra_automatic_2012,
	title = {Automatic verification of boundary annotations from mechanical turk},
	rights = {All rights reserved},
	url = {https://www.overleaf.com/1624982skhqpv},
	eventtitle = {Women in Machine Learning Workshop at {NIPS}},
	booktitle = {Women in Machine Learning Workshop at {NIPS}},
	author = {{Naomi Saphra}},
	urldate = {2015-01-23},
	date = {2012},
}

@report{blaschko_towards_2012,
	title = {Towards a detailed understanding of objects and scenes in natural images},
	rights = {All rights reserved},
	url = {http://hal.archives-ouvertes.fr/hal-00776027/},
	author = {Blaschko, Matthew and Girshick, Ross B. and Kannala, Juho and Kokkinos, Iasonas and Mahendran, Siddarth and Maji, Subhransu and Mohamed, Sammy and Rahtu, Esa and Saphra, Naomi and Simonyan, Karen},
	urldate = {2014-11-01},
	date = {2012},
}

@inproceedings{cotterell_algerian_2014,
	title = {An Algerian Arabic-French Code-Switched Corpus},
	rights = {All rights reserved},
	url = {http://cis.upenn.edu/~ccb/publications/arabic-french-codeswitching.pdf},
	eventtitle = {{LREC} Workshop on Free/Open-Source Arabic Corpora and Corpora Processing Tools},
	author = {Cotterell, Ryan and Renduchintala, Adithya and Saphra, Naomi and Callison-Burch, Chris},
	urldate = {2014-04-30},
	date = {2014},
}

@article{bengio_practical_2012,
	title = {Practical recommendations for gradient-based training of deep architectures},
	url = {http://arxiv.org/abs/1206.5533},
	abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
	journaltitle = {{arXiv}:1206.5533 [cs]},
	author = {Bengio, Yoshua},
	urldate = {2016-10-28},
	date = {2012-06-24},
	eprinttype = {arxiv},
	eprint = {1206.5533},
	keywords = {Computer Science - Learning},
}

@article{huang_deep_2016,
	title = {Deep Networks with Stochastic Depth},
	url = {http://arxiv.org/abs/1603.09382},
	abstract = {Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91\% on {CIFAR}-10).},
	journaltitle = {{arXiv}:1603.09382 [cs]},
	author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian},
	urldate = {2016-10-14},
	date = {2016-03-30},
	eprinttype = {arxiv},
	eprint = {1603.09382},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{mobahi_training_2016,
	title = {Training Recurrent Neural Networks by Diffusion},
	url = {http://arxiv.org/abs/1601.04114},
	abstract = {This work presents a new algorithm for training recurrent neural networks (although ideas are applicable to feedforward networks as well). The algorithm is derived from a theory in nonconvex optimization related to the diffusion equation. The contributions made in this work are two fold. First, we show how some seemingly disconnected mechanisms used in deep learning such as smart initialization, annealed learning rate, layerwise pretraining, and noise injection (as done in dropout and {SGD}) arise naturally and automatically from this framework, without manually crafting them into the algorithms. Second, we present some preliminary results on comparing the proposed method against {SGD}. It turns out that the new algorithm can achieve similar level of generalization accuracy of {SGD} in much fewer number of epochs.},
	journaltitle = {{arXiv}:1601.04114 [cs]},
	author = {Mobahi, Hossein},
	urldate = {2016-10-14},
	date = {2016-01-15},
	eprinttype = {arxiv},
	eprint = {1601.04114},
	keywords = {Computer Science - Learning},
}

@article{fan_self-paced_2016,
	title = {Self-Paced Learning: an Implicit Regularization Perspective},
	url = {http://arxiv.org/abs/1606.00128},
	shorttitle = {Self-Paced Learning},
	abstract = {Self-paced learning ({SPL}) mimics the cognitive mechanism of humans and animals that gradually learns from easy to hard samples. One key issue in {SPL} is to obtain better weighting strategy that is determined by minimizer function. Existing methods usually pursue this by artificially designing the explicit form of {SPL} regularizer. In this paper, we focus on the minimizer function, and study a group of new regularizer, named self-paced implicit regularizer that is deduced from robust loss function. Based on the convex conjugacy theory, the minimizer function for self-paced implicit regularizer can be directly learned from the latent loss function, while the analytic form of the regularizer can be even known. A general framework (named {SPL}-{IR}) for {SPL} is developed accordingly. We demonstrate that the learning procedure of {SPL}-{IR} is associated with latent robust loss functions, thus can provide some theoretical inspirations for its working mechanism. We further analyze the relation between {SPL}-{IR} and half-quadratic optimization. Finally, we implement {SPL}-{IR} to both supervised and unsupervised tasks, and experimental results corroborate our ideas and demonstrate the correctness and effectiveness of implicit regularizers.},
	journaltitle = {{arXiv}:1606.00128 [cs]},
	author = {Fan, Yanbo and He, Ran and Liang, Jian and Hu, Bao-Gang},
	urldate = {2016-10-09},
	date = {2016-06-01},
	eprinttype = {arxiv},
	eprint = {1606.00128},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
}

@article{gulcehre_mollifying_2016,
	title = {Mollifying Networks},
	url = {http://arxiv.org/abs/1608.04980},
	abstract = {The optimization of deep neural networks can be more challenging than traditional convex optimization problems due to the highly non-convex nature of the loss function, e.g. it can involve pathological landscapes such as saddle-surfaces that can be difficult to escape for algorithms based on simple gradient descent. In this paper, we attack the problem of optimization of highly non-convex neural networks by starting with a smoothed -- or {\textbackslash}textit\{mollified\} -- objective function that gradually has a more non-convex energy landscape during the training. Our proposition is inspired by the recent studies in continuation methods: similar to curriculum methods, we begin learning an easier (possibly convex) objective function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, objective function. The complexity of the mollified networks is controlled by a single hyperparameter which is annealed during the training. We show improvements on various difficult optimization tasks and establish a relationship with recent works on continuation methods for neural networks and mollifiers.},
	journaltitle = {{arXiv}:1608.04980 [cs]},
	author = {Gulcehre, Caglar and Moczulski, Marcin and Visin, Francesco and Bengio, Yoshua},
	urldate = {2016-10-07},
	date = {2016-08-17},
	eprinttype = {arxiv},
	eprint = {1608.04980},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{omlin_pruning_1994,
	title = {Pruning recurrent neural networks for improved generalization performance},
	doi = {10.1109/NNSP.1994.365996},
	abstract = {The experimental results in this paper demonstrate that a simple pruning/retraining method effectively improves the generalization performance of recurrent neural networks trained to recognize regular languages. The technique also permits the extraction of symbolic knowledge in the form of deterministic finite-state automata ({DFA}) which are more consistent with the rules to be learned. Weight decay has also been shown to improve a network's generalization performance. Simulations with two small {DFA} (⩽10 states) and a large finite-memory machine (64 states) demonstrate that the performance improvement due to pruning/retraining is generally superior to the improvement due to training with weight decay. In addition, there is no need to guess a `good' decay rate},
	eventtitle = {Neural Networks for Signal Processing [1994] {IV}. Proceedings of the 1994 {IEEE} Workshop},
	pages = {690--699},
	booktitle = {Neural Networks for Signal Processing [1994] {IV}. Proceedings of the 1994 {IEEE} Workshop},
	author = {Omlin, C. W. and Giles, C. L.},
	date = {1994-09},
	keywords = {Computer science, Doped fiber amplifiers, Educational institutions, Electronic mail, Learning automata, National electric code, Neurons, decay rate, deterministic finite-state automata, finite automata, finite-memory machine, generalisation (artificial intelligence), generalization performance, knowledge acquisition, learning (artificial intelligence), network pruning, recurrent neural nets, recurrent neural networks, symbolic knowledge extraction, weight decay},
}

@article{srivastava_training_2015,
	title = {Training Very Deep Networks},
	url = {http://arxiv.org/abs/1507.06228},
	abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
	journaltitle = {{arXiv}:1507.06228 [cs]},
	author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
	urldate = {2016-10-06},
	date = {2015-07-22},
	eprinttype = {arxiv},
	eprint = {1507.06228},
	keywords = {68T01, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, G.1.6, I.2.6},
}

@article{maclaurin_gradient-based_2015,
	title = {Gradient-based Hyperparameter Optimization through Reversible Learning},
	url = {http://arxiv.org/abs/1502.03492},
	abstract = {Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.},
	journaltitle = {{arXiv}:1502.03492 [cs, stat]},
	author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P.},
	urldate = {2016-10-05},
	date = {2015-02-11},
	eprinttype = {arxiv},
	eprint = {1502.03492},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{srinivas_data-free_2015,
	title = {Data-free parameter pruning for deep neural networks},
	url = {http://arxiv.org/abs/1507.06149},
	journaltitle = {{arXiv} preprint {arXiv}:1507.06149},
	author = {Srinivas, Suraj and Babu, R. Venkatesh},
	urldate = {2016-10-05},
	date = {2015},
}

@incollection{desell_evolving_2015,
	title = {Evolving Deep Recurrent Neural Networks Using Ant Colony Optimization},
	rights = {©2015 Springer International Publishing Switzerland},
	isbn = {9783319164670 9783319164687},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-16468-7_8},
	series = {Lecture Notes in Computer Science},
	abstract = {This paper presents a novel strategy for using ant colony optimization ({ACO}) to evolve the structure of deep recurrent neural networks. While versions of {ACO} for continuous parameter optimization have been previously used to train the weights of neural networks, to the authors’ knowledge they have not been used to actually design neural networks. The strategy presented is used to evolve deep neural networks with up to 5 hidden and 5 recurrent layers for the challenging task of predicting general aviation flight data, and is shown to provide improvements of 63 \% for airspeed, a 97 \% for altitude and 120 \% for pitch over previously best published results, while at the same time not requiring additional input neurons for residual values. The strategy presented also has many benefits for neuro evolution, including the fact that it is easily parallizable and scalable, and can operate using any method for training neural networks. Further, the networks it evolves can typically be trained in fewer iterations than fully connected networks.},
	pages = {86--98},
	number = {9026},
	booktitle = {Evolutionary Computation in Combinatorial Optimization},
	publisher = {Springer International Publishing},
	author = {Desell, Travis and Clachar, Sophine and Higgins, James and Wild, Brandon},
	editor = {Ochoa, Gabriela and Chicano, Francisco},
	urldate = {2016-10-05},
	date = {2015-04-08},
	langid = {english},
	doi = {10.1007/978-3-319-16468-7_8},
	keywords = {Algorithm Analysis and Problem Complexity, Ant colony optimization, Artificial Intelligence (incl. Robotics), Aviation informatics, Computation by Abstract Devices, Discrete Mathematics in Computer Science, Flight prediction, Neural networks, Numeric Computing, Time-series prediction},
}

@article{godfrey_continuum_2016,
	title = {A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks},
	url = {http://arxiv.org/abs/1602.01321},
	abstract = {We present the soft exponential activation function for artificial neural networks that continuously interpolates between logarithmic, linear, and exponential functions. This activation function is simple, differentiable, and parameterized so that it can be trained as the rest of the network is trained. We hypothesize that soft exponential has the potential to improve neural network learning, as it can exactly calculate many natural operations that typical neural networks can only approximate, including addition, multiplication, inner product, distance, polynomials, and sinusoids.},
	journaltitle = {{arXiv}:1602.01321 [cs]},
	author = {Godfrey, Luke B. and Gashler, Michael S.},
	urldate = {2016-10-05},
	date = {2016-02-03},
	eprinttype = {arxiv},
	eprint = {1602.01321},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{bousmalis_domain_2016,
	title = {Domain Separation Networks},
	url = {http://arxiv.org/abs/1608.06019},
	journaltitle = {{arXiv} preprint {arXiv}:1608.06019},
	author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
	urldate = {2016-10-05},
	date = {2016},
}

@inproceedings{jiang_self-paced_2014,
	title = {Self-paced learning with diversity},
	url = {http://papers.nips.cc/paper/5568-self-paced-learning-with-diversity},
	pages = {2078--2086},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Jiang, Lu and Meng, Deyu and Yu, Shoou-I. and Lan, Zhenzhong and Shan, Shiguang and Hauptmann, Alexander},
	urldate = {2016-09-28},
	date = {2014},
}
@inproceedings{mikolov_strategies_2011,
	title = {Strategies for training large scale neural network language models},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6163930},
	pages = {196--201},
	booktitle = {Automatic Speech Recognition and Understanding ({ASRU}), 2011 {IEEE} Workshop on},
	publisher = {{IEEE}},
	author = {Mikolov, Tomáš and Deoras, Anoop and Povey, Daniel and Burget, Lukáš and Černockỳ, Jan},
	urldate = {2016-09-28},
	date = {2011},
}

@inproceedings{mikolov_recurrent_2010,
	title = {Recurrent neural network based language model.},
	volume = {2},
	url = {http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf},
	pages = {3},
	booktitle = {Interspeech},
	author = {Mikolov, Tomas and Karafiát, Martin and Burget, Lukas and Cernockỳ, Jan and Khudanpur, Sanjeev},
	urldate = {2016-09-28},
	date = {2010},
}

@article{chung_hierarchical_2016,
	title = {Hierarchical Multiscale Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1609.01704},
	abstract = {Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.},
	journaltitle = {{arXiv}:1609.01704 [cs]},
	author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
	urldate = {2016-09-28},
	date = {2016-09-06},
	eprinttype = {arxiv},
	eprint = {1609.01704},
	keywords = {Computer Science - Learning},
}

@article{hinton_distilling_2015,
	title = {Distilling the Knowledge in a Neural Network},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on {MNIST} and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	journaltitle = {{arXiv}:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	urldate = {2016-09-22},
	date = {2015-03-09},
	eprinttype = {arxiv},
	eprint = {1503.02531},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{rusu_progressive_2016,
	title = {Progressive Neural Networks},
	url = {http://arxiv.org/abs/1606.04671},
	abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
	journaltitle = {{arXiv}:1606.04671 [cs]},
	author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
	urldate = {2016-09-14},
	date = {2016-06-15},
	eprinttype = {arxiv},
	eprint = {1606.04671},
	keywords = {Computer Science - Learning},
}

@article{dhingra_end--end_2016,
	title = {End-to-End Reinforcement Learning of Dialogue Agents for Information Access},
	url = {http://arxiv.org/abs/1609.00777},
	abstract = {This paper proposes {\textbackslash}emph\{{KB}-{InfoBot}\}---a dialogue agent that provides users with an entity from a knowledge base ({KB}) by interactively asking for its attributes. All components of the {KB}-{InfoBot} are trained in an end-to-end fashion using reinforcement learning. Goal-oriented dialogue systems typically need to interact with an external database to access real-world knowledge (e.g. movies playing in a city). Previous systems achieved this by issuing a symbolic query to the database and adding retrieved results to the dialogue state. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced "soft" posterior distribution over the {KB} that indicates which entities the user is interested in. We also provide a modified version of the episodic {REINFORCE} algorithm, which allows the {KB}-{InfoBot} to explore and learn both the policy for selecting dialogue acts and the posterior over the {KB} for retrieving the correct entities. Experimental results show that the end-to-end trained {KB}-{InfoBot} outperforms competitive rule-based baselines, as well as agents which are not end-to-end trainable.},
	journaltitle = {{arXiv}:1609.00777 [cs]},
	author = {Dhingra, Bhuwan and Li, Lihong and Li, Xiujun and Gao, Jianfeng and Chen, Yun-Nung and Ahmed, Faisal and Deng, Li},
	urldate = {2016-09-06},
	date = {2016-09-02},
	eprinttype = {arxiv},
	eprint = {1609.00777},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
}

@article{kawaguchi_deep_2016,
	title = {Deep Learning without Poor Local Minima},
	url = {http://arxiv.org/abs/1605.07110},
	abstract = {In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory ({COLT}) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice.},
	journaltitle = {{arXiv}:1605.07110 [cs, math, stat]},
	author = {Kawaguchi, Kenji},
	urldate = {2016-08-17},
	date = {2016-05-23},
	eprinttype = {arxiv},
	eprint = {1605.07110},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{shi_recurrent_2015,
	title = {Recurrent Neural Network Language Model Adaptation with Curriculum Learning},
	url = {https://www.researchgate.net/publication/281643876_Recurrent_Neural_Network_Language_Model_Adaptation_with_Curriculum_Learning},
	abstract = {Official Full-Text Publication: Recurrent Neural Network Language Model Adaptation with Curriculum Learning on {ResearchGate}, the professional network for scientists.},
	journaltitle = {{ResearchGate}},
	author = {Shi, Yangyang and Larson, Martha and Jonker, Catholijn M.},
	urldate = {2016-08-16},
	date = {2015-09-11},
}

@article{ling_latent_2016,
	title = {Latent Predictor Networks for Code Generation},
	url = {http://arxiv.org/abs/1603.06744},
	abstract = {Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.},
	journaltitle = {{arXiv}:1603.06744 [cs]},
	author = {Ling, Wang and Grefenstette, Edward and Hermann, Karl Moritz and Kočiský, Tomáš and Senior, Andrew and Wang, Fumin and Blunsom, Phil},
	urldate = {2016-08-15},
	date = {2016-03-22},
	eprinttype = {arxiv},
	eprint = {1603.06744},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@article{hu_harnessing_2016,
	title = {Harnessing Deep Neural Networks with Logic Rules},
	url = {http://arxiv.org/abs/1603.06318},
	abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., {CNNs} and {RNNs}) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a {CNN} for sentiment analysis, and an {RNN} for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
	journaltitle = {{arXiv}:1603.06318 [cs, stat]},
	author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
	urldate = {2016-08-10},
	date = {2016-03-20},
	eprinttype = {arxiv},
	eprint = {1603.06318},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning},
}

@article{bolukbasi_man_2016,
	title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
	url = {http://arxiv.org/abs/1607.06520},
	shorttitle = {Man is to Computer Programmer as Woman is to Homemaker?},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	journaltitle = {{arXiv}:1607.06520 [cs, stat]},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	urldate = {2016-08-01},
	date = {2016-07-21},
	eprinttype = {arxiv},
	eprint = {1607.06520},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning},
}

@article{jakobsen_mutual_2013,
	title = {Mutual information matrices are not always positive semi-definite},
	url = {http://arxiv.org/abs/1307.6673},
	abstract = {For discrete random variables X\_1,..., X\_n we construct an n by n matrix. In the (i,j) entry we put the mutual information I(X\_i;X\_j) between X\_i and X\_j. In particular, in the (i,i) entry we put the entropy H(X\_i)=I(X\_i;X\_i) of X\_i. This matrix, called the mutual information matrix of (X\_1,...,X\_n), has been conjectured to be positive semi-definite. In this note, we give counterexamples to the conjecture, and show that the conjecture holds for up to three random variables.},
	journaltitle = {{arXiv}:1307.6673 [cs, math]},
	author = {Jakobsen, Sune K.},
	urldate = {2016-07-22},
	date = {2013-07-25},
	eprinttype = {arxiv},
	eprint = {1307.6673},
	keywords = {94A15, Computer Science - Information Theory, H.1.1},
}

@article{smith_comparative_2013,
	title = {A Comparative Evaluation of Curriculum Learning with Filtering and Boosting},
	url = {http://arxiv.org/abs/1312.4986},
	abstract = {Not all instances in a data set are equally beneficial for inferring a model of the data. Some instances (such as outliers) are detrimental to inferring a model of the data. Several machine learning techniques treat instances in a data set differently during training such as curriculum learning, filtering, and boosting. However, an automated method for determining how beneficial an instance is for inferring a model of the data does not exist. In this paper, we present an automated method that orders the instances in a data set by complexity based on the their likelihood of being misclassified (instance hardness). The underlying assumption of this method is that instances with a high likelihood of being misclassified represent more complex concepts in a data set. Ordering the instances in a data set allows a learning algorithm to focus on the most beneficial instances and ignore the detrimental ones. We compare ordering the instances in a data set in curriculum learning, filtering and boosting. We find that ordering the instances significantly increases classification accuracy and that filtering has the largest impact on classification accuracy. On a set of 52 data sets, ordering the instances increases the average accuracy from 81\% to 84\%.},
	journaltitle = {{arXiv}:1312.4986 [cs]},
	author = {Smith, Michael R. and Martinez, Tony},
	urldate = {2016-07-19},
	date = {2013-12-17},
	eprinttype = {arxiv},
	eprint = {1312.4986},
	keywords = {Computer Science - Learning},
}

@article{pentina_curriculum_2014,
	title = {Curriculum Learning of Multiple Tasks},
	url = {http://arxiv.org/abs/1412.1353},
	abstract = {Sharing information between multiple tasks enables algorithms to achieve good generalization performance even from small amounts of training data. However, in a realistic scenario of multi-task learning not all tasks are equally related to each other, hence it could be advantageous to transfer information only between the most related tasks. In this work we propose an approach that processes multiple tasks in a sequence with sharing between subsequent tasks instead of solving all tasks jointly. Subsequently, we address the question of curriculum learning of tasks, i.e. finding the best order of tasks to be learned. Our approach is based on a generalization bound criterion for choosing the task order that optimizes the average expected classification performance over all tasks. Our experimental results show that learning multiple related tasks sequentially can be more effective than learning them jointly, the order in which tasks are being solved affects the overall performance, and that our model is able to automatically discover the favourable order of tasks.},
	journaltitle = {{arXiv}:1412.1353 [cs, stat]},
	author = {Pentina, Anastasia and Sharmanska, Viktoriia and Lampert, Christoph H.},
	urldate = {2016-07-19},
	date = {2014-12-03},
	eprinttype = {arxiv},
	eprint = {1412.1353},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{sangineto_self_2016,
	title = {Self Paced Deep Learning for Weakly Supervised Object Detection},
	url = {http://arxiv.org/abs/1605.07651},
	abstract = {In a weakly-supervised scenario, object detectors need to be trained using image-level annotation only. Since bounding-box-level ground truth is not available, most of the solutions proposed so far are based on an iterative approach in which the classifier, obtained in the previous iteration, is used to predict the objects' positions which are used for training in the current iteration. However, the errors in these predictions can make the process drift. In this paper we propose a self-paced learning protocol to alleviate this problem. The main idea is to iteratively select a subset of samples that are most likely correct, which are used for training. While similar strategies have been recently adopted for {SVMs} and other classifiers, as far as we know, we are the first showing that a self-paced approach can be used with deep-net-based classifiers. We show results on Pascal {VOC} and {ImageNet}, outperforming the previous state of the art on both datasets and specifically obtaining more than 100\% relative improvement on {ImageNet}.},
	journaltitle = {{arXiv}:1605.07651 [cs]},
	author = {Sangineto, Enver and Nabi, Moin and Culibrk, Dubravko and Sebe, Nicu},
	urldate = {2016-07-19},
	date = {2016-05-24},
	eprinttype = {arxiv},
	eprint = {1605.07651},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{scardapane_group_2016,
	title = {Group Sparse Regularization for Deep Neural Networks},
	url = {http://arxiv.org/abs/1607.00485},
	abstract = {In this paper, we consider the joint task of simultaneously optimizing (i) the weights of a deep neural network, (ii) the number of neurons for each hidden layer, and (iii) the subset of active input features (i.e., feature selection). While these problems are generally dealt with separately, we present a simple regularized formulation allowing to solve all three of them in parallel, using standard optimization routines. Specifically, we extend the group Lasso penalty (originated in the linear regression literature) in order to impose group-level sparsity on the network's connections, where each group is defined as the set of outgoing weights from a unit. Depending on the specific case, the weights can be related to an input variable, to a hidden neuron, or to a bias unit, thus performing simultaneously all the aforementioned tasks in order to obtain a compact network. We perform an extensive experimental evaluation, by comparing with classical weight decay and Lasso penalties. We show that a sparse version of the group Lasso penalty is able to achieve competitive performances, while at the same time resulting in extremely compact networks with a smaller number of input features. We evaluate both on a toy dataset for handwritten digit recognition, and on multiple realistic large-scale classification problems.},
	journaltitle = {{arXiv}:1607.00485 [cs, stat]},
	author = {Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
	urldate = {2016-07-06},
	date = {2016-07-02},
	eprinttype = {arxiv},
	eprint = {1607.00485},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{goodman_eu_2016,
	title = {{EU} regulations on algorithmic decision-making and a "right to explanation"},
	url = {http://arxiv.org/abs/1606.08813},
	abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the {EU} in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
	journaltitle = {{arXiv}:1606.08813 [cs, stat]},
	author = {Goodman, Bryce and Flaxman, Seth},
	urldate = {2016-07-04},
	date = {2016-06-28},
	eprinttype = {arxiv},
	eprint = {1606.08813},
	keywords = {Computer Science - Computers and Society, Computer Science - Learning, Statistics - Machine Learning},
}

@article{faruqui_problems_2016,
	title = {Problems With Evaluation of Word Embeddings Using Word Similarity Tasks},
	url = {http://arxiv.org/abs/1605.02276},
	abstract = {Lacking standardized extrinsic evaluation methods for vector representations of words, the {NLP} community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of semantic similarity is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods.},
	journaltitle = {{arXiv}:1605.02276 [cs]},
	author = {Faruqui, Manaal and Tsvetkov, Yulia and Rastogi, Pushpendre and Dyer, Chris},
	urldate = {2016-06-20},
	date = {2016-05-08},
	eprinttype = {arxiv},
	eprint = {1605.02276},
	keywords = {Computer Science - Computation and Language},
}

@article{bottou_optimization_2016,
	title = {Optimization Methods for Large-Scale Machine Learning},
	url = {http://arxiv.org/abs/1606.04838},
	abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient ({SG}) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile {SG} algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
	journaltitle = {{arXiv}:1606.04838 [cs, math, stat]},
	author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	urldate = {2016-06-17},
	date = {2016-06-15},
	eprinttype = {arxiv},
	eprint = {1606.04838},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{akanksha_shallow_2016,
	title = {Shallow Discourse Parsing Using Distributed Argument Representations and Bayesian Optimization},
	url = {http://arxiv.org/abs/1606.04503},
	abstract = {This paper describes the Georgia Tech team's approach to the {CoNLL}-2016 supplementary evaluation on discourse relation sense classification. We use long short-term memories ({LSTM}) to induce distributed representations of each argument, and then combine these representations with surface features in a neural network. The architecture of the neural network is determined by Bayesian hyperparameter search.},
	journaltitle = {{arXiv}:1606.04503 [cs]},
	author = {Akanksha and Eisenstein, Jacob},
	urldate = {2016-06-17},
	date = {2016-06-14},
	eprinttype = {arxiv},
	eprint = {1606.04503},
	keywords = {Computer Science - Computation and Language},
}

@article{marblestone_towards_2016,
	title = {Towards an integration of deep learning and neuroscience},
	url = {http://arxiv.org/abs/1606.03813},
	abstract = {Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) these cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.},
	journaltitle = {{arXiv}:1606.03813 [q-bio]},
	author = {Marblestone, Adam and Wayne, Greg and Kording, Konrad},
	urldate = {2016-06-14},
	date = {2016-06-13},
	eprinttype = {arxiv},
	eprint = {1606.03813},
	keywords = {Quantitative Biology - Neurons and Cognition},
}

@article{warde-farley_self-informed_2014,
	title = {Self-informed neural network structure learning},
	url = {http://arxiv.org/abs/1412.6563},
	journaltitle = {{arXiv} preprint {arXiv}:1412.6563},
	author = {Warde-Farley, David and Rabinovich, Andrew and Anguelov, Dragomir},
	urldate = {2016-06-09},
	date = {2014},
}

@incollection{bengio_deep_2013,
	title = {Deep learning of representations: Looking forward},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-39593-2_1},
	shorttitle = {Deep learning of representations},
	pages = {1--37},
	booktitle = {Statistical language and speech processing},
	publisher = {Springer},
	author = {Bengio, Yoshua},
	urldate = {2016-06-09},
	date = {2013},
}

@book{graves_supervised_2012,
	title = {Supervised sequence labelling},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-24797-2_2},
	publisher = {Springer},
	author = {Graves, Alex},
	urldate = {2016-06-06},
	date = {2012},
}

@article{dyer_recurrent_2016,
	title = {Recurrent Neural Network Grammars},
	url = {http://arxiv.org/abs/1602.07776},
	abstract = {We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential {RNNs} in English and Chinese.},
	journaltitle = {{arXiv}:1602.07776 [cs]},
	author = {Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A.},
	urldate = {2016-05-31},
	date = {2016-02-24},
	eprinttype = {arxiv},
	eprint = {1602.07776},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@article{lapedriza_are_2013,
	title = {Are all training examples equally valuable?},
	url = {http://arxiv.org/abs/1311.6510},
	journaltitle = {{arXiv} preprint {arXiv}:1311.6510},
	author = {Lapedriza, Agata and Pirsiavash, Hamed and Bylinskii, Zoya and Torralba, Antonio},
	urldate = {2016-05-30},
	date = {2013},
}

@article{li_visualizing_2015,
	title = {Visualizing and understanding neural models in {NLP}},
	url = {http://arxiv.org/abs/1506.01066},
	journaltitle = {{arXiv} preprint {arXiv}:1506.01066},
	author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
	urldate = {2016-05-26},
	date = {2015},
}

@inproceedings{bergstra_algorithms_2011,
	title = {Algorithms for hyper-parameter optimization},
	url = {http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization},
	pages = {2546--2554},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Bergstra, James S. and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
	urldate = {2016-05-26},
	date = {2011},
}

@article{srinivas_learning_2015,
	title = {Learning the Architecture of Deep Neural Networks},
	url = {http://arxiv.org/abs/1511.05497},
	abstract = {Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state {ReLU}, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.},
	journaltitle = {{arXiv}:1511.05497 [cs]},
	author = {Srinivas, Suraj and Babu, R. Venkatesh},
	urldate = {2016-05-26},
	date = {2015-11-17},
	eprinttype = {arxiv},
	eprint = {1511.05497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{han_learning_2015,
	title = {Learning both Weights and Connections for Efficient Neural Networks},
	url = {http://arxiv.org/abs/1506.02626},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the {ImageNet} dataset, our method reduced the number of parameters of {AlexNet} by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with {VGG}-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
	journaltitle = {{arXiv}:1506.02626 [cs]},
	author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
	urldate = {2016-05-26},
	date = {2015-06-08},
	eprinttype = {arxiv},
	eprint = {1506.02626},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{jozefowicz_empirical_2015,
	title = {An empirical exploration of recurrent network architectures},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/icml2015_jozefowicz15.pdf},
	pages = {2342--2350},
	booktitle = {Proceedings of the 32nd International Conference on Machine Learning ({ICML}-15)},
	author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
	urldate = {2016-05-26},
	date = {2015},
}

@article{andreas_learning_2016,
	title = {Learning to Compose Neural Networks for Question Answering},
	url = {http://arxiv.org/abs/1601.01705},
	journaltitle = {{arXiv} preprint {arXiv}:1601.01705},
	author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
	urldate = {2016-05-26},
	date = {2016},
}

@incollection{snoek_practical_2012,
	title = {Practical Bayesian Optimization of Machine Learning Algorithms},
	url = {http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf},
	pages = {2951--2959},
	booktitle = {Advances in Neural Information Processing Systems 25},
	publisher = {Curran Associates, Inc.},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	urldate = {2016-05-26},
	date = {2012},
}

@inproceedings{kumar_self-paced_2010,
	title = {Self-paced learning for latent variable models},
	url = {http://papers.nips.cc/paper/3923-self-paced-learning-for-latent-variable-models},
	pages = {1189--1197},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Kumar, M. Pawan and Packer, Benjamin and Koller, Daphne},
	urldate = {2016-05-26},
	date = {2010},
}

@inproceedings{spitkovsky_baby_2010,
	title = {From baby steps to leapfrog: How less is more in unsupervised dependency parsing},
	url = {http://dl.acm.org/citation.cfm?id=1858115},
	shorttitle = {From baby steps to leapfrog},
	pages = {751--759},
	booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Spitkovsky, Valentin I. and Alshawi, Hiyan and Jurafsky, Daniel},
	urldate = {2016-05-26},
	date = {2010},
}
@inproceedings{bengio_curriculum_2009,
	title = {Curriculum learning},
	url = {http://dl.acm.org/citation.cfm?id=1553380},
	pages = {41--48},
	booktitle = {Proceedings of the 26th annual international conference on machine learning},
	publisher = {{ACM}},
	author = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
	urldate = {2016-05-26},
	date = {2009},
}

@article{hashimoto_adaptive_2016,
	title = {Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings},
	url = {http://arxiv.org/abs/1603.06067},
	journaltitle = {{arXiv} preprint {arXiv}:1603.06067},
	author = {Hashimoto, Kazuma and Tsuruoka, Yoshimasa},
	urldate = {2016-05-25},
	date = {2016},
}

@article{lake_human-level_2015,
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	url = {http://science.sciencemag.org/content/350/6266/1332.short},
	pages = {1332--1338},
	number = {6266},
	journaltitle = {Science},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	urldate = {2016-05-23},
	date = {2015},
}

@article{tian_convergent_2016,
	title = {On the Convergent Properties of Word Embedding Methods},
	url = {http://arxiv.org/abs/1605.03956},
	abstract = {Do word embeddings converge to learn similar things over different initializations? How repeatable are experiments with word embeddings? Are all word embedding techniques equally reliable? In this paper we propose evaluating methods for learning word representations by their consistency across initializations. We propose a measure to quantify the similarity of the learned word representations under this setting (where they are subject to different random initializations). Our preliminary results illustrate that our metric not only measures a intrinsic property of word embedding methods but also correlates well with other evaluation metrics on downstream tasks. We believe our methods are is useful in characterizing robustness -- an important property to consider when developing new word embedding methods.},
	journaltitle = {{arXiv}:1605.03956 [cs]},
	author = {Tian, Yingtao and Kulkarni, Vivek and Perozzi, Bryan and Skiena, Steven},
	urldate = {2016-05-18},
	date = {2016-05-12},
	eprinttype = {arxiv},
	eprint = {1605.03956},
	keywords = {Computer Science - Computation and Language},
}

@article{dietterich_approximate_1998,
	title = {Approximate statistical tests for comparing supervised classification learning algorithms},
	volume = {10},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017197},
	pages = {1895--1923},
	number = {7},
	journaltitle = {Neural computation},
	author = {Dietterich, Thomas G.},
	urldate = {2016-05-12},
	date = {1998},
}

@article{eldan_power_2015,
	title = {The Power of Depth for Feedforward Neural Networks},
	url = {http://arxiv.org/abs/1512.03965},
	abstract = {We show that there is a simple (approximately radial) function on \${\textbackslash}reals{\textasciicircum}d\$, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for virtually all known activation functions, including rectified linear units, sigmoids and thresholds, and formally demonstrates that depth -- even if increased by 1 -- can be exponentially more valuable than width for standard feedforward neural networks. Moreover, compared to related results in the context of Boolean functions, our result requires fewer assumptions, and the proof techniques and construction are very different.},
	journaltitle = {{arXiv}:1512.03965 [cs, stat]},
	author = {Eldan, Ronen and Shamir, Ohad},
	urldate = {2016-05-11},
	date = {2015-12-12},
	eprinttype = {arxiv},
	eprint = {1512.03965},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{wortham_lexicon_2014,
	title = {A Lexicon of Instant Argot},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2014/01/04/technology/a-lexicon-of-the-internet-updated-by-its-users.html},
	abstract = {Urban Dictionary has become a real-time archive for new and slang terms, particularly those that have risen because of social media and the web.},
	journaltitle = {The New York Times},
	author = {Wortham, Jenna},
	urldate = {2016-05-11},
	date = {2014-01-03},
	keywords = {Crowdsourcing (Internet), Dictionaries, Peckham, Aaron, Slang, Urban Dictionary},
}

@article{bowman_generating_2015,
	title = {Generating Sentences from a Continuous Space},
	url = {http://arxiv.org/abs/1511.06349},
	abstract = {The standard unsupervised recurrent neural network language model ({RNNLM}) generates sentences one word at a time and does not work from an explicit global distributed sentence representation. In this work, we present an {RNN}-based variational autoencoder language model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate strong performance in the imputation of missing tokens, and explore many interesting properties of the latent sentence space.},
	journaltitle = {{arXiv}:1511.06349 [cs]},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	urldate = {2016-03-07},
	date = {2015-11-19},
	eprinttype = {arxiv},
	eprint = {1511.06349},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
}

@inproceedings{jiang_self-paced_2015,
	title = {Self-Paced Curriculum Learning.},
	volume = {2},
	url = {http://www.cs.cmu.edu/~lujiang/camera_ready_papers/AAAI_SPCL_2015.pdf},
	pages = {6},
	booktitle = {{AAAI}},
	author = {Jiang, Lu and Meng, Deyu and Zhao, Qian and Shan, Shiguang and Hauptmann, Alexander G.},
	urldate = {2016-02-29},
	date = {2015},
}

@article{kingma_auto-encoding_2013,
	title = {Auto-Encoding Variational Bayes},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	journaltitle = {{arXiv}:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2016-02-29},
	date = {2013-12-20},
	eprinttype = {arxiv},
	eprint = {1312.6114},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{sennrich_neural_2015,
	title = {Neural Machine Translation of Rare Words with Subword Units},
	url = {http://arxiv.org/abs/1508.07909},
	abstract = {Neural machine translation ({NMT}) models typically operate with a fixed vocabulary, so the translation of rare and unknown words is an open problem. Previous work addresses this problem through back-off dictionaries. In this paper, we introduce a simpler and more effective approach, making the {NMT} model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units, based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character \$n\$-gram models and a segmentation based on the {\textbackslash}emph\{byte pair encoding\} compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the {WMT} 15 translation tasks English-German and English-Russian by 1.1 and 1.3 {BLEU}, respectively.},
	journaltitle = {{arXiv}:1508.07909 [cs]},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	urldate = {2016-01-08},
	date = {2015-08-31},
	eprinttype = {arxiv},
	eprint = {1508.07909},
	keywords = {Computer Science - Computation and Language},
}

@article{lei_low-rank_nodate,
	title = {Low-Rank Tensors for Scoring Dependency Structures},
	url = {https://people.csail.mit.edu/regina/my_papers/tens14.pdf},
	author = {Lei, Tao and Xin, Yu and Zhang, Yuan and Barzilay, Regina and Jaakkola, Tommi},
	urldate = {2014-06-29},
}

@article{karpathy_visualizing_2015,
	title = {Visualizing and Understanding Recurrent Networks},
	url = {http://arxiv.org/abs/1506.02078},
	abstract = {Recurrent Neural Networks ({RNNs}), and specifically a variant with Long Short-Term Memory ({LSTM}), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while {LSTMs} provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the {LSTM} improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
	journaltitle = {{arXiv}:1506.02078 [cs]},
	author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
	urldate = {2016-02-24},
	date = {2015-06-05},
	eprinttype = {arxiv},
	eprint = {1506.02078},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{manning_computational_2016,
	title = {Computational Linguistics and Deep Learning},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/COLI_a_00239},
	journaltitle = {Computational Linguistics},
	author = {Manning, Christopher D.},
	urldate = {2016-02-23},
	date = {2016},
}

@inproceedings{sculley_hidden_2015,
	title = {Hidden Technical Debt in Machine Learning Systems},
	url = {http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems},
	pages = {2494--2502},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-François and Dennison, Dan},
	urldate = {2016-02-18},
	date = {2015},
}

@inproceedings{singh_towards_2015,
	title = {Towards combined matrix and tensor factorization for universal schema relation extraction},
	url = {http://www.aclweb.org/anthology/W15-1519},
	pages = {135--142},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Singh, Sameer and Rocktäschel, Tim and Riedel, Sebastian},
	urldate = {2016-02-18},
	date = {2015},
}

@article{ling_finding_2015,
	title = {Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation},
	url = {http://arxiv.org/abs/1508.02096},
	shorttitle = {Finding Function in Form},
	abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional {LSTMs}. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
	journaltitle = {{arXiv}:1508.02096 [cs]},
	author = {Ling, Wang and Luís, Tiago and Marujo, Luís and Astudillo, Ramón Fernandez and Amir, Silvio and Dyer, Chris and Black, Alan W. and Trancoso, Isabel},
	urldate = {2016-01-04},
	date = {2015-08-09},
	eprinttype = {arxiv},
	eprint = {1508.02096},
	keywords = {Computer Science - Computation and Language},
}

@incollection{vinyals_grammar_2015,
	title = {Grammar as a Foreign Language},
	url = {http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf},
	pages = {2755--2763},
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	author = {Vinyals, Oriol and Kaiser, Łukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R. and Garnett, R.},
	urldate = {2015-12-29},
	date = {2015},
}

@article{kiros_skip-thought_2015,
	title = {Skip-Thought Vectors},
	url = {http://arxiv.org/abs/1506.06726},
	abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.},
	journaltitle = {{arXiv}:1506.06726 [cs]},
	author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
	urldate = {2015-12-11},
	date = {2015-06-22},
	eprinttype = {arxiv},
	eprint = {1506.06726},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
}

@article{rasmus_semi-supervised_2015,
	title = {Semi-Supervised Learning with Ladder Networks},
	url = {http://arxiv.org/abs/1507.02672},
	abstract = {We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on the Ladder network proposed by Valpola (2015), which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised {MNIST} and {CIFAR}-10 classification, in addition to permutation-invariant {MNIST} classification with all labels.},
	journaltitle = {{arXiv}:1507.02672 [cs, stat]},
	author = {Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
	urldate = {2015-12-11},
	date = {2015-07-09},
	eprinttype = {arxiv},
	eprint = {1507.02672},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{kim_character-aware_2015,
	title = {Character-Aware Neural Language Models},
	url = {http://arxiv.org/abs/1508.06615},
	abstract = {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network ({CNN}) and a highway network over characters, whose output is given to a long short-term memory ({LSTM}) recurrent neural network language model ({RNN}-{LM}). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60\% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level {LSTM} baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.},
	journaltitle = {{arXiv}:1508.06615 [cs, stat]},
	author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
	urldate = {2015-12-11},
	date = {2015-08-26},
	eprinttype = {arxiv},
	eprint = {1508.06615},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{andoni_sketching_2014,
	title = {Sketching and Embedding are Equivalent for Norms},
	url = {http://arxiv.org/abs/1411.2577},
	abstract = {An outstanding open question (Question \#5, http://sublinear.info) asks to characterize metric spaces in which distances can be estimated using efficient sketches. Specifically, we say that a sketching algorithm is efficient if it achieves constant approximation using constant sketch size. A well-known result of Indyk (J. {ACM}, 2006) implies that a metric that admits a constant-distortion embedding into \${\textbackslash}ell\_p\$ for \$p{\textbackslash}in(0,2]\$ also admits an efficient sketching scheme. But is the converse true, i.e., is embedding into \${\textbackslash}ell\_p\$ the only way to achieve efficient sketching? We address these questions for the important special case of normed spaces, by providing an almost complete characterization of sketching in terms of embeddings. In particular, we prove that a finite-dimensional normed space allows efficient sketches if and only if it embeds (linearly) into \${\textbackslash}ell\_\{1-{\textbackslash}varepsilon\}\$ with constant distortion. We further prove that for norms that are closed under sum-product, efficient sketching is equivalent to embedding into \${\textbackslash}ell\_1\$ with constant distortion. Examples of such norms include the Earth Mover's Distance (specifically its norm variant, called Kantorovich-Rubinstein norm), and the trace norm (a.k.a. Schatten \$1\$-norm or the nuclear norm). Using known non-embeddability theorems for these norms by Naor and Schechtman ({SICOMP}, 2007) and by Pisier (Compositio. Math., 1978), we then conclude that these spaces do not admit efficient sketches either, making progress towards answering another open question (Question \#7, http://sublinear.info). Finally, we observe that resolving whether "sketching is equivalent to embedding into \${\textbackslash}ell\_1\$ for general norms" (i.e., without the above restriction) is equivalent to resolving a well-known open problem in Functional Analysis posed by Kwapien in 1969.},
	journaltitle = {{arXiv}:1411.2577 [cs, math]},
	author = {Andoni, Alexandr and Krauthgamer, Robert and Razenshteyn, Ilya},
	urldate = {2015-12-11},
	date = {2014-11-10},
	eprinttype = {arxiv},
	eprint = {1411.2577},
	keywords = {Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, Mathematics - Functional Analysis},
}

@article{abdullah_sketching_2015,
	title = {Sketching, Embedding, and Dimensionality Reduction for Information Spaces},
	url = {http://arxiv.org/abs/1503.05225},
	abstract = {Information distances like the Hellinger distance and the Jensen-Shannon divergence have deep roots in information theory and machine learning. They are used extensively in data analysis especially when the objects being compared are high dimensional empirical probability distributions built from data. However, we lack common tools needed to actually use information distances in applications efficiently and at scale with any kind of provable guarantees. We can't sketch these distances easily, or embed them in better behaved spaces, or even reduce the dimensionality of the space while maintaining the probability structure of the data. In this paper, we build these tools for information distances---both for the Hellinger distance and Jensen--Shannon divergence, as well as related measures, like the \${\textbackslash}chi{\textasciicircum}2\$ divergence. We first show that they can be sketched efficiently (i.e. up to multiplicative error in sublinear space) in the aggregate streaming model. This result is exponentially stronger than known upper bounds for sketching these distances in the strict turnstile streaming model. Second, we show a finite dimensionality embedding result for the Jensen-Shannon and \${\textbackslash}chi{\textasciicircum}2\$ divergences that preserves pair wise distances. Finally we prove a dimensionality reduction result for the Hellinger, Jensen--Shannon, and \${\textbackslash}chi{\textasciicircum}2\$ divergences that preserves the information geometry of the distributions (specifically, by retaining the simplex structure of the space). While our second result above already implies that these divergences can be explicitly embedded in Euclidean space, retaining the simplex structure is important because it allows us to continue doing inference in the reduced space. In essence, we preserve not just the distance structure but the underlying geometry of the space.},
	journaltitle = {{arXiv}:1503.05225 [cs, math]},
	author = {Abdullah, Amirali and Kumar, Ravi and {McGregor}, Andrew and Vassilvitskii, Sergei and Venkatasubramanian, Suresh},
	urldate = {2015-12-11},
	date = {2015-03-17},
	eprinttype = {arxiv},
	eprint = {1503.05225},
	keywords = {Computer Science - Computational Geometry, Computer Science - Data Structures and Algorithms, Computer Science - Information Theory},
}

@incollection{mnih_recurrent_2014,
	title = {Recurrent Models of Visual Attention},
	url = {http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf},
	pages = {2204--2212},
	booktitle = {Advances in Neural Information Processing Systems 27},
	publisher = {Curran Associates, Inc.},
	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and kavukcuoglu, koray},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	urldate = {2015-12-11},
	date = {2014},
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2015-12-11},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{luxburg_tutorial_2007,
	title = {A tutorial on spectral clustering},
	volume = {17},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/article/10.1007/s11222-007-9033-z},
	doi = {10.1007/s11222-007-9033-z},
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
	pages = {395--416},
	number = {4},
	journaltitle = {Statistics and Computing},
	shortjournal = {Stat Comput},
	author = {Luxburg, Ulrike von},
	urldate = {2015-05-04},
	date = {2007-08-22},
	langid = {english},
	keywords = {Artificial Intelligence (incl. Robotics), Graph Laplacian, Mathematics, general, Numeric Computing, Spectral clustering, Statistics and Computing/Statistics Programs, Statistics, general},
}

@article{faruqui_retrofitting_2014,
	title = {Retrofitting Word Vectors to Semantic Lexicons},
	url = {http://arxiv.org/abs/1411.4166},
	journaltitle = {{arXiv} preprint {arXiv}:1411.4166},
	author = {Faruqui, Manaal and Dodge, Jesse and Jauhar, Sujay K. and Dyer, Chris and Hovy, Eduard and Smith, Noah A.},
	urldate = {2015-04-23},
	date = {2014},
}

@article{van_rooyen_theory_2015,
	title = {A Theory of Feature Learning},
	url = {http://arxiv.org/abs/1504.00083},
	journaltitle = {{arXiv} preprint {arXiv}:1504.00083},
	author = {van Rooyen, Brendan and Williamson, Robert C.},
	urldate = {2015-04-23},
	date = {2015},
}

@online{levyomer_improving_nodate,
	title = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
	url = {https://levyomer.wordpress.com/2015/03/30/improving-distributional-similarity-with-lessons-learned-from-word-embeddings/},
	abstract = {Improving Distributional Similarity with Lessons Learned from Word Embeddings Omer Levy, Yoav Goldberg, and Ido Dagan. {TACL} 2015. [pdf] We reveal that much of the performance gains of word embeddin...},
	titleaddon = {Omer Levy},
	author = {{levyomer}},
	urldate = {2015-04-21},
}

@article{yang_unsupervised_nodate,
	title = {Unsupervised Multi-Domain Adaptation with Feature Embeddings},
	url = {http://www.cc.gatech.edu/~yyang319/download/yang-naacl-2015.pdf},
	author = {Yang, Yi and Eisenstein, Jacob},
	urldate = {2015-04-21},
}

@book{chung_spectral_1996,
	location = {Providence, R.I.},
	title = {Spectral Graph Theory},
	isbn = {9780821803158},
	abstract = {Beautifully written and elegantly presented, this book is based on 10 lectures given at the {CBMS} workshop on spectral graph theory in June 1994 at Fresno State University. Chung's well-written exposition can be likened to a conversation with a good teacher--one who not only gives you the facts, but tells you what is really going on, why it is worth doing, and how it is related to familiar ideas in other areas. The monograph is accessible to the nonexpert who is interested in reading about this evolving area of mathematics.},
	pagetotal = {207},
	publisher = {American Mathematical Society},
	author = {Chung, Fan R. K.},
	date = {1996-12-03},
}

@inproceedings{wang_active_2014,
	title = {Active transfer learning under model shift},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2014c2_wangi14},
	pages = {1305--1313},
	booktitle = {Proceedings of the 31st International Conference on Machine Learning ({ICML}-14)},
	author = {Wang, Xuezhi and Huang, Tzu-Kuo and Schneider, Jeff},
	urldate = {2015-04-14},
	date = {2014},
}

@inproceedings{raina_constructing_2006,
	title = {Constructing informative priors using transfer learning},
	url = {http://dl.acm.org/citation.cfm?id=1143934},
	pages = {713--720},
	booktitle = {Proceedings of the 23rd international conference on Machine learning},
	publisher = {{ACM}},
	author = {Raina, Rajat and Ng, Andrew Y. and Koller, Daphne},
	urldate = {2015-04-09},
	date = {2006},
}

@inproceedings{naomi_saphra_amrica:_2015,
	title = {{AMRICA}: an {AMR} Inspector for Cross-language Alignments},
	rights = {All rights reserved},
	abstract = {Abstract Meaning Representation ({AMR}), an annotation scheme for natural language semantics, has drawn attention for its simplicity and representational power. Because {AMR} annotations are not designed for human readability, we present {AMRICA}, a visual aid for exploration of {AMR} annotations. {AMRICA} can visualize an {AMR} or the difference between two {AMRs} to help users diagnose interannotator disagreement or errors from an {AMR} parser. {AMRICA} can also automatically align and visualize the {AMRs} of a sentence and its translation in a parallel text. We believe {AMRICA} will simplify and streamline exploratory research on cross-lingual {AMR} corpora.},
	eventtitle = {{NAACL}},
	booktitle = {Proc. of {NAACL}},
	author = {{Naomi Saphra} and {Adam Lopez}},
	date = {2015},
}

@report{jing_xiang_linear_2014,
	title = {Linear Algebra Review},
	author = {{Jing Xiang}},
	date = {2014-03},
}

@report{zico_kolter_linear_2008,
	title = {Linear Algebra Review and Reference},
	author = {{Zico Kolter}},
	date = {2008-10},
}
@inproceedings{blitzer_domain_2006,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Domain Adaptation with Structural Correspondence Learning},
	isbn = {1-932432-73-6},
	url = {http://dl.acm.org/citation.cfm?id=1610075.1610094},
	series = {{EMNLP} '06},
	abstract = {Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many {NLP} tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.},
	pages = {120--128},
	booktitle = {Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Blitzer, John and {McDonald}, Ryan and Pereira, Fernando},
	urldate = {2015-01-15},
	date = {2006},
}

@article{richtarik_distributed_2013,
	title = {Distributed coordinate descent method for learning with big data},
	url = {http://arxiv.org/abs/1310.2059},
	journaltitle = {{arXiv} preprint {arXiv}:1310.2059},
	author = {Richtárik, Peter and Takáč, Martin},
	urldate = {2015-02-23},
	date = {2013},
}

@incollection{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	url = {http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf},
	pages = {3320--3328},
	booktitle = {Advances in Neural Information Processing Systems 27},
	publisher = {Curran Associates, Inc.},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	urldate = {2015-02-19},
	date = {2014},
}

@article{kaeshammer_complex_nodate,
	title = {On Complex Word Alignment Configurations},
	url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/390_Paper.pdf},
	author = {Kaeshammer, Miriam and Westburg, Anika},
	urldate = {2015-02-16},
	keywords = {alopez},
}

@article{kaeshammer_synchronous_2013,
	title = {Synchronous linear context-free rewriting systems for machine translation},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.380.8748&rep=rep1&type=pdf},
	pages = {68},
	journaltitle = {Syntax, Semantics and Structure in Statistical Translation},
	author = {Kaeshammer, Miriam},
	urldate = {2015-02-16},
	date = {2013},
	keywords = {alopez},
}

@incollection{irsoy_deep_2014,
	title = {Deep Recursive Neural Networks for Compositionality in Language},
	url = {http://papers.nips.cc/paper/5551-deep-recursive-neural-networks-for-compositionality-in-language.pdf},
	pages = {2096--2104},
	booktitle = {Advances in Neural Information Processing Systems 27},
	publisher = {Curran Associates, Inc.},
	author = {Irsoy, Ozan and Cardie, Claire},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	urldate = {2015-02-16},
	date = {2014},
}

@incollection{ba_deep_2014,
	title = {Do Deep Nets Really Need to be Deep?},
	url = {http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf},
	pages = {2654--2662},
	booktitle = {Advances in Neural Information Processing Systems 27},
	publisher = {Curran Associates, Inc.},
	author = {Ba, Jimmy and Caruana, Rich},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	urldate = {2015-02-09},
	date = {2014},
}

@article{ma_adaptive_2014,
	title = {Adaptive Sparse Reduced-rank Regression},
	url = {http://arxiv.org/abs/1403.1922},
	journaltitle = {{arXiv} preprint {arXiv}:1403.1922},
	author = {Ma, Zongming and Sun, Tingni},
	urldate = {2015-01-22},
	date = {2014},
}

@incollection{francis_bach_convex_2011,
	title = {Convex Optimization with Sparsity-Inducing Norms},
	booktitle = {Optimization for Machine Learning},
	publisher = {{MIT} Press},
	author = {{Francis Bach}},
	date = {2011},
}

@book{erwin_kreyszig_introductory_nodate,
	title = {Introductory Functional Analysis with Applications},
	author = {{Erwin Kreyszig}},
}

@book{gerald_teschl_topics_nodate,
	title = {Topics in Real and Functional Analysis},
	author = {{Gerald Teschl}},
}

@inproceedings{gouws_unsupervised_2011,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Unsupervised Mining of Lexical Variants from Noisy Text},
	isbn = {978-1-937284-13-8},
	url = {http://dl.acm.org/citation.cfm?id=2140458.2140468},
	series = {{EMNLP} '11},
	abstract = {The amount of data produced in user-generated content continues to grow at a staggering rate. However, the text found in these media can deviate wildly from the standard rules of orthography, syntax and even semantics and present significant problems to downstream applications which make use of this noisy data. In this paper we present a novel unsupervised method for extracting domain-specific lexical variants given a large volume of text. We demonstrate the utility of this method by applying it to normalize text messages found in the online social media service, Twitter, into their most likely standard English versions. Our method yields a 20\% reduction in word error rate over an existing state-of-the-art approach.},
	pages = {82--90},
	booktitle = {Proceedings of the First Workshop on Unsupervised Learning in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Gouws, Stephan and Hovy, Dirk and Metzler, Donald},
	urldate = {2015-01-19},
	date = {2011},
	keywords = {m},
}

@article{chen_marginalized_2012,
	title = {Marginalized Denoising Autoencoders for Domain Adaptation},
	url = {http://arxiv.org/abs/1206.4683},
	abstract = {Stacked denoising autoencoders ({SDAs}) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. {SDAs} learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized {SDA} ({mSDA}) that addresses two crucial limitations of {SDAs}: high computational cost and lack of scalability to high-dimensional features. In contrast to {SDAs}, our approach of {mSDA} marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters ? in fact, they are computed in closed-form. Consequently, {mSDA}, which can be implemented in only 20 lines of {MATLAB}{\textasciicircum}\{{TM}\}, significantly speeds up {SDAs} by two orders of magnitude. Furthermore, the representations learnt by {mSDA} are as effective as the traditional {SDAs}, attaining almost identical accuracies in benchmark tasks.},
	journaltitle = {{arXiv}:1206.4683 [cs]},
	author = {Chen, Minmin and Xu, Zhixiang and Weinberger, Kilian and Sha, Fei},
	urldate = {2015-01-15},
	date = {2012-06-18},
	eprinttype = {arxiv},
	eprint = {1206.4683},
	keywords = {Computer Science - Learning},
}

@article{udell_generalized_2014,
	title = {Generalized Low Rank Models},
	url = {http://arxiv.org/abs/1410.0342},
	journaltitle = {{arXiv} preprint {arXiv}:1410.0342},
	author = {Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd, Stephen},
	urldate = {2015-01-08},
	date = {2014},
}

@article{duchi_adaptive_2011,
	title = {Adaptive subgradient methods for online learning and stochastic optimization},
	volume = {12},
	url = {http://dl.acm.org/citation.cfm?id=2021068},
	pages = {2121--2159},
	journaltitle = {The Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	urldate = {2015-01-08},
	date = {2011},
}

@article{yang_unsupervised_2014,
	title = {Unsupervised Domain Adaptation with Feature Embeddings},
	url = {http://arxiv.org/abs/1412.4385},
	journaltitle = {{arXiv} preprint {arXiv}:1412.4385},
	author = {Yang, Yi and Eisenstein, Jacob},
	urldate = {2015-01-08},
	date = {2014},
}

@article{hoffman_stochastic_2012,
	title = {Stochastic Variational Inference},
	url = {http://arxiv.org/abs/1206.7051},
	abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
	journaltitle = {{arXiv}:1206.7051 [cs, stat]},
	author = {Hoffman, Matt and Blei, David M. and Wang, Chong and Paisley, John},
	urldate = {2014-12-30},
	date = {2012-06-29},
	eprinttype = {arxiv},
	eprint = {1206.7051},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology, from reddit},
}

@article{jeffrey_pennington_glove:_nodate,
	title = {{GloVe}: Global Vectors for Word Representation},
	url = {http://nlp.stanford.edu/pubs/glove.pdf},
	shorttitle = {{GloVe}},
	author = {{Jeffrey Pennington} and Manning, Christopher D and {Richard Socher}},
	urldate = {2014-09-16},
}

@incollection{goodfellow_generative_2014,
	title = {Generative Adversarial Nets},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	pages = {2672--2680},
	booktitle = {Advances in Neural Information Processing Systems 27},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	urldate = {2014-12-05},
	date = {2014},
}

@article{eisenstein_diffusion_2014,
	title = {Diffusion of Lexical Change in Social Media},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pone.0113114},
	doi = {10.1371/journal.pone.0113114},
	abstract = {Computer-mediated communication is driving fundamental changes in the nature of written language. We investigate these changes by statistical analysis of a dataset comprising 107 million Twitter messages (authored by 2.7 million unique user accounts). Using a latent vector autoregressive model to aggregate across thousands of words, we identify high-level patterns in diffusion of linguistic change over the United States. Our model is robust to unpredictable changes in Twitter's sampling rate, and provides a probabilistic characterization of the relationship of macro-scale linguistic influence to a set of demographic and geographic predictors. The results of this analysis offer support for prior arguments that focus on geographical proximity and population size. However, demographic similarity – especially with regard to race – plays an even more central role, as cities with similar racial demographics are far more likely to share linguistic influence. Rather than moving towards a single unified “netspeak” dialect, language evolution in computer-mediated communication reproduces existing fault lines in spoken American English.},
	pages = {e113114},
	number = {11},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Eisenstein, Jacob and O'Connor, Brendan and Smith, Noah A. and Xing, Eric P.},
	urldate = {2014-11-21},
	date = {2014-11-19},
}

@article{dorr_machine_1994,
	title = {Machine translation divergences: A formal description and proposed solution},
	volume = {20},
	url = {http://dl.acm.org/citation.cfm?id=203993},
	shorttitle = {Machine translation divergences},
	pages = {597--633},
	number = {4},
	journaltitle = {Computational Linguistics},
	author = {Dorr, Bonnie J.},
	urldate = {2014-11-21},
	date = {1994},
	keywords = {Adam},
}

@inproceedings{marecek_automatic_2008,
	title = {Automatic Alignment of Czech and English Deep Syntactic Dependency Trees},
	url = {http://ufal.mff.cuni.cz/~toman/pcedt/publications/Marecek2008_eamt.pdf},
	pages = {102--111},
	booktitle = {Proceedings of the Twelfth {EAMT} Conference, Hamburg, {HITEC} {eV}},
	author = {Marecek, David and Zabokrtskỳ, Zdenek and Novák, Václav},
	urldate = {2014-11-21},
	date = {2008},
}

@article{lu_how_2014,
	title = {How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets},
	url = {http://arxiv.org/abs/1411.4000},
	abstract = {In this paper, we investigate how to scale up kernel methods to take on large-scale problems, on which deep neural networks have been prevailing. To this end, we leverage existing techniques and develop new ones. These techniques include approximating kernel functions with features derived from random projections, parallel training of kernel models with 100 million parameters or more, and new schemes for combining kernel functions as a way of learning representations. We demonstrate how to muster those ideas skillfully to implement large-scale kernel machines for challenging problems in automatic speech recognition. We valid our approaches with extensive empirical studies on real-world speech datasets on the tasks of acoustic modeling. We show that our kernel models are equally competitive as well-engineered deep neural networks ({DNNs}). In particular, kernel models either attain similar performance to, or surpass their {DNNs} counterparts. Our work thus avails more tools to machine learning researchers in addressing large-scale learning problems.},
	journaltitle = {{arXiv}:1411.4000 [cs, stat]},
	author = {Lu, Zhiyun and May, Avner and Liu, Kuan and Garakani, Alireza Bagheri and Guo, Dong and Bellet, Aurélien and Fan, Linxi and Collins, Michael and Kingsbury, Brian and Picheny, Michael and Sha, Fei},
	urldate = {2014-11-20},
	date = {2014-11-14},
	eprinttype = {arxiv},
	eprint = {1411.4000},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@unpublished{raman_arora_kernel_nodate,
	title = {Kernel {PCA} Notes (Representation Learning Lecture)},
	author = {{Raman Arora}},
}

@unpublished{raman_arora_cca_nodate,
	title = {{CCA} Notes},
	author = {{Raman Arora}},
}

@article{kwiatkowski_scaling_2013,
	title = {Scaling semantic parsers with on-the-fly ontology matching},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.409.14},
	author = {Kwiatkowski, Tom and Choi, Eunsol and Artzi, Yoav and Zettlemoyer, Luke},
	urldate = {2014-10-27},
	date = {2013},
}

@inproceedings{lee_algorithms_2001,
	title = {Algorithms for non-negative matrix factorization},
	url = {http://papers.nips.cc/paper/1861-alg},
	pages = {556--562},
	booktitle = {Advances in neural information processing systems},
	author = {Lee, Daniel D. and Seung, H. Sebastian},
	urldate = {2014-10-16},
	date = {2001},
}

@article{arora_multiplicative_2012,
	title = {The Multiplicative Weights Update Method: a Meta-Algorithm and Applications.},
	volume = {8},
	url = {http://tocbeta.cs.uchicago.edu/articles/v008a006/v008a006.pdf},
	shorttitle = {The Multiplicative Weights Update Method},
	pages = {121--164},
	number = {1},
	journaltitle = {Theory of Computing},
	author = {Arora, Sanjeev and Hazan, Elad and Kale, Satyen},
	urldate = {2014-10-16},
	date = {2012},
	keywords = {Raman},
}

@article{goodman_probabilistic_2014,
	title = {Probabilistic semantics and pragmatics: Uncertainty in language and thought},
	url = {https://www.stanford.edu/~ngoodman/papers/Goodman-HCS-final.pdf},
	shorttitle = {Probabilistic semantics and pragmatics},
	journaltitle = {Handbook of Contemporary Semantic Theory, Wiley-Blackwell},
	author = {Goodman, Noah D. and Lassiter, Daniel},
	urldate = {2014-10-16},
	date = {2014},
}

@article{hinton_reducing_2006,
	title = {Reducing the Dimensionality of Data with Neural Networks},
	volume = {313},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/313/5786/504},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	pages = {504--507},
	number = {5786},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	urldate = {2014-10-15},
	date = {2006-07-28},
	langid = {english},
	pmid = {16873662},
}

@article{scholkopf_nonlinear_1998,
	title = {Nonlinear component analysis as a kernel eigenvalue problem},
	volume = {10},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017467},
	pages = {1299--1319},
	number = {5},
	journaltitle = {Neural computation},
	author = {Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
	urldate = {2014-10-14},
	date = {1998},
}

@incollection{scholkopf_kernel_1997,
	title = {Kernel principal component analysis},
	url = {http://link.springer.com/chapter/10.1007/BFb0020217},
	pages = {583--588},
	booktitle = {Artificial Neural Networks—{ICANN}'97},
	publisher = {Springer},
	author = {Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
	urldate = {2014-10-14},
	date = {1997},
}

@inproceedings{balzano_online_2010,
	title = {Online identification and tracking of subspaces from highly incomplete information},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5706976},
	pages = {704--711},
	booktitle = {Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on},
	publisher = {{IEEE}},
	author = {Balzano, Laura and Nowak, Robert and Recht, Benjamin},
	urldate = {2014-10-09},
	date = {2010},
}

@inproceedings{arora_stochastic_2012,
	title = {Stochastic optimization for {PCA} and {PLS}.},
	pages = {861--868},
	booktitle = {Allerton Conference},
	publisher = {Citeseer},
	author = {Arora, Raman and Cotter, Andrew and Livescu, Karen and Srebro, Nathan},
	date = {2012},
}

@article{collobert_natural_2011,
	title = {Natural language processing (almost) from scratch},
	volume = {12},
	url = {http://dl.acm.org/citation.cfm?id=2078186},
	pages = {2493--2537},
	journaltitle = {The Journal of Machine Learning Research},
	author = {Collobert, Ronan and Weston, Jason and Bottou, Léon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	urldate = {2014-10-02},
	date = {2011},
}

@inproceedings{levy_neural_2014,
	title = {Neural Word Embedding as Implicit Matrix Factorization},
	eventtitle = {{NIPS} 2014},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Levy, Omer and Goldberg, Yoav},
	date = {2014},
	keywords = {alopez},
}

@inproceedings{bailly_unsupervised_2013,
	title = {Unsupervised spectral learning of finite state transducers},
	url = {http://papers.nips.cc/paper/4862-unsupervised-spectral-learning-of-finite-state-transducers},
	pages = {800--808},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Bailly, Raphaël and Carreras, Xavier and Quattoni, Ariadna},
	urldate = {2014-09-16},
	date = {2013},
}

@inproceedings{levy_dependency-based_2014,
	title = {Dependency-Based Word Embeddings},
	volume = {2},
	url = {http://www.cs.bgu.ac.il/~yoavg/publications/acl2014syntemb.pdf},
	booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
	author = {Levy, Omer and Goldberg, Yoav},
	urldate = {2014-09-16},
	date = {2014},
	keywords = {Raman, embeddings},
}
@inproceedings{zou_bilingual_2013,
	title = {Bilingual Word Embeddings for Phrase-Based Machine Translation.},
	url = {http://www.aclweb.org/anthology/D13-1141},
	pages = {1393--1398},
	booktitle = {{EMNLP}},
	author = {Zou, Will Y. and Socher, Richard and Cer, Daniel M. and Manning, Christopher D.},
	urldate = {2014-09-16},
	date = {2013},
}

@article{faruqui_improving_2014,
	title = {Improving vector space word representations using multilingual correlation},
	url = {http://www.aclweb.org/anthology/E14-1049},
	journaltitle = {Proc. of {EACL}. Association for Computational Linguistics},
	author = {Faruqui, Manaal and Dyer, Chris},
	urldate = {2014-09-16},
	date = {2014},
}

@article{lebret_word_2013,
	title = {Word Emdeddings through Hellinger {PCA}},
	url = {http://arxiv.org/abs/1312.5542},
	abstract = {Word embeddings resulting from neural language models have been shown to be successful for a large variety of {NLP} tasks. However, such architecture might be difficult to train and time-consuming. Instead, we propose to drastically simplify the word embeddings computation through a Hellinger {PCA} of the word co-occurence matrix. We compare those new word embeddings with some well-known embeddings on {NER} and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks.},
	journaltitle = {{arXiv}:1312.5542 [cs]},
	author = {Lebret, Rémi and Collobert, Ronan},
	urldate = {2014-09-16},
	date = {2013-12-19},
	eprinttype = {arxiv},
	eprint = {1312.5542},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Raman},
}

@article{mikolov_efficient_2013,
	title = {Efficient estimation of word representations in vector space},
	url = {http://arxiv.org/abs/1301.3781},
	journaltitle = {{arXiv} preprint {arXiv}:1301.3781},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2014-09-16},
	date = {2013},
	keywords = {Raman},
}

@article{dhillon_two_2012,
	title = {Two Step {CCA}: A new spectral method for estimating vector models of words},
	url = {http://arxiv.org/abs/1206.6403},
	shorttitle = {Two Step {CCA}},
	abstract = {Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank "dictionary" by an eigen-decomposition of the word co-occurrence matrix (e.g. using {PCA} or {CCA}). In this paper, we present a new spectral method based on {CCA} to learn an eigenword dictionary. Our improved procedure computes two set of {CCAs}, the first one between the left and right contexts of the given word and the second one between the projections resulting from this {CCA} and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step {CCA} ({TSCCA}) procedure on the tasks of {POS} tagging and sentiment classification.},
	journaltitle = {{arXiv}:1206.6403 [cs]},
	author = {Dhillon, Paramveer and Rodu, Jordan and Foster, Dean and Ungar, Lyle},
	urldate = {2014-09-16},
	date = {2012-06-27},
	eprinttype = {arxiv},
	eprint = {1206.6403},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Raman},
}

@article{little_praise_2013,
	title = {In Praise of Simplicity not Mathematistry! Ten Simple Powerful Ideas for the Statistical Scientist},
	volume = {108},
	issn = {0162-1459},
	url = {http://amstat.tandfonline.com/doi/full/10.1080/01621459.2013.787932},
	doi = {10.1080/01621459.2013.787932},
	pages = {359--369},
	number = {502},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Little, Roderick J.},
	urldate = {2014-09-08},
	date = {2013-06-01},
}

@article{bornn_herded_2013,
	title = {Herded Gibbs Sampling},
	url = {http://arxiv.org/abs/1301.4168},
	abstract = {The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an \$O(1/T)\$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with {MRFs} and named entity recognition with {CRFs}. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.},
	journaltitle = {{arXiv}:1301.4168 [cs, stat]},
	author = {Bornn, Luke and Chen, Yutian and de Freitas, Nando and Eskelin, Mareija and Fang, Jing and Welling, Max},
	urldate = {2014-09-05},
	date = {2013-01-17},
	eprinttype = {arxiv},
	eprint = {1301.4168},
	keywords = {Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning},
}

@article{kuhlmann_linkoping:_2014,
	title = {Linköping: Cubic-Time Graph Parsing with a Simple Scoring Scheme},
	url = {http://www.aclweb.org/anthology/S/S14/S14-2.pdf#page=415},
	shorttitle = {Linköping},
	pages = {395},
	journaltitle = {{SemEval} 2014},
	author = {Kuhlmann, Marco},
	urldate = {2014-09-04},
	date = {2014},
	keywords = {alopez},
}

@article{korattikara_austerity_2013,
	title = {Austerity in {MCMC} land: Cutting the Metropolis-Hastings budget},
	url = {http://arxiv.org/abs/1304.5299},
	shorttitle = {Austerity in {MCMC} land},
	journaltitle = {{arXiv} preprint {arXiv}:1304.5299},
	author = {Korattikara, Anoop and Chen, Yutian and Welling, Max},
	urldate = {2014-09-02},
	date = {2013},
}

@article{bengio_representation_2012,
	title = {Representation Learning: A Review and New Perspectives},
	url = {http://arxiv.org/abs/1206.5538},
	shorttitle = {Representation Learning},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for {AI} is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	journaltitle = {{arXiv}:1206.5538 [cs]},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	urldate = {2014-09-01},
	date = {2012-06-24},
	eprinttype = {arxiv},
	eprint = {1206.5538},
	keywords = {Computer Science - Learning},
}

@inproceedings{huang_adversarial_2011,
	location = {New York, {NY}, {USA}},
	title = {Adversarial Machine Learning},
	isbn = {978-1-4503-1003-1},
	url = {http://doi.acm.org/10.1145/2046684.2046692},
	doi = {10.1145/2046684.2046692},
	series = {{AISec} '11},
	abstract = {In this paper (expanded from an invited talk at {AISEC} 2010), we discuss an emerging field of study: adversarial machine learning---the study of effective machine learning techniques against an adversarial opponent. In this paper, we: give a taxonomy for classifying attacks against online machine learning algorithms; discuss application-specific factors that limit an adversary's capabilities; introduce two models for modeling an adversary's capabilities; explore the limits of an adversary's knowledge about the algorithm, feature space, training, and input data; explore vulnerabilities in machine learning algorithms; discuss countermeasures against attacks; introduce the evasion challenge; and discuss privacy-preserving learning techniques.},
	pages = {43--58},
	booktitle = {Proceedings of the 4th {ACM} Workshop on Security and Artificial Intelligence},
	publisher = {{ACM}},
	author = {Huang, Ling and Joseph, Anthony D. and Nelson, Blaine and Rubinstein, Benjamin I.P. and Tygar, J. D.},
	urldate = {2014-08-12},
	date = {2011},
	keywords = {adversarial learning, computer security, game theory, intrusion detection, machine learning, security metrics, spam filters, statistical learning},
}

@article{knuth_dancing_2000,
	title = {Dancing links},
	url = {http://arxiv.org/abs/cs/0011047},
	journaltitle = {{arXiv} preprint cs/0011047},
	author = {Knuth, Donald E.},
	urldate = {2014-08-12},
	date = {2000},
}

@article{costello_surprisingly_2014,
	title = {Surprisingly rational: Probability theory plus noise explains biases in judgment},
	volume = {121},
	issn = {1939-1471},
	doi = {10.1037/a0037010},
	shorttitle = {Surprisingly rational},
	abstract = {The systematic biases seen in people's probability judgments are typically taken as evidence that people do not use the rules of probability theory when reasoning about probability but instead use heuristics, which sometimes yield reasonable judgments and sometimes yield systematic biases. This view has had a major impact in economics, law, medicine, and other fields; indeed, the idea that people cannot reason with probabilities has become a truism. We present a simple alternative to this view, where people reason about probability according to probability theory but are subject to random variation or noise in the reasoning process. In this account the effect of noise is canceled for some probabilistic expressions. Analyzing data from 2 experiments, we find that, for these expressions, people's probability judgments are strikingly close to those required by probability theory. For other expressions, this account produces systematic deviations in probability estimates. These deviations explain 4 reliable biases in human probabilistic reasoning (conservatism, subadditivity, conjunction, and disjunction fallacies). These results suggest that people's probability judgments embody the rules of probability theory and that biases in those judgments are due to the effects of random noise. ({PsycINFO} Database Record (c) 2014 {APA}, all rights reserved).},
	pages = {463--480},
	number = {3},
	journaltitle = {Psychological Review},
	shortjournal = {Psychol Rev},
	author = {Costello, Fintan and Watts, Paul},
	date = {2014-07},
	pmid = {25090427},
}

@inproceedings{gutmann_noise-contrastive_2010,
	title = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GutmannH10.pdf},
	shorttitle = {Noise-contrastive estimation},
	pages = {297--304},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	author = {Gutmann, Michael and Hyvärinen, Aapo},
	urldate = {2014-08-11},
	date = {2010},
}

@inproceedings{gonzalez_parallel_2011,
	title = {Parallel gibbs sampling: From colored fields to thin junction trees},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GonzalezLGG11.pdf},
	shorttitle = {Parallel gibbs sampling},
	pages = {324--332},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	author = {Gonzalez, Joseph and Low, Yucheng and Gretton, Arthur and Guestrin, Carlos},
	urldate = {2014-08-11},
	date = {2011},
}

@inproceedings{bouchard-cote_randomized_2009,
	title = {Randomized pruning: Efficiently calculating expectations in large dynamic programs},
	url = {http://papers.nips.cc/paper/3710-randomized-pruning-efficiently-calculating-expectations-in-large-dynamic-programs},
	shorttitle = {Randomized pruning},
	pages = {144--152},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Bouchard-Côté, Alexandre and Petrov, Slav and Klein, Dan},
	urldate = {2014-08-08},
	date = {2009},
}

@inproceedings{kok_hitting_2010,
	title = {Hitting the right paraphrases in good time},
	url = {http://dl.acm.org/citation.cfm?id=1858016},
	pages = {145--153},
	booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Kok, Stanley and Brockett, Chris},
	urldate = {2014-08-08},
	date = {2010},
}

@inproceedings{levy_linguistic_2014,
	title = {Linguistic Regularities in Sparse and Explicit Word Representations},
	url = {http://www.cs.bgu.ac.il/~yoavg/publications/conll2014analogies.pdf},
	booktitle = {Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Baltimore, Maryland, {USA}, June. Association for Computational Linguistics},
	author = {Levy, Omer and Goldberg, Yoav},
	urldate = {2014-08-07},
	date = {2014},
}

@article{dyer_notes_nodate,
	title = {Notes on {AdaGrad}},
	url = {http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf},
	author = {Dyer, Chris},
	urldate = {2014-08-07},
}

@inproceedings{cohen_provably_2014,
	title = {A provably correct learning algorithm for latent-variable pcfgs},
	url = {http://homepages.inf.ed.ac.uk/scohen/acl14pivot+supp.pdf},
	booktitle = {Proceedings of {ACL}},
	author = {Cohen, Shay B. and Collins, Michael},
	urldate = {2014-08-07},
	date = {2014},
	keywords = {Raman},
}

@inproceedings{baroni_nouns_2010,
	title = {Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space},
	url = {http://dl.acm.org/citation.cfm?id=1870773},
	shorttitle = {Nouns are vectors, adjectives are matrices},
	pages = {1183--1193},
	booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Baroni, Marco and Zamparelli, Roberto},
	urldate = {2014-07-25},
	date = {2010},
}

@article{lampson_hints_1983,
	title = {Hints for computer system design},
	volume = {17},
	url = {http://dl.acm.org/citation.cfm?id=806614},
	pages = {33--48},
	number = {5},
	journaltitle = {{ACM} {SIGOPS} Operating Systems Review},
	author = {Lampson, Butler W.},
	urldate = {2014-07-16},
	date = {1983},
}

@book{hal_daume_course_nodate,
	title = {A Course In Machine Learning},
	author = {{Hal Daume}},
}

@article{minas_parsing_2006,
	title = {Parsing of adaptive star grammars},
	volume = {4},
	url = {http://journal.ub.tu-berlin.de/index.php/eceasst/article/download/17/8},
	journaltitle = {Electronic Communications of the {EASST}},
	author = {Minas, Mark},
	urldate = {2014-07-16},
	date = {2006},
}

@article{drewes_adaptive_2010,
	title = {Adaptive star grammars and their languages},
	volume = {411},
	url = {http://www.sciencedirect.com/science/article/pii/S0304397510002550},
	pages = {3090--3109},
	number = {34},
	journaltitle = {Theoretical Computer Science},
	author = {Drewes, Frank and Hoffmann, Berthold and Janssens, Dirk and Minas, Mark},
	urldate = {2014-07-16},
	date = {2010},
}

@incollection{drewes_adaptive_2006,
	title = {Adaptive star grammars},
	url = {http://link.springer.com/chapter/10.1007/11841883_7},
	pages = {77--91},
	booktitle = {Graph Transformations},
	publisher = {Springer},
	author = {Drewes, Frank and Hoffmann, Berthold and Janssens, Dirk and Minas, Mark and Van Eetvelde, Niels},
	urldate = {2014-07-16},
	date = {2006},
}

@article{xue_not_nodate,
	title = {Not an Interlingua, But Close: Comparison of English {AMRs} to Chinese and Czech},
	url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/384_Paper.pdf},
	shorttitle = {Not an Interlingua, But Close},
	author = {Xue, Nianwen and Bojar, Ondrej and Hajic, Jan and Palmer, Martha and Urešová, Zdenka and Zhang, Xiuhong},
	urldate = {2014-07-16},
}

@inproceedings{fokkens_offspring_2013,
	title = {Offspring from Reproduction Problems: What Replication Failure Teaches Us.},
	url = {http://wordpress.let.vupr.nl/reproducingnlpresearch/files/2013/05/FokkensErpPostmaPedersenVossenFreireACL2013.pdf},
	shorttitle = {Offspring from Reproduction Problems},
	pages = {1691--1701},
	booktitle = {{ACL} (1)},
	author = {Fokkens, Antske and Van Erp, Marieke and Postma, Marten and Pedersen, Ted and Vossen, Piek and Freire, Nuno},
	urldate = {2014-07-11},
	date = {2013},
}

@inproceedings{burges_hamilton_nodate,
	title = {Hamilton, and Hullender, G. 2005. Learning to rank using gradient descent},
	publisher = {{ICML}},
	author = {Burges, C. and Shaked, T. and Renshaw, E. and Lazier, A. and Deeds, M.},
}

@article{domingos_few_2012,
	title = {A few useful things to know about machine learning},
	volume = {55},
	url = {http://dl.acm.org/citation.cfm?id=2347755},
	pages = {78--87},
	number = {10},
	journaltitle = {Communications of the {ACM}},
	author = {Domingos, Pedro},
	urldate = {2014-07-10},
	date = {2012},
}

@article{zhang_kneser-ney_nodate,
	title = {Kneser-Ney Smoothing on Expected Counts},
	url = {http://www.isi.edu/~chiang/papers/zhang-acl14.pdf},
	author = {Zhang, Hui and Chiang, David},
	urldate = {2014-07-10},
}

@inproceedings{yogatama_linguistic_2014,
	title = {Linguistic Structured Sparsity in Text Categorization},
	url = {http://www.cs.cmu.edu/~nasmith/papers/yogatama+smith.acl14.pdf},
	booktitle = {Proc. of {ACL}},
	author = {Yogatama, Dani and Smith, Noah A.},
	urldate = {2014-07-10},
	date = {2014},
}

@inproceedings{martins_concise_2009,
	title = {Concise integer linear programming formulations for dependency parsing},
	url = {http://dl.acm.org/citation.cfm?id=1687928},
	pages = {342--350},
	booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}: Volume 1-Volume 1},
	publisher = {Association for Computational Linguistics},
	author = {Martins, André {FT} and Smith, Noah A. and Xing, Eric P.},
	urldate = {2014-07-10},
	date = {2009},
}

@article{goldberg_tabular_nodate,
	title = {A Tabular Method for Dynamic Oracles in Transition-Based Parsing},
	url = {http://www.cs.bgu.ac.il/~yoavg/publications/tacl2014arcstandard.pdf},
	author = {Goldberg, Yoav and Sartorio, Francesco and Satta, Giorgio},
	urldate = {2014-07-10},
}

@inproceedings{baroni_dont_2014,
	title = {Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors},
	volume = {1},
	url = {http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf},
	booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
	author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, Germán},
	urldate = {2014-07-10},
	date = {2014},
}

@article{knight_bayesian_2009,
	title = {Bayesian Inference with Tears},
	url = {http://www.illc.uva.nl/LaCo/clas/ull14/papers/knight09bayeswithtears.pdf},
	journaltitle = {A tutorial workbook for natural language researchers},
	author = {Knight, Kevin},
	urldate = {2014-07-10},
	date = {2009},
}

@report{resnik_gibbs_2010,
	title = {Gibbs sampling for the uninitiated},
	url = {http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA523027},
	institution = {{DTIC} Document},
	author = {Resnik, Philip and Hardisty, Eric},
	urldate = {2014-07-10},
	date = {2010},
}

@article{xu_shift-reduce_nodate,
	title = {Shift-Reduce {CCG} Parsing with a Dependency Model},
	url = {http://www.cl.cam.ac.uk/~wx217/papers/acl2014.pdf},
	author = {Xu, Wenduan and Clark, Stephen and Zhang, Yue},
	urldate = {2014-06-29},
}

@article{ji_representation_nodate,
	title = {Representation Learning for Text-level Discourse Parsing},
	url = {http://www.cc.gatech.edu/grads/y/yji37/papers/ji-acl-2014.pdf},
	author = {Ji, Yangfeng and Eisenstein, Jacob},
	urldate = {2014-06-29},
}

@article{schluter_maximum_nodate,
	title = {On maximum spanning {DAG} algorithms for semantic {DAG} parsing},
	url = {http://sp14.ws/pub/schluter-sp14-2014.pdf},
	author = {Schluter, Natalie},
	urldate = {2014-06-29},
}

@article{kobayashi_perplexity_nodate,
	title = {Perplexity on Reduced Corpora},
	url = {http://www.hayatokobayashi.com/paper/ACL2014_Kobayashi.pdf},
	author = {Kobayashi, Hayato},
	urldate = {2014-06-29},
}

@article{tian_efficient_nodate,
	title = {Efficient Logical Inference for Semantic Processing},
	url = {http://kmcs.nii.ac.jp/tianran/files/2014/05/manuscript1.pdf},
	author = {Tian, Ran and Miyao, Yusuke and Matsuzaki, Takuya},
	urldate = {2014-06-29},
}

@article{cao_online_nodate,
	title = {Online Learning in Tensor Space},
	url = {http://acl2014.org/acl2014/P14-1/pdf/P14-1063.pdf},
	author = {Cao, Yuan and Khudanpur, Sanjeev},
	urldate = {2014-06-29},
}
@article{srivastava_vector_nodate,
	title = {Vector space semantics with frequency-driven motifs},
	url = {http://www.ling.uni-potsdam.de/~koller/aclpub/P14-1/cdrom/pdf/P14-1060.pdf},
	author = {Srivastava, Shashank and Hovy, Eduard},
	urldate = {2014-06-29},
}

@article{bollegala_learning_nodate,
	title = {Learning to Predict Distributions of Words Across Domains},
	url = {http://cgi.csc.liv.ac.uk/~danushka/papers/ACL_2014.pdf},
	author = {Bollegala, Danushka and Weir, David and Carroll, John},
	urldate = {2014-06-29},
}

@article{anzaroot_learning_2014,
	title = {Learning Soft Linear Constraints with Application to Citation Field Extraction},
	url = {http://arxiv.org/abs/1403.1349},
	journaltitle = {{arXiv} preprint {arXiv}:1403.1349},
	author = {Anzaroot, Sam and Passos, Alexandre and Belanger, David and {McCallum}, Andrew},
	urldate = {2014-06-29},
	date = {2014},
}

@article{muller_predicting_nodate,
	title = {Predicting the relevance of distributional semantic similarity with contextual information},
	url = {http://acl2014.org/acl2014/P14-1/pdf/P14-1045.pdf},
	author = {Muller, Philippe and Fabre, Cécile and Adam, Clémentine},
	urldate = {2014-06-29},
}

@article{pilehvar_robust_2014,
	title = {A Robust Approach to Aligning Heterogeneous Lexical Resources},
	volume = {1},
	url = {http://wwwusers.di.uniroma1.it/~navigli/pubs/ACL_2014_Pilehvar_Navigli.pdf},
	pages = {c2},
	journaltitle = {A← A},
	author = {Pilehvar, Mohammad Taher and Navigli, Roberto},
	urldate = {2014-06-29},
	date = {2014},
}

@article{bansal_structured_nodate,
	title = {Structured Learning for Taxonomy Induction with Belief Propagation},
	url = {http://ttic.uchicago.edu/~mbansal/papers/acl14_structuredTaxonomy.pdf},
	author = {Bansal, Mohit and Burkett, David and de Melo, Gerard and Klein, Dan},
	urldate = {2014-06-29},
}

@inproceedings{goes_robust_2014,
	title = {Robust Stochastic Principal Component Analysis},
	url = {http://jmlr.org/proceedings/papers/v33/goes14.pdf},
	pages = {266--274},
	booktitle = {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics},
	author = {Goes, John and Zhang, Teng and Arora, Raman and Lerman, Gilad},
	urldate = {2014-06-12},
	date = {2014},
}

@incollection{arora_stochastic_2013,
	title = {Stochastic Optimization of {PCA} with Capped {MSG}},
	url = {http://papers.nips.cc/paper/5033-stochastic-optimization-of-pca-with-capped-msg.pdf},
	pages = {1815--1823},
	booktitle = {Advances in Neural Information Processing Systems 26},
	publisher = {Curran Associates, Inc.},
	author = {Arora, Raman and Cotter, Andy and Srebro, Nati},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	urldate = {2014-06-11},
	date = {2013},
}

@inproceedings{arora_learning_2012,
	title = {Learning topic models–going beyond svd},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6375276},
	pages = {1--10},
	booktitle = {Foundations of Computer Science ({FOCS}), 2012 {IEEE} 53rd Annual Symposium on},
	publisher = {{IEEE}},
	author = {Arora, Sanjeev and Ge, Rong and Moitra, Ankur},
	urldate = {2014-06-11},
	date = {2012},
}

@book{axler_linear_2004,
	location = {New York},
	edition = {2nd edition},
	title = {Linear Algebra Done Right},
	isbn = {9780387982588},
	abstract = {This text for a second course in linear algebra, aimed at math majors and graduates, adopts a novel approach by banishing determinants to the end of the book and focusing on understanding the structure of linear operators on vector spaces. The author has taken unusual care to motivate concepts and to simplify proofs. For example, the book presents - without having defined determinants - a clean proof that every linear operator on a finite-dimensional complex vector space has an eigenvalue. The book starts by discussing vector spaces, linear independence, span, basics, and dimension. Students are introduced to inner-product spaces in the first half of the book and shortly thereafter to the finite- dimensional spectral theorem. A variety of interesting exercises in each chapter helps students understand and manipulate the objects of linear algebra. This second edition features new chapters on diagonal matrices, on linear functionals and adjoints, and on the spectral theorem; some sections, such as those on self-adjoint and normal operators, have been entirely rewritten; and hundreds of minor improvements have been made throughout the text.},
	pagetotal = {252},
	publisher = {Springer},
	author = {Axler, Sheldon},
	date = {2004-04-06},
}

@incollection{naumann_spectral_2012,
	location = {Boca Raton},
	edition = {1 edition},
	title = {Spectral Graph Theory},
	isbn = {9781439827352},
	abstract = {Combinatorial Scientific Computing explores the latest research on creating algorithms and software tools to solve key combinatorial problems on large-scale high-performance computing architectures. It includes contributions from international researchers who are pioneers in designing software and applications for high-performance computing systems.  The book offers a state-of-the-art overview of the latest research, tool development, and applications. It focuses on load balancing and parallelization on high-performance computers, large-scale optimization, algorithmic differentiation of numerical simulation code, sparse matrix software tools, and combinatorial challenges and applications in large-scale social networks. The authors unify these seemingly disparate areas through a common set of abstractions and algorithms based on combinatorics, graphs, and hypergraphs.  Combinatorial algorithms have long played a crucial enabling role in scientific and engineering computations and their importance continues to grow with the demands of new applications and advanced architectures. By addressing current challenges in the field, this volume sets the stage for the accelerated development and deployment of fundamental enabling technologies in high-performance scientific computing.},
	booktitle = {Combinatorial Scientific Computing},
	publisher = {Chapman and Hall/{CRC}},
	author = {Naumann, Uwe and Schenk, Olaf and Daniel Spielman},
	date = {2012-01-25},
}

@inproceedings{cohen_spectral_2012,
	title = {Spectral learning of latent-variable {PCFGs}},
	url = {http://dl.acm.org/citation.cfm?id=2390556},
	pages = {223--231},
	booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1},
	publisher = {Association for Computational Linguistics},
	author = {Cohen, Shay B. and Stratos, Karl and Collins, Michael and Foster, Dean P. and Ungar, Lyle},
	urldate = {2014-05-20},
	date = {2012},
	keywords = {Raman},
}

@article{szegedy_intriguing_2013,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	journaltitle = {{arXiv} preprint {arXiv}:1312.6199},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	urldate = {2014-05-19},
	date = {2013},
}

@online{noauthor_citing_nodate,
	title = {Citing Data},
	url = {https://www.ldc.upenn.edu/data-management/citing},
	abstract = {The Linguistic Data Consortium is an international non-profit supporting language-related education, research and technology development by creating and sharing linguistic resources including data, tools and standards.},
	urldate = {2014-05-09},
	keywords = {Broadcast News, Discourse Modeling, Human Machine Communication, Information Retrieval, {LDC}, {LDC} Online, Language, Language Data, Language Identification, Language Teaching, Linguistic, Linguistic Data Consortium, Linguistic Research, Linguistic Resources, Machine Translation, Message Understanding, Parsing, Prosody, Research, Sense Disambiguation, Speaker Identification, Speech Recognition, Speech Synthesis, Text Databases, Voice Recognition},
}

@inproceedings{tseng_conditional_2005,
	title = {A conditional random field word segmenter for sighan bakeoff 2005},
	volume = {171},
	url = {http://acl.ldc.upenn.edu/I/I05/I05-3027.pdf},
	booktitle = {Proceedings of the Fourth {SIGHAN} Workshop on Chinese Language Processing},
	author = {Tseng, Huihsin and Chang, Pichuan and Andrew, Galen and Jurafsky, Daniel and Manning, Christopher},
	urldate = {2014-05-09},
	date = {2005},
}

@inproceedings{carbonell_discriminative_2014,
	title = {A Discriminative Graph-Based Parser for the Abstract Meaning Representation},
	url = {http://www.cs.cmu.edu/~nasmith/papers/flanigan+thomson+dyer+carbonell+smith.acl14.pdf},
	eventtitle = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	author = {Carbonell, Jaime and Smith, Noah A. and {Jeffrey Flanigan} and {Chris Dyer} and {Sam Thomson}},
	urldate = {2014-05-09},
	date = {2014-06},
}

@inproceedings{liang_structure_2008,
	title = {Structure compilation: trading structure for features},
	url = {http://dl.acm.org/citation.cfm?id=1390231},
	shorttitle = {Structure compilation},
	pages = {592--599},
	booktitle = {Proceedings of the 25th international conference on Machine learning},
	publisher = {{ACM}},
	author = {Liang, Percy and Daumé {III}, Hal and Klein, Dan},
	urldate = {2014-05-06},
	date = {2008},
}

@inproceedings{vedaldi_understanding_2014,
	title = {Understanding Objects in Detail with Fine-grained Attributes.},
	rights = {All rights reserved},
	url = {http://hal.archives-ouvertes.fr/hal-00981125/},
	eventtitle = {{CVPR}},
	booktitle = {Proc. {IEEE} Conf. on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Vedaldi, A. and Mahendran, S. and Tsogkas, S. and Maji, S. and Girshick, B. and Kannala, J. and Rahtu, E. and Kokkinos, I. and Blaschko, M. B. and Weiss, D. and Taskar, B. and Simonyan, K. and Saphra, N. and Mohamed, S.},
	date = {2014},
}

@article{nguyen_anchors_nodate,
	title = {Anchors Regularized: Adding Robustness and Extensibility to Scalable Topic-Modeling Algorithms},
	url = {http://www.cs.umd.edu/~ynhu/publications/acl2014_spectral.pdf},
	shorttitle = {Anchors Regularized},
	author = {Nguyen, Thang and Hu, Yuening and Boyd-Graber, Jordan},
	urldate = {2014-04-29},
}

@inproceedings{vogel_he_2012,
	title = {He said, she said: Gender in the {ACL} anthology},
	url = {http://dl.acm.org/citation.cfm?id=2390512},
	shorttitle = {He said, she said},
	pages = {33--41},
	booktitle = {Proceedings of the {ACL}-2012 Special Workshop on Rediscovering 50 Years of Discoveries},
	publisher = {Association for Computational Linguistics},
	author = {Vogel, Adam and Jurafsky, Dan},
	urldate = {2014-04-23},
	date = {2012},
}

@inproceedings{koller_dependency_2009,
	title = {Dependency trees and the strong generative capacity of {CCG}},
	url = {http://dl.acm.org/citation.cfm?id=1609118},
	pages = {460--468},
	booktitle = {Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Koller, Alexander and Kuhlmann, Marco},
	urldate = {2014-04-23},
	date = {2009},
}

@inproceedings{mikolov_distributed_2013,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
	eventtitle = {{NIPS}},
	pages = {3111--3119},
	booktitle = {Advances in Neural Information Processing Systems 26},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	urldate = {2014-04-23},
	date = {2013},
	keywords = {Raman},
}

@article{mcallester_pac-bayesian_2013,
	title = {A {PAC}-Bayesian Tutorial with A Dropout Bound},
	url = {http://arxiv.org/abs/1307.2118},
	journaltitle = {{arXiv} preprint {arXiv}:1307.2118},
	author = {{McAllester}, David},
	urldate = {2014-04-18},
	date = {2013},
}

@thesis{srivastava_improving_2013,
	title = {Improving neural networks with dropout},
	url = {http://www.cs.toronto.edu/~nitish/msc_thesis.pdf},
	institution = {University of Toronto},
	type = {phdthesis},
	author = {Srivastava, Nitish},
	urldate = {2014-04-18},
	date = {2013},
}

@report{jurgen_schmidhuber_deep_2014,
	title = {Deep Learning in Neural Networks: An Overview},
	abstract = {In recent years, deep neural networks (including recurrent ones
) have won numerous contests in
pattern recognition and machine learning. This historical survey compac
tly summarises relevant work,
much of it from the previous millennium. Shallow and deep learners are dis
tinguished by the depth
of their
credit assignment paths
, which are chains of possibly learnable, causal links between actions
and effects. I review deep supervised learning (also recapitulating the h
istory of backpropagation), un-
supervised learning, reinforcement learning \& evolutionary computatio
n, and indirect search for short
programs encoding deep and large networks.},
	institution = {{IDSIA}},
	author = {{Jürgen Schmidhuber}},
	date = {2014-03},
}

@inproceedings{chiang_parsing_2013,
	title = {Parsing graphs with hyperedge replacement grammars},
	url = {http://oldsite.aclweb.org/anthology-new/P/P13/P13-1091.pdf},
	booktitle = {Proceedings of the 51st Meeting of the {ACL}},
	author = {Chiang, David and Andreas, Jacob and Bauer, Daniel and Hermann, Karl Moritz and Jones, Bevan and Knight, Kevin},
	urldate = {2014-03-11},
	date = {2013},
	keywords = {graph parsing, graphs, hrg, parsing},
}

@article{fortunato_community_2010,
	title = {Community detection in graphs},
	volume = {486},
	issn = {0370-1573},
	url = {http://www.sciencedirect.com/science/article/pii/S0370157309002841},
	doi = {10.1016/j.physrep.2009.11.002},
	abstract = {The modern science of networks has brought significant advances to our understanding of complex systems. One of the most relevant features of graphs representing real systems is community structure, or clustering, i.e. the organization of vertices in clusters, with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters. Such clusters, or communities, can be considered as fairly independent compartments of a graph, playing a similar role like, e.g., the tissues or the organs in the human body. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. This problem is very hard and not yet satisfactorily solved, despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years. We will attempt a thorough exposition of the topic, from the definition of the main elements of the problem, to the presentation of most methods developed, with a special focus on techniques designed by statistical physicists, from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other, to the description of applications to real networks.},
	pages = {75--174},
	number = {3},
	journaltitle = {Physics Reports},
	shortjournal = {Physics Reports},
	author = {Fortunato, Santo},
	urldate = {2014-04-14},
	date = {2010-02},
	keywords = {Clusters, Graphs, Statistical physics},
}

@online{kuhlmann_generalized_nodate,
	title = {A Generalized View on Parsing and Translation},
	url = {http://www.ida.liu.se/~marku61/publications/koller2011generalized.html},
	author = {Kuhlmann, Marco},
	urldate = {2014-04-14},
	langid = {english},
}

@inproceedings{clark_building_2002,
	title = {Building deep dependency structures with a wide-coverage {CCG} parser},
	url = {http://dl.acm.org/citation.cfm?id=1073138},
	pages = {327--334},
	booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Stephen and Hockenmaier, Julia and Steedman, Mark},
	urldate = {2014-04-08},
	date = {2002},
}

@letter{martin_popel_any_nodate,
	title = {Any bitext corpora with semantic annotations out there?},
	abstract = {Anyone know if there are translations for the {PDT} segments with
tectogrammatical layer annotations, for instance?
{PDT} was never translated to English, but we have a translation of {PennTB} to Czech manually annotated on both sides on the tectogrammatical layer. It is called {PCEDT} http://ufal.mff.cuni.cz/pcedt2.0/en/index.html

You can also download much bigger English-Czech parallel treebank {CzEng} http://ufal.mff.cuni.cz/czeng/.

My colleagues (Roman Sudarikov under the supervision of Ondřej Bojar) are working on (at least partially automated) conversion of tecto trees to {AMR}. Our plan is to convert {CzEng} to (kind of) {AMR}.

Best,
Martin Popel},
	type = {E-mail},
	author = {{Martin Popel}},
}

@letter{adam_lopez_fwd:_nodate,
	title = {Fwd: Suggested unit for {MT} reading group: {AMR}/ graph semantics},
	abstract = {---------- Forwarded message ----------
From: Adam Lopez {\textless}alopez@cs.jhu.edu{\textgreater}
Date: Sat, Mar 29, 2014 at 8:46 {AM}
Subject: Suggested unit for {MT} reading group: {AMR}/ graph semantics
To: Philipp Koehn {\textless}pkoehn@inf.ed.ac.uk{\textgreater}


Hi Philipp --

I want to suggest that we devote several weeks of the {MT} reading group to {AMR} and related topics. The practical motivation: you, me, Adi, and Naomi (and Sanjeev?) are all going to the {AMR} workshop in Prague this summer, so we all should understand what's been done. It will be especially good for the students.

Some suggested readings:

-The {AMR} paper and {AMR} guidelines:
http://amr.isi.edu/a.pdf
https://github.com/amrisi/amr-guidelines/blob/master/amr.md

-Semantics-based {MT} with hyperedge replacement grammar
http://www.isi.edu/natural-language/people/sembmt-coling-12.pdf

-Parsing hyperedge replacement grammar
http://www.isi.edu/{\textasciitilde}chiang/papers/hrg-parsing.pdf

-The {SMATCH} paper
http://amr.isi.edu/smatch-13.pdf

Jeff Flanigan \& Sam Thomson from {CMU} have an upcoming {ACL} paper on parsing sentences to {AMR} (attached). Jeff is one of Chris Dyer's students; he'll be at the Prague workshop.

Steve Clark has an old paper on parsing to graph representations of semantics.
http://aclweb.org/anthology//P/P02/P02-1042.pdf

Finally, I had a long chat with Phil Williams in Edinburgh where he talked about his thesis -- he has come a very long way since I last saw him and it was really interesting. Unification grammars are closely related to {AMRs} (they can produce acyclic graph structures). So we could also review one of his papers or a thesis chapter.

{\textless}{AMR} Parsing.pdf{\textgreater}},
	type = {E-mail},
	author = {{Adam Lopez}},
}

@article{bengio_learning_2009,
	title = {Learning Deep Architectures for {AI}},
	volume = {2},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-machine-learning/MAL-006?result_Number=0&start=0&q=deep+learning&results_Page_Size=10&total_Results=2&rows=10},
	doi = {10.1561/2200000006},
	pages = {1--127},
	number = {1},
	journaltitle = {Foundations and Trends® in Machine Learning},
	author = {Bengio, Yoshua},
	urldate = {2014-04-14},
	date = {2009},
	langid = {english},
}

@inproceedings{das_exact_2012,
	title = {An exact dual decomposition algorithm for shallow semantic parsing with constraints},
	url = {http://dl.acm.org/citation.cfm?id=2387671},
	pages = {209--217},
	booktitle = {Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation},
	publisher = {Association for Computational Linguistics},
	author = {Das, Dipanjan and Martins, André {FT} and Smith, Noah A.},
	urldate = {2014-04-14},
	date = {2012},
}

@inproceedings{kuhlmann_mildly_2007,
	title = {Mildly context-sensitive dependency languages},
	volume = {45},
	url = {http://acl.ldc.upenn.edu/P/P07/P07-1021.pdf?origin=publication_detail},
	pages = {160},
	booktitle = {{ANNUAL} {MEETING}-{ASSOCIATION} {FOR} {COMPUTATIONAL} {LINGUISTICS}},
	author = {Kuhlmann, Marco and Möhl, Mathias},
	urldate = {2014-04-08},
	date = {2007},
}

@article{lopez_beyond_nodate,
	title = {Beyond bitext: Five open problems in machine translation},
	url = {http://hltcoe.jhu.edu/uploads/publications/papers/14700_slides.pdf},
	shorttitle = {Beyond bitext},
	author = {Lopez, Adam and Post, Matt},
	urldate = {2014-04-08},
}

@article{baker_singular_2005,
	title = {Singular value decomposition tutorial},
	url = {http://lsa-svd-application-for-analysis.googlecode.com/svn-history/r120/trunk/LSA/Other/LsaToRead/SVDTut.pdf},
	journaltitle = {The Ohio State University},
	author = {Baker, Kirk},
	urldate = {2014-02-28},
	date = {2005},
}

@article{anandkumar_tensor_2012,
	title = {Tensor decompositions for learning latent variable models},
	url = {http://arxiv.org/abs/1210.7559},
	abstract = {This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models---including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation---which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.},
	journaltitle = {{arXiv}:1210.7559 [cs, math, stat]},
	author = {Anandkumar, Anima and Ge, Rong and Hsu, Daniel and Kakade, Sham M. and Telgarsky, Matus},
	urldate = {2014-02-28},
	date = {2012-10-29},
	keywords = {775, Computer Science - Learning, Mathematics - Numerical Analysis, Raman, Statistics - Machine Learning, spectral},
}

@article{simkin_theory_2006,
	title = {Theory of aces: high score by skill or luck?},
	url = {http://arxiv.org/abs/physics/0607109},
	shorttitle = {Theory of aces},
	abstract = {We studied the distribution of World War I fighter pilots by the number of victories they were credited with, along with casualty reports. Using the maximum entropy method we obtained the underlying distribution of pilots by their skill. We find that the variance of this skill distribution is not very large, and that the top aces achieved their victory scores mostly by luck. For example, the ace of aces, Manfred von Richthofen, most likely had a skill in the top quarter of the active {WWI} German fighter pilots and was no more special than that. When combined with our recent study (cond-mat/0310049), showing that fame grows exponentially with victory scores, these results (derived from real data) show that both outstanding achievement records and resulting fame are mostly due to chance.},
	journaltitle = {{arXiv}:physics/0607109},
	author = {Simkin, M. V. and Roychowdhury, V. P.},
	urldate = {2014-04-04},
	date = {2006-07-12},
	note = {Journal of Mathematical Sociology, v32, pp 129-141, 2008},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Physics - Data Analysis, Statistics and Probability, Physics - Physics and Society},
}

@article{liang_learning_2013,
	title = {Learning dependency-based compositional semantics},
	volume = {39},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00127},
	pages = {389--446},
	number = {2},
	journaltitle = {Computational Linguistics},
	author = {Liang, Percy and Jordan, Michael I. and Klein, Dan},
	urldate = {2014-04-02},
	date = {2013},
	keywords = {semantics},
}

@inproceedings{schneider_framework_2014,
	title = {A framework for (under) specifying dependency syntax without overloading annotators},
	rights = {All rights reserved},
	url = {http://arxiv.org/abs/1306.2091},
	eventtitle = {Linguistic Annotation Workshop},
	booktitle = {Linguistic Annotation Workshop},
	author = {Schneider, Nathan and O'Connor, Brendan and Saphra, Naomi and Bamman, David and Faruqui, Manaal and Smith, Noah A. and Dyer, Chris and Baldridge, Jason},
	date = {2014},
}

@inproceedings{burkett_fast_2012,
	title = {Fast inference in phrase extraction models with belief propagation},
	url = {http://dl.acm.org/citation.cfm?id=2382035},
	pages = {29--38},
	booktitle = {Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Burkett, David and Klein, Dan},
	urldate = {2014-03-31},
	date = {2012},
}
@article{santos_semantics_1992,
	title = {Semantics and (machine) translation},
	url = {http://www.linguateca.pt/Diana/download/SantosAPL92.pdf},
	pages = {451--464},
	journaltitle = {Actas do {VIII} Encontro da Associação Portuguesa de Linguística},
	author = {Santos, Diana},
	urldate = {2014-03-31},
	date = {1992},
}

@article{balle_spectral_2013,
	title = {Spectral learning of weighted automata},
	url = {http://link.springer.com/article/10.1007/s10994-013-5416-x},
	pages = {1--31},
	journaltitle = {Machine Learning},
	author = {Balle, Borja and Carreras, Xavier and Luque, Franco M. and Quattoni, Ariadna},
	urldate = {2014-03-31},
	date = {2013},
}

@inproceedings{jones_semantics-based_2012,
	title = {Semantics-Based Machine Translation with Hyperedge Replacement Grammars.},
	url = {http://www.kde.cs.tut.ac.jp/~aono/pdf/COLING2012/PAPERS/pdf/PAPERS083.pdf},
	pages = {1359--1376},
	booktitle = {{COLING}},
	author = {Jones, Bevan and Andreas, Jacob and Bauer, Daniel and Hermann, Karl Moritz and Knight, Kevin},
	urldate = {2014-03-31},
	date = {2012},
	keywords = {468 project, graph parsing, hrg},
}

@inproceedings{cohen_tensor_2012,
	title = {Tensor Decomposition for Fast Parsing with Latent-Variable {PCFGs}.},
	url = {https://papers.nips.cc/paper/4622-tensor-decomposition-for-fast-parsing-with-latent-variable-pcfgs.pdf},
	pages = {2528--2536},
	booktitle = {{NIPS}},
	author = {Cohen, Shay B. and Collins, Michael},
	urldate = {2014-03-31},
	date = {2012},
}

@inproceedings{dreyer_latent-variable_2008,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Latent-variable Modeling of String Transductions with Finite-state Methods},
	url = {http://dl.acm.org/citation.cfm?id=1613715.1613856},
	series = {{EMNLP} '08},
	abstract = {String-to-string transduction is a central problem in computational linguistics and natural language processing. It occurs in tasks as diverse as name transliteration, spelling correction, pronunciation modeling and inflectional morphology. We present a conditional loglinear model for string-to-string transduction, which employs overlapping features over latent alignment sequences, and which learns latent classes and latent string pair regions from incomplete training data. We evaluate our approach on morphological tasks and demonstrate that latent variables can dramatically improve results, even when trained on small data sets. On the task of generating morphological forms, we outperform a baseline method reducing the error rate by up to 48\%. On a lemmatization task, we reduce the error rates in Wicentowski (2002) by 38--92\%.},
	pages = {1080--1089},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Dreyer, Markus and Smith, Jason R. and Eisner, Jason},
	urldate = {2014-03-28},
	date = {2008},
}

@article{galley_regularized_nodate,
	title = {Regularized Minimum Error Rate Training},
	url = {http://oldsite.aclweb.org/anthology-new/D/D13/D13-1201.pdf},
	author = {Galley, Michel and Quirk, Chris and Cherry, Colin and Toutanova, Kristina},
	urldate = {2014-03-26},
}

@inproceedings{rambow_simple_2010,
	title = {The simple truth about dependency and phrase structure representations: An opinion piece},
	url = {http://dl.acm.org/citation.cfm?id=1858048},
	shorttitle = {The simple truth about dependency and phrase structure representations},
	pages = {337--340},
	booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Rambow, Owen},
	urldate = {2014-03-12},
	date = {2010},
	keywords = {alopez},
}

@article{murphy_hidden_2002,
	title = {Hidden semi-markov models (hsmms)},
	url = {http://pdf.aminer.org/000/273/212/learning_with_segment_boundaries_for_hierarchical_hmms.pdf},
	journaltitle = {unpublished notes},
	author = {Murphy, Kevin P.},
	urldate = {2014-03-11},
	date = {2002},
}

@book{mattson_patterns_2004,
	title = {Patterns for parallel programming},
	url = {http://books.google.com/books?hl=en&lr=&id=LNcFvN5Z4RMC&oi=fnd&pg=PT4&dq=patterns+for+parallel+programming&ots=QFwkuteD-a&sig=YVcvgKik0agkGBq06QmPdqwfTN0},
	publisher = {Pearson Education},
	author = {Mattson, Timothy G. and Sanders, Beverly A. and Massingill, Berna L.},
	urldate = {2014-03-10},
	date = {2004},
}

@article{petersen_matrix_2008,
	title = {The matrix cookbook},
	url = {http://g.mouciel.com/etudes/cours/archives-web-cours/IFT6266/web/hebdomadaire/3/matrix_cookbook.pdf},
	pages = {7--15},
	journaltitle = {Technical University of Denmark},
	author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
	urldate = {2014-03-07},
	date = {2008},
}

@inproceedings{pauls_k-best_2009,
	title = {k-best A* parsing},
	url = {http://dl.acm.org/citation.cfm?id=1690281},
	pages = {958--966},
	booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}: Volume 2-Volume 2},
	publisher = {Association for Computational Linguistics},
	author = {Pauls, Adam and Klein, Dan},
	urldate = {2014-03-07},
	date = {2009},
	keywords = {765},
}

@unpublished{noauthor_presentation_nodate,
	title = {Presentation Notes on Spectral Methods},
	keywords = {775, spectral},
}

@book{olivier_chapelle_semi-supervised_2006,
	location = {Cambridge, {MA}},
	title = {Semi-Supervised Learning},
	url = {http://www.kyb.tuebingen.mpg.de/ssl-book},
	publisher = {{MIT} Press},
	author = {{Olivier Chapelle} and {Bernhard Schölkopf} and {Alexander Zien}},
	date = {2006},
}

@article{recht_beneath_2012,
	title = {Beneath the valley of the noncommutative arithmetic-geometric mean inequality: conjectures, case-studies, and consequences},
	url = {http://arxiv.org/abs/1202.4184},
	shorttitle = {Beneath the valley of the noncommutative arithmetic-geometric mean inequality},
	abstract = {Randomized algorithms that base iteration-level decisions on samples from some pool are ubiquitous in machine learning and optimization. Examples include stochastic gradient descent and randomized coordinate descent. This paper makes progress at theoretically evaluating the difference in performance between sampling with- and without-replacement in such algorithms. Focusing on least means squares optimization, we formulate a noncommutative arithmetic-geometric mean inequality that would prove that the expected convergence rate of without-replacement sampling is faster than that of with-replacement sampling. We demonstrate that this inequality holds for many classes of random matrices and for some pathological examples as well. We provide a deterministic worst-case bound on the gap between the discrepancy between the two sampling models, and explore some of the impediments to proving this inequality in full generality. We detail the consequences of this inequality for stochastic gradient descent and the randomized Kaczmarz algorithm for solving linear systems.},
	journaltitle = {{arXiv}:1202.4184 [math, stat]},
	author = {Recht, Benjamin and Re, Christopher},
	urldate = {2014-03-07},
	date = {2012-02-19},
	keywords = {Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{borga_unified_1997,
	title = {A unified approach to pca, pls, mlr and cca},
	url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2:288565},
	author = {Borga, Magnus and Landelius, Tomas and Knutsson, Hans},
	urldate = {2014-03-07},
	date = {1997},
	keywords = {775, Raman, spectral},
}

@incollection{bauer_intuitionistic_nodate,
	title = {Intuitionistic Mathematics and Realizability in the Physical World},
	booktitle = {A Computable Universe},
	publisher = {World Scientific},
	author = {Bauer, Andrej},
}

@book{comon_tree_2007,
	title = {Tree automata techniques and applications},
	url = {http://www.citeulike.org/group/14833/article/8971032},
	author = {Comon, Hubert and Dauchet, Max and Gilleron, Rémi and Löding, Christof and Jacquemard, Florent and Lugiez, Denis and Tison, Sophie and Tommasi, Marc},
	urldate = {2014-03-07},
	date = {2007},
	keywords = {771},
}

@online{bulatov_machine_2014,
	title = {Machine Learning, etc: Stochastic Gradient Methods 2014},
	url = {http://yaroslavvb.blogspot.com/2014/03/stochastic-gradient-methods-2014.html},
	shorttitle = {Machine Learning, etc},
	titleaddon = {Machine Learning, etc},
	author = {Bulatov, Yaroslav},
	urldate = {2014-03-06},
	date = {2014-03-05},
}

@article{simion_convex_nodate,
	title = {A Convex Alternative to {IBM} Model 2},
	url = {http://www1.cs.columbia.edu/~mcollins/papers/ibm2convex.pdf},
	author = {Simion, Andrei and Collins, Michael and Stein, Clifford},
	urldate = {2014-03-01},
	keywords = {765, machine translation},
}

@report{jan_hajic_final_nodate,
	title = {Final Report: Natural Language Generation in the Context of Machine Translation},
	author = {{Jan Hajicˇ} and {Martin Cˇmejrek} and {Bonnie Dorr} and {Yuan Ding} and {Jason Eisner} and {Dan Gildea} and {Terry Koo} and {Kristen Parton} and {Gerald Penn} and {Dragomir Radev} and {Owen Rambow}},
	keywords = {1},
}

@report{sproat_normalization_nodate,
	title = {Normalization of Non-Standard Words: {WS} '99 Final Report},
	url = {http://old-site.clsp.jhu.edu/ws99/},
	author = {Sproat, Richard and Black, Alan W. and Chen, Stanley and Kumar, Shankar and Ostendorf, Mari and Richards, Christopher},
	urldate = {2014-03-01},
}

@article{hsu_spectral_2008,
	title = {A Spectral Algorithm for Learning Hidden Markov Models},
	url = {http://arxiv.org/abs/0811.4413},
	abstract = {Hidden Markov Models ({HMMs}) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning {HMMs} from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the {HMM} parameters), there is an efficient and provably correct algorithm for learning {HMMs}. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying {HMM}. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.},
	journaltitle = {{arXiv}:0811.4413 [cs]},
	author = {Hsu, Daniel and Kakade, Sham M. and Zhang, Tong},
	urldate = {2014-02-28},
	date = {2008-11-26},
	note = {Journal of Computer and System Sciences, 78(5):1460-1480, 2012},
	keywords = {775, Computer Science - Artificial Intelligence, Computer Science - Learning, spectral},
}

@unpublished{cohen_spectral_nodate,
	title = {Spectral Learning Algorithms for Natural Language Processing},
	url = {http://www.cs.columbia.edu/~scohen/naacl13tutorial/},
	note = {{NAACL} 2013},
	author = {Cohen, Shay and Collins, Michael and Foster, Dean P. and Stratos, Karl and Ungar, Lyle H.},
	keywords = {775, spectral},
}

@unpublished{gordon_spectral_nodate,
	title = {Spectral Approaches to Learning Latent Variable Models},
	url = {https://www.cs.cmu.edu/~ggordon/spectral-learning/},
	author = {Gordon, Geoffrey J and Song, Le and Boots, Byron},
	keywords = {775, spectral},
}

@article{aho_syntax_1969,
	title = {Syntax directed translations and the pushdown assembler},
	volume = {3},
	url = {http://www.sciencedirect.com/science/article/pii/S0022000069800061},
	pages = {37--56},
	number = {1},
	journaltitle = {Journal of Computer and System Sciences},
	author = {Aho, Alfred V. and Ullman, Jeffrey D.},
	urldate = {2014-02-24},
	date = {1969},
	keywords = {771},
}

@inproceedings{lee_efficient_2006,
	title = {Efficient Structure Learning of Markov Networks using \$ L\_1 \$-Regularization},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_849.pdf},
	pages = {817--824},
	booktitle = {Advances in neural Information processing systems},
	author = {Lee, Su-In and Ganapathi, Varun and Koller, Daphne},
	urldate = {2014-02-24},
	date = {2006},
	keywords = {775, structured sparsity},
}

@article{wallach_conditional_2004,
	title = {Conditional random fields: An introduction},
	url = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1011&context=cis_reports},
	shorttitle = {Conditional random fields},
	pages = {22},
	journaltitle = {Technical Reports ({CIS})},
	author = {Wallach, Hanna M.},
	urldate = {2014-02-19},
	date = {2004},
}

@article{huang_optimized_nodate,
	title = {Optimized Event Storyline Generation based on Mixture-Event-Aspect Model},
	url = {http://www.newdesign.aclweb.org/anthology/D/D13/D13-1068.pdf},
	author = {Huang, Lifu},
	urldate = {2014-02-19},
	keywords = {fferraro},
}

@inproceedings{diao_unified_2013,
	title = {A Unified Model for Topics, Events and Users on Twitter},
	url = {http://oldsite.aclweb.org/anthology-new/D/D13/D13-1192.pdf},
	pages = {1869--1879},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	author = {Diao, Qiming and Jiang, Jing},
	urldate = {2014-02-19},
	date = {2013},
	keywords = {fferraro},
}

@inproceedings{tse_challenges_2012,
	title = {The challenges of parsing Chinese with combinatory categorial grammar},
	url = {http://dl.acm.org/citation.cfm?id=2382066},
	pages = {295--304},
	booktitle = {Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Tse, Daniel and Curran, James R.},
	urldate = {2014-02-19},
	date = {2012},
}

@inproceedings{das_unsupervised_2011,
	title = {Unsupervised part-of-speech tagging with bilingual graph-based projections},
	url = {http://dl.acm.org/citation.cfm?id=2002549},
	pages = {600--609},
	booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1},
	publisher = {Association for Computational Linguistics},
	author = {Das, Dipanjan and Petrov, Slav},
	urldate = {2014-02-19},
	date = {2011},
}

@inproceedings{burkett_joint_2010,
	title = {Joint parsing and alignment with weakly synchronized grammars},
	url = {http://dl.acm.org/citation.cfm?id=1858014},
	pages = {127--135},
	booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Burkett, David and Blitzer, John and Klein, Dan},
	urldate = {2014-02-19},
	date = {2010},
}

@inproceedings{snyder_unsupervised_2009,
	title = {Unsupervised multilingual grammar induction},
	url = {http://dl.acm.org/citation.cfm?id=1687890},
	pages = {73--81},
	booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}: Volume 1-Volume 1},
	publisher = {Association for Computational Linguistics},
	author = {Snyder, Benjamin and Naseem, Tahira and Barzilay, Regina},
	urldate = {2014-02-19},
	date = {2009},
}

@inproceedings{cohn_bayesian_2009,
	title = {A Bayesian model of syntax-directed tree to string grammar induction},
	url = {http://dl.acm.org/citation.cfm?id=1699557},
	pages = {352--361},
	booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1},
	publisher = {Association for Computational Linguistics},
	author = {Cohn, Trevor and Blunsom, Phil},
	urldate = {2014-02-19},
	date = {2009},
}

@inproceedings{yarowsky_inducing_2001,
	title = {Inducing multilingual text analysis tools via robust projection across aligned corpora},
	url = {http://dl.acm.org/citation.cfm?id=1072187},
	pages = {1--8},
	booktitle = {Proceedings of the first international conference on Human language technology research},
	publisher = {Association for Computational Linguistics},
	author = {Yarowsky, David and Ngai, Grace and Wicentowski, Richard},
	urldate = {2014-02-19},
	date = {2001},
}

@article{pitler_finding_2013,
	title = {Finding optimal 1-endpoint-crossing trees},
	url = {http://www.cis.upenn.edu/~epitler/papers/TACL2013.pdf},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	author = {Pitler, Emily and Kannan, Sampath and Marcus, Mitchell},
	urldate = {2014-02-19},
	date = {2013},
}

@article{kuhlmann_mildly_2013,
	title = {Mildly non-projective dependency grammar},
	volume = {39},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00125},
	pages = {355--387},
	number = {2},
	journaltitle = {Computational Linguistics},
	author = {Kuhlmann, Marco},
	urldate = {2014-02-19},
	date = {2013},
}

@inproceedings{wellington_empirical_2006,
	title = {Empirical lower bounds on the complexity of translational equivalence},
	url = {http://dl.acm.org/citation.cfm?id=1220298},
	pages = {977--984},
	booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Wellington, Benjamin and Waxmonsky, Sonjia and Melamed, I. Dan},
	urldate = {2014-02-19},
	date = {2006},
	keywords = {2},
}

@article{hwa_bootstrapping_2005,
	title = {Bootstrapping parsers via syntactic projection across parallel texts},
	volume = {11},
	url = {http://journals.cambridge.org/production/action/cjoGetFulltext?fulltextid=338344},
	pages = {311--325},
	number = {3},
	journaltitle = {Natural language engineering},
	author = {Hwa, Rebecca and Resnik, Philip and Weinberg, Amy and Cabezas, Clara and Kolak, Okan},
	urldate = {2014-02-19},
	date = {2005},
	keywords = {2},
}
@inproceedings{fox_phrasal_2002,
	title = {Phrasal cohesion and statistical machine translation},
	url = {http://dl.acm.org/citation.cfm?id=1118732},
	pages = {304--3111},
	booktitle = {Proceedings of the {ACL}-02 conference on Empirical methods in natural language processing-Volume 10},
	publisher = {Association for Computational Linguistics},
	author = {Fox, Heidi J.},
	urldate = {2014-02-19},
	date = {2002},
	keywords = {1},
}

@book{cai_smatch:_2012,
	title = {Smatch: an evaluation metric for semantic feature structures},
	url = {http://www.newdesign.aclweb.org/anthology-new/P/P13/P13-2131.pdf},
	shorttitle = {Smatch},
	publisher = {submitted},
	author = {Cai, Shu and Knight, Kevin},
	urldate = {2014-02-19},
	date = {2012},
}

@inproceedings{banarescu_abstract_2013,
	title = {Abstract meaning representation for sembanking},
	pages = {178--186},
	booktitle = {Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse},
	author = {Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
	date = {2013},
}

@inproceedings{schmidt_convex_2010,
	title = {Convex structure learning in log-linear models: Beyond pairwise potentials},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_SchmidtM10.pdf},
	shorttitle = {Convex structure learning in log-linear models},
	pages = {709--716},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	author = {Schmidt, Mark W. and Murphy, Kevin P.},
	urldate = {2014-02-17},
	date = {2010},
	keywords = {775, structured sparsity},
}

@article{goodman_semiring_1999,
	title = {Semiring parsing},
	volume = {25},
	url = {http://dl.acm.org/citation.cfm?id=973230},
	pages = {573--605},
	number = {4},
	journaltitle = {Computational Linguistics},
	author = {Goodman, Joshua},
	urldate = {2014-02-07},
	date = {1999},
	keywords = {771},
}

@inproceedings{liang_online_2009,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Online {EM} for Unsupervised Models},
	isbn = {978-1-932432-41-1},
	url = {http://dl.acm.org/citation.cfm?id=1620754.1620843},
	series = {{NAACL} '09},
	abstract = {The (batch) {EM} algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence. In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch {EM}. We support these findings on four unsupervised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment.},
	pages = {611--619},
	booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Liang, Percy and Klein, Dan},
	urldate = {2014-02-05},
	date = {2009},
	keywords = {765},
}

@book{szeliski_computer_2011,
	location = {London; New York},
	title = {Computer vision algorithms and applications},
	isbn = {9781848829343 1848829345 9781848829350  1848829353},
	url = {http://dx.doi.org/10.1007/978-1-84882-935-0},
	publisher = {Springer},
	author = {Szeliski, Richard},
	urldate = {2013-10-16},
	date = {2011},
}

@inproceedings{han_lexical_2011,
	title = {Lexical Normalisation of Short Text Messages: Makn Sens a\# twitter.},
	url = {http://ww2.cs.mu.oz.au/~hanb/acl2011-normalisation-slides.pdf},
	shorttitle = {Lexical Normalisation of Short Text Messages},
	pages = {368--378},
	booktitle = {{ACL}},
	author = {Han, Bo and Baldwin, Timothy},
	urldate = {2014-01-13},
	date = {2011},
}

@book{settles_active_2012,
	location = {San Rafael, Calif. (1537 Fourth Street, San Rafael, {CA}  94901 {USA})},
	title = {Active learning},
	isbn = {9781608457267  1608457265},
	url = {http://dx.doi.org/10.2200/S00429ED1V01Y201207AIM018},
	abstract = {The key idea behind active learning is that a machine learning algorithm can perform better with less training if it is allowed to choose the data from which it learns. An active learner may pose "queries," usually in the form of unlabeled data instances to be labeled by an "oracle" (e.g., a human annotator) that already understands the nature of the problem. This sort of approach is well-motivated in many modern machine learning and data mining applications, where unlabeled data may be abundant or easy to come by, but training labels are difficult, time-consuming, or expensive to obtain. This book is a general introduction to active learning. It outlines several scenarios in which queries might be formulated, and details many query selection algorithms which have been organized into four broad categories, or "query selection frameworks."We also touch on some of the theoretical foundations of active learning, and conclude with an overview of the strengths and weaknesses of these approaches in practice, including a summary of ongoing work to address these open challenges and opportunities.},
	publisher = {Morgan \& Claypool},
	author = {Settles, Burr},
	urldate = {2013-10-16},
	date = {2012},
	keywords = {765, active learning, machine learning},
}

@book{sra_optimization_2012,
	location = {Cambridge, Mass.},
	title = {Optimization for machine learning},
	isbn = {9780262298773  0262298775},
	url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=399078},
	publisher = {{MIT} Press},
	author = {Sra, Suvrit and Nowozin, Sebastian and Wright, Stephen J},
	urldate = {2014-02-03},
	date = {2012},
}

@book{axler_linear_1997,
	edition = {2},
	title = {Linear Algebra Done Right},
	author = {Axler, Sheldon},
	urldate = {2014-02-03},
	date = {1997},
}

@book{rice_mathematical_2007,
	title = {Mathematical statistics and data analysis},
	url = {http://books.google.com/books?hl=en&lr=&id=EKA-yeX2GVgC&oi=fnd&pg=PR4&dq=%22written+permission+of+the%22+%22e-mail+to%22+%222+3+4+5+6+7+10+09+08+07%22+%22The+Multiplication%22+%22a+request+online%22+%22more+information+about+our+products,+contact+us%22+&ots=YfxoAXnDG3&sig=8Mjxp3Q2ZGt0Z_UFhU-SiHhRB-0},
	publisher = {Cengage Learning},
	author = {Rice, John A.},
	urldate = {2014-02-03},
	date = {2007},
}

@book{stewart_calculus:_2012,
	location = {Belmont, Cal.},
	title = {Calculus: early transcendentals},
	isbn = {0538497904  9780538497909  0840058853  9780840058850  0538498714  9780538498715  0538498870 9780538498876 0840048254  9780840048257},
	shorttitle = {Calculus},
	publisher = {Brooks/Cole, Cengage Learning},
	author = {Stewart, James},
	date = {2012},
}

@book{strang_introduction_2003,
	location = {Wellesly, {MA}},
	title = {Introduction to linear algebra},
	isbn = {0961408898 9780961408893},
	publisher = {Wellesley-Cambridge Press},
	author = {Strang, Gilbert},
	date = {2003},
}

@book{bertsekas_introduction_2002,
	location = {Belmont, Mass},
	title = {Introduction to probability},
	isbn = {188652940X  9781886529403},
	publisher = {Athena Scientific},
	author = {Bertsekas, Dimitri P and Tsitsiklis, John N},
	date = {2002},
}

@book{zill_first_2009,
	location = {Belmont, {CA}},
	title = {A First course in differential equations with modeling applications},
	isbn = {9780495108245 0495108243},
	publisher = {Brooks/Cole, Cengage Learning},
	author = {Zill, Dennis G},
	date = {2009},
}

@book{luenberger_linear_2008,
	location = {New York},
	title = {Linear and nonlinear programming},
	isbn = {9780387745039 0387745033 9780387745022  0387745025},
	url = {http://site.ebrary.com/id/10531276},
	publisher = {Springer},
	author = {Luenberger, David G and , Yinyu, Ye},
	urldate = {2014-02-03},
	date = {2008},
}

@book{bishop_pattern_2009,
	location = {New York, {NY}},
	title = {Pattern recognition and machine learning},
	isbn = {0387310738  9780387310732},
	publisher = {Springer},
	author = {Bishop, Christopher M},
	date = {2009},
}

@book{rasmussen_gaussian_2006,
	location = {Cambridge, Mass.},
	title = {Gaussian processes for machine learning},
	isbn = {026218253X 9780262182539},
	abstract = {"Gaussian processes ({GPs}) provide a principled, practical, probabilistic approach to learning in kernel machines. {GPs} have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of {GPs} in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics."--Jacket.},
	publisher = {{MIT} Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I},
	date = {2006},
}

@book{boyd_convex_2004,
	location = {Cambridge, {UK}; New York},
	title = {Convex optimization},
	isbn = {0521833787 9780521833783},
	abstract = {From the publisher. Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics.},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen P and Vandenberghe, Lieven},
	date = {2004},
}

@book{roark_computational_2007,
	location = {Oxford; New York},
	title = {Computational approaches to morphology and syntax},
	isbn = {9780191534515  019153451X},
	url = {http://site.ebrary.com/id/10194773},
	abstract = {This book provides a critical and practical guide to computational techniques for handling morphological and syntactic phenomena, showing how these techniques have been used and modified in practice. - ;The book will appeal to scholars and advanced students of morphology, syntax, computational linguistics and natural language processing ({NLP}). It provides a critical and practical guide to computational techniques for handling morphological and syntactic phenomena, showing how these techniques have been used and modified in practice. The authors discuss the nature and uses of syntactic parsers.},
	publisher = {Oxford University Press},
	author = {Roark, Brian and Sproat, Richard William},
	urldate = {2014-02-03},
	date = {2007},
}

@book{grune_parsing_2008,
	title = {Parsing Techniques 2: A Practical Guide},
	url = {http://books.google.com/books?hl=en&lr=&id=05xA_d5dSwAC&oi=fnd&pg=PR5&dq=%22thinking+should+pervade+this+book.+Technical+terms+pertaining+to%22+%22the+positive+side,+and+that+is+the+main+purpose+of+this+enterprise,+we+hope%22+%22(Prentice-Hall,+1985)+or+Programming+from+first+principles+by+Richard%22+&ots=3NztaDmaOb&sig=lbSrBZ6jURRtjm5u6ybPm7IrBiI},
	shorttitle = {Parsing Techniques 2},
	publisher = {Springer},
	author = {Grune, Dick and Jacobs, Ceriel {JH}},
	urldate = {2014-02-03},
	date = {2008},
}

@inproceedings{goldwater_fully_2007,
	title = {A fully Bayesian approach to unsupervised part-of-speech tagging},
	volume = {45},
	url = {http://acl.ldc.upenn.edu/P/P07/P07-1094.pdf},
	pages = {744},
	booktitle = {Annual meeting-association for computational linguistics},
	author = {Goldwater, Sharon and Griffiths, Tom},
	urldate = {2014-02-03},
	date = {2007},
}

@book{box_bayesian_1973,
	location = {Reading, Mass.},
	title = {Bayesian inference in statistical analysis},
	isbn = {0201006227 9780201006223},
	publisher = {Addison-Wesley Pub. Co.},
	author = {Box, George E. P and Tiao, George C},
	date = {1973},
}

@book{mclachlan_em_2008,
	location = {Hoboken, N.J.},
	title = {The {EM} algorithm and extensions},
	isbn = {9780471201700  0471201707},
	abstract = {"Complete with updates that capture developments from the past decade, The {EM} Algorithm and Extensions, Second Edition successfully provides a basic understanding of the {EM} algorithm by describing its inception, implementation, and applicability in numerous statistical contexts. In conjunction with the fundamentals of the topic, the authors discuss convergence issues and computation of standard errors, and, in addition, unveil many parallels and connections between the {EM} algorithm and Markov chain Monte Carlo algorithms. Thorough discussions on the complexities and drawbacks that arise from the basic {EM} algorithm, such as slow convergence and lack of an in-built procedure to compute the covariance matrix of parameter estimates, are also presented." "The {EM} Algorithm and Extensions, Second Edition serves as an excellent text for graduate-level statistics students and is also a comprehensive resource for theoreticians, practitioners, and researchers in the social and physical sciences who would like to extend their knowledge of the {EM} algorithm."--{BOOK} {JACKET}.},
	publisher = {Wiley-Interscience},
	author = {{McLachlan}, Geoffrey J and , T, Krishnan},
	date = {2008},
}

@book{nocedal_numerical_2006,
	location = {New York},
	title = {Numerical optimization},
	isbn = {9780387400655 0387400656 0387303030  9780387303031 0387987932 9780387987934},
	url = {http://site.ebrary.com/id/10228772},
	abstract = {'Numerical Optimization' presents a comprehensive description of the effective methods in continuous optimization. The book includes chapters on nonlinear interior methods \& derivative-free methods for optimization. It is useful for graduate students, researchers and practitioners.},
	publisher = {Springer},
	author = {Nocedal, Jorge and Wright, Stephen J},
	urldate = {2014-02-03},
	date = {2006},
}

@book{kleinberg_algorithm_2006,
	location = {Boston},
	title = {Algorithm design},
	isbn = {0321295358  9780321295354},
	abstract = {"Algorithm Design takes a fresh approach to the algorithms course, introducing algorithmic ideas through the real-world problems that motivate them. In a clear, direct style, Jon Kleinberg and Eva Tardos teach students to analyze and define problems for themselves, and from this to recognize which design principles are appropriate for a given situation. The text encourages a greater understanding of the algorithm design process and an appreciation of the role of algorithms in the broader field of computer science."--{BOOK} {JACKET}.},
	publisher = {Pearson/Addison-Wesley},
	author = {Kleinberg, Jon and Tardos, Éva},
	date = {2006},
}

@book{bentley_programming_2000,
	location = {Reading, Mass.},
	title = {Programming pearls},
	isbn = {0201657880 9780201657883},
	publisher = {Addison-Wesley},
	author = {Bentley, Jon},
	date = {2000},
}

@book{dayan_theoretical_2001,
	location = {Cambridge, Mass.},
	title = {Theoretical neuroscience: computational and mathematical modeling of neural systems},
	isbn = {0262041995  9780262041997  0262541858  9780262541855},
	shorttitle = {Theoretical neuroscience},
	abstract = {{DSU} Title {III} 2007-2012.},
	publisher = {Massachusetts Institute of Technology Press},
	author = {Dayan, Peter and Abbott, L. F},
	date = {2001},
}

@book{grewal_kalman_2008,
	location = {Hoboken},
	title = {Kalman filtering theory and practice using {MATLAB}},
	isbn = {9780470173664  0470173661  9780470377819  047037781X},
	publisher = {John Wiley},
	author = {Grewal, Mohinder S and Andrews, Angus P},
	date = {2008},
}

@book{sipser_introduction_2013,
	location = {Boston, {MA}},
	title = {Introduction to the theory of computation},
	isbn = {9781133187790 113318779X},
	publisher = {Cengage Learning},
	author = {Sipser, Michael},
	date = {2013},
}

@book{mohri_foundations_2012,
	location = {Cambridge, {MA}},
	title = {Foundations of machine learning},
	isbn = {9780262305662  0262305666},
	url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=478737},
	publisher = {{MIT} Press},
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	urldate = {2014-02-03},
	date = {2012},
}

@book{russell_mining_2011,
	title = {Mining the social web: Analyzing data from Facebook, Twitter, {LinkedIn}, and other social media sites},
	url = {http://books.google.com/books?hl=en&lr=&id=S5TZeDRYWdMC&oi=fnd&pg=PR9&dq=%22Similarity+Metrics+for%22+%22Detection+and%22+%22Average,+Do+%23JustinBieber+or+%23TeaParty+Tweets+Have%22+%22Greedy+Approach+to%22+%22Twitter:+The+Tweet,+the+Whole+Tweet,+and+Nothing+but+the+Tweet+.+.+.+.+.+.+.+.+.+.+.+.%22+&ots=_U-8laIHRX&sig=VMGLD3aaHqn2u_S7YA2wIm6HBig},
	shorttitle = {Mining the social web},
	publisher = {O'Reilly},
	author = {Russell, Matthew A.},
	urldate = {2014-02-03},
	date = {2011},
}

@book{sole_sabater_experimental_2007,
	location = {New York},
	title = {Experimental approaches to phonology},
	isbn = {9780199296675  0199296677  9780199296828  0199296820},
	publisher = {Oxford University Press},
	author = {Solé Sabater, Maria Josep and Ohala, Manjari and Beddor, Patrice Speeter},
	date = {2007},
}

@book{droste_handbook_2009,
	location = {Berlin},
	title = {Handbook of weighted automata},
	isbn = {9783642014925  3642014925},
	url = {http://site.ebrary.com/id/10332893},
	publisher = {Springer},
	author = {Droste, Manfred and Kuich, Werner and Vogler, Heiko},
	urldate = {2014-02-03},
	date = {2009},
}

@book{getoor_introduction_2007,
	location = {Cambridge, Mass.},
	title = {Introduction to statistical relational learning},
	isbn = {9780262072885 0262072882},
	publisher = {{MIT} Press},
	author = {Getoor, Lise and Taskar, Ben},
	date = {2007},
}

@book{ross_introduction_2007,
	location = {Amsterdam; Boston},
	title = {Introduction to probability models},
	isbn = {9780080467825  0080467822  9780125980623  0125980620  9780123736352  0123736358},
	url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=185760},
	publisher = {Academic Press},
	author = {Ross, Sheldon M},
	urldate = {2014-02-03},
	date = {2007},
}

@book{crochemore_jewels_2003,
	title = {Jewels of stringology: text algorithms},
	url = {http://books.google.com/books?hl=en&lr=&id=9NdohJXtIyYC&oi=fnd&pg=PR5&dq=%22get+information+from+the+files+and+transform+them+through+different+existing%22+%22pattern+can%22+%22problem,+pattern-matching+problem+and,+more+generally,+the+problem%22+%22%7B+action2%22+%22instance,+the+following+awk+program+prints+the+number+of+lines+of+the+input%22+&ots=lmc94xR-K8&sig=0eb7kfWwONqwvvLGqPherVrCYIo},
	shorttitle = {Jewels of stringology},
	publisher = {World Scientific},
	author = {Crochemore, Maxime and Rytter, Wojciech},
	urldate = {2014-02-03},
	date = {2003},
}

@book{graham_concrete_1994,
	location = {Reading, {MA} [etc.]},
	title = {Concrete mathematics: a foundation for computer science},
	isbn = {0201558025 9780201558029},
	shorttitle = {Concrete mathematics},
	publisher = {Addison-Wesley},
	author = {Graham, Ronald L and Knuth, Donald Ervin and Patashnik, Oren},
	date = {1994},
}

@book{colley_vector_2012,
	location = {Boston},
	title = {Vector calculus},
	isbn = {9780321780652 0321780655},
	publisher = {Pearson},
	author = {Colley, Susan Jane},
	date = {2012},
}

@book{smith_digital_2003,
	location = {Amsterdam [etc.]},
	title = {Digital signal processing: a practical guide for engineers and scientists},
	isbn = {075067444X 9780750674447},
	shorttitle = {Digital signal processing},
	publisher = {Newnes},
	author = {Smith, Steven W},
	date = {2003},
}

@book{gelman_data_2007,
	title = {Data analysis using regression and multilevel/hierarchical models},
	url = {http://books.google.com/books?hl=en&lr=&id=c9xLKzZWoZ4C&oi=fnd&pg=PR17&dq=%22Michael+Alvarez,+California+Institute+of%22+%22Public+A%EF%AC%80airs+at+Columbia+University.+She+has+coauthored+articles+that+have%22+%22public+health.+The+series+serves+a+mix+of+students+and+researchers+in+the+social%22+&ots=b9V4O_Smnf&sig=HlfCWu0IdsdIu91hIWBBPbLfrNA},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew},
	urldate = {2014-02-03},
	date = {2007},
}

@book{carnie_syntax:_2007,
	location = {Malden, {MA}},
	title = {Syntax: a generative introduction},
	isbn = {1405133848  9781405133845},
	shorttitle = {Syntax},
	publisher = {Blackwell Pub.},
	author = {Carnie, Andrew},
	date = {2007},
}

@article{bach_structured_2012,
	title = {Structured sparsity through convex optimization},
	volume = {27},
	url = {http://projecteuclid.org/euclid.ss/1356098550},
	pages = {450--468},
	number = {4},
	journaltitle = {Statistical Science},
	author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
	urldate = {2014-01-31},
	date = {2012},
	keywords = {775, structured sparsity},
}

@inproceedings{zhu_unified_2007,
	title = {A unified tagging approach to text normalization},
	volume = {45},
	url = {http://acl.ldc.upenn.edu/P/P07/P07-1087.pdf},
	pages = {688},
	booktitle = {{ANNUAL} {MEETING}-{ASSOCIATION} {FOR} {COMPUTATIONAL} {LINGUISTICS}},
	author = {Zhu, Conghui and Tang, Jie and Li, Hang and Ng, Hwee Tou and Zhao, Tiejun},
	urldate = {2014-01-25},
	date = {2007},
}

@inproceedings{liu_recognizing_2011,
	title = {Recognizing Named Entities in Tweets.},
	url = {https://www.aclweb.org/anthology-new/P/P11/P11-1037.pdf},
	pages = {359--367},
	booktitle = {{ACL}},
	author = {Liu, Xiaohua and Zhang, Shaodian and Wei, Furu and Zhou, Ming},
	urldate = {2014-01-25},
	date = {2011},
}

@inproceedings{xue_normalizing_2011,
	title = {Normalizing Microtext.},
	url = {http://www.aaai.org/ocs/index.php/WS/AAAIW11/paper/download/3987/4335},
	booktitle = {Analyzing Microtext},
	author = {Xue, Zhenzhen and Yin, Dawei and Davison, Brian D. and Davison, B. D.},
	urldate = {2014-01-25},
	date = {2011},
}

@inproceedings{kobus_normalizing_2008,
	title = {Normalizing {SMS}: are two metaphors better than one?},
	url = {http://dl.acm.org/citation.cfm?id=1599137},
	shorttitle = {Normalizing {SMS}},
	pages = {441--448},
	booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1},
	author = {Kobus, Catherine and Yvon, Fran\{{\textbackslash}textbackslash\}ccois and Damnati, Géraldine},
	urldate = {2014-01-25},
	date = {2008},
}
@article{oliva_sms_2013,
	title = {A {SMS} normalization system integrating multiple grammatical resources.},
	volume = {19},
	url = {http://journals.cambridge.org/production/action/cjoGetFulltext?fulltextid=8606535},
	pages = {121--141},
	number = {1},
	journaltitle = {Natural Language Engineering},
	author = {Oliva, Jesus and Serrano, Jose Ignacio and del Castillo, M. Dolores and Iglesias, Ángel},
	urldate = {2014-01-25},
	date = {2013},
}

@inproceedings{cook_unsupervised_2009,
	location = {Stroudsburg, {PA}, {USA}},
	title = {An Unsupervised Model for Text Message Normalization},
	isbn = {978-1-932432-36-7},
	url = {http://dl.acm.org/citation.cfm?id=1642011.1642021},
	series = {{CALC} '09},
	abstract = {Cell phone text messaging users express themselves briefly and colloquially using a variety of creative forms. We analyze a sample of creative, non-standard text message word forms to determine frequent word formation processes in texting language. Drawing on these observations, we construct an unsupervised noisy-channel model for text message normalization. On a test set of 303 text message forms that differ from their standard form, our model achieves 59\% accuracy, which is on par with the best supervised results reported on this dataset.},
	pages = {71--78},
	booktitle = {Proceedings of the Workshop on Computational Approaches to Linguistic Creativity},
	publisher = {Association for Computational Linguistics},
	author = {Cook, Paul and Stevenson, Suzanne},
	urldate = {2014-01-25},
	date = {2009},
}

@inproceedings{li_first-_2009,
	location = {Stroudsburg, {PA}, {USA}},
	title = {First- and Second-order Expectation Semirings with Applications to Minimum-risk Training on Translation Forests},
	isbn = {978-1-932432-59-6},
	url = {http://dl.acm.org/citation.cfm?id=1699510.1699517},
	series = {{EMNLP} '09},
	abstract = {Many statistical translation models can be regarded as weighted logical deduction. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs). We then introduce a novel second-order expectation semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy). This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk. We use these semirings in an open-source machine translation toolkit, Joshua, enabling minimum-risk training for a benefit of up to 1.0 bleu point.},
	pages = {40--51},
	booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1},
	publisher = {Association for Computational Linguistics},
	author = {Li, Zhifei and Eisner, Jason},
	urldate = {2014-01-23},
	date = {2009},
	keywords = {771},
}

@inproceedings{huang_improving_2012,
	title = {Improving word representations via global context and multiple word prototypes},
	url = {http://dl.acm.org/citation.cfm?id=2390645},
	pages = {873--882},
	booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1},
	author = {Huang, Eric H. and Socher, Richard and Manning, Christopher D. and Ng, Andrew Y.},
	urldate = {2013-12-19},
	date = {2012},
}

@inproceedings{turian_word_2010,
	title = {Word representations: a simple and general method for semi-supervised learning},
	url = {http://dl.acm.org/citation.cfm?id=1858721},
	shorttitle = {Word representations},
	pages = {384--394},
	booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
	author = {Turian, Joseph and Ratinov, Lev and Bengio, Yoshua},
	urldate = {2013-12-19},
	date = {2010},
}

@inproceedings{mnih_learning_2013,
	title = {Learning word embeddings efficiently with noise-contrastive estimation},
	url = {http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {2265--2273},
	author = {Mnih, Andriy and Kavukcuoglu, Koray},
	urldate = {2013-12-17},
	date = {2013},
}

@inproceedings{eisner_parameter_2002,
	title = {Parameter estimation for probabilistic finite-state transducers},
	url = {http://dl.acm.org/citation.cfm?id=1073085},
	pages = {1--8},
	booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
	author = {Eisner, Jason},
	urldate = {2013-11-06},
	date = {2002},
	keywords = {771, {AT}-{AT}, alopez, edit distance, fst, toread 3, transducer},
}

@online{j2kun_how_nodate,
	title = {How to Conquer Tensorphobia},
	url = {http://jeremykun.com/2014/01/17/how-to-conquer-tensorphobia/},
	abstract = {A professor at Stanford once said, If you really want to impress your friends and confound your enemies, you can invoke tensor products... People run in terror from the \$latex {\textbackslash}otimes\$ symbol. He w...},
	titleaddon = {Math ∩ Programming},
	author = {{j2kun}},
	urldate = {2014-01-21},
}

@article{chen_expressive_2013,
	title = {The Expressive Power of Word Embeddings},
	url = {http://arxiv.org/abs/1301.3226},
	abstract = {We seek to better understand the difference in quality of the several publicly released embeddings. We propose several tasks that help to distinguish the characteristics of different embeddings. Our evaluation of sentiment polarity and synonym/antonym relations shows that embeddings are able to capture surprisingly nuanced semantics even in the absence of sentence structure. Moreover, benchmarking the embeddings shows great variance in quality and characteristics of the semantics captured by the tested embeddings. Finally, we show the impact of varying the number of dimensions and the resolution of each dimension on the effective useful features captured by the embedding space. Our contributions highlight the importance of embeddings for {NLP} tasks and the effect of their quality on the final results.},
	journaltitle = {{arXiv}:1301.3226 [cs, stat]},
	author = {Chen, Yanqing and Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	urldate = {2014-01-21},
	date = {2013-01-14},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning},
}

@online{j2kun_probability_nodate,
	title = {Probability Theory — A Primer},
	url = {http://jeremykun.com/2013/01/04/probability-theory-a-primer/},
	abstract = {It is a wonder that we have yet to officially write about probability theory on this blog. Probability theory underlies a huge portion of artificial intelligence, machine learning, and statistics, ...},
	titleaddon = {Math ∩ Programming},
	author = {{j2kun}},
	urldate = {2014-01-17},
}

@inproceedings{pereira_inside-outside_1992,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Inside-outside Reestimation from Partially Bracketed Corpora},
	url = {http://dx.doi.org/10.3115/981967.981984},
	doi = {10.3115/981967.981984},
	series = {{ACL} '92},
	abstract = {The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modeling of hierarchical structure than the original one. In particular, over 90\% test set bracketing accuracy was achieved for grammars inferred by our algorithm from a training set of handparsed part-of-speech strings for sentences in the Air Travel Information System spoken language corpus. Finally, the new algorithm has better time complexity than the original one when sufficient bracketing is provided.},
	pages = {128--135},
	booktitle = {Proceedings of the 30th Annual Meeting on Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Pereira, Fernando and Schabes, Yves},
	urldate = {2014-01-16},
	date = {1992},
}

@inproceedings{kaufmann_syntactic_2010,
	title = {Syntactic normalization of Twitter messages},
	url = {http://cs.uccs.edu/~jkalita/work/reu/REUFinalPapers2010/Kaufmann.pdf},
	booktitle = {International conference on natural language processing, Kharagpur, India},
	author = {Kaufmann, Max and Kalita, Jugal},
	urldate = {2014-01-13},
	date = {2010},
}

@inproceedings{aw_phrase-based_2006,
	title = {A phrase-based statistical model for {SMS} text normalization},
	url = {http://dl.acm.org/citation.cfm?id=1273078},
	pages = {33--40},
	booktitle = {Proceedings of the {COLING}/{ACL} on Main conference poster sessions},
	author = {Aw, {AiTi} and Zhang, Min and Xiao, Juan and Su, Jian},
	urldate = {2014-01-13},
	date = {2006},
}

@inproceedings{pennell_character-level_2011,
	title = {A Character-Level Machine Translation Approach for Normalization of {SMS} Abbreviations.},
	url = {http://www.mt-archive.info/IJCNLP-2011-Pennell.pdf},
	pages = {974--982},
	booktitle = {{IJCNLP}},
	author = {Pennell, Deana and Liu, Yang},
	urldate = {2014-01-13},
	date = {2011},
}

@inproceedings{pennell_toward_2011,
	title = {Toward text message normalization: Modeling abbreviation generation},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5947570},
	shorttitle = {Toward text message normalization},
	pages = {5364--5367},
	booktitle = {Acoustics, Speech and Signal Processing ({ICASSP}), 2011 {IEEE} International Conference on},
	author = {Pennell, Deana and Liu, Yang},
	urldate = {2014-01-13},
	date = {2011},
}

@article{choudhury_investigation_2007,
	title = {Investigation and modeling of the structure of texting language},
	volume = {10},
	url = {http://link.springer.com/article/10.1007/s10032-007-0054-0},
	pages = {157--174},
	number = {3},
	journaltitle = {International Journal of Document Analysis and Recognition ({IJDAR})},
	author = {Choudhury, Monojit and Saraf, Rahul and Jain, Vijit and Mukherjee, Animesh and Sarkar, Sudeshna and Basu, Anupam},
	urldate = {2014-01-13},
	date = {2007},
}

@article{liu_insertion_2011,
	title = {Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision.},
	volume = {11},
	url = {http://www.newdesign.aclweb.org/anthology-new/P/P11/P11-2013.pdf},
	shorttitle = {Insertion, Deletion, or Substitution?},
	pages = {71--76},
	journaltitle = {{ACL} (Short Papers)},
	author = {Liu, Fei and Weng, Fuliang and Wang, Bingqing and Liu, Yang},
	urldate = {2014-01-13},
	date = {2011},
}

@inproceedings{brody_cooooooooooooooollllllllllllll:_2011,
	title = {Cooooooooooooooollllllllllllll‼‼‼‼‼‼‼: using word lengthening to detect sentiment in microblogs},
	url = {http://dl.acm.org/citation.cfm?id=2145498},
	shorttitle = {Cooooooooooooooollllllllllllll‼‼‼‼‼‼‼},
	pages = {562--570},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	author = {Brody, Samuel and Diakopoulos, Nicholas},
	urldate = {2014-01-13},
	date = {2011},
}

@inproceedings{ju_language-modeling_2008,
	title = {A language-modeling approach to inverse text normalization and data cleanup for multimodal voice search applications.},
	url = {http://www.cslu.ogi.edu/~sproatr/Courses/TextNorm/Papers/IS080913.pdf},
	pages = {2179--2182},
	booktitle = {{INTERSPEECH}},
	author = {Ju, Yun-Cheng and Odell, Julian},
	urldate = {2014-01-13},
	date = {2008},
}

@inproceedings{collins_unsupervised_1999,
	title = {Unsupervised models for named entity classification},
	url = {http://acl.ldc.upenn.edu/W/W99/W99-0613.pdf?ref=Sawos.OrgR%7B.%EF%BF%BD%EF%BF%BD%EF%BF%BD%EF%BF%BD%C7%9D%EF%BF%BD%E2%80%A1%5E%EF%BF%BD%EF%BF%BD%C3%A87},
	pages = {189--196},
	booktitle = {Proceedings of the joint {SIGDAT} conference on empirical methods in natural language processing and very large corpora},
	author = {Collins, Michael and Singer, Yoram},
	urldate = {2014-01-13},
	date = {1999},
}

@article{bikel_algorithm_1999,
	title = {An algorithm that learns what's in a name},
	volume = {34},
	url = {http://link.springer.com/article/10.1023/A:1007558221122},
	pages = {211--231},
	number = {1},
	journaltitle = {Machine learning},
	author = {Bikel, Daniel M. and Schwartz, Richard and Weischedel, Ralph M.},
	urldate = {2014-01-13},
	date = {1999},
}

@inproceedings{mikheev_document_2000,
	title = {Document centered approach to text normalization},
	url = {http://dl.acm.org/citation.cfm?id=345564},
	pages = {136--143},
	booktitle = {Proceedings of the 23rd annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	author = {Mikheev, Andrei},
	urldate = {2014-01-13},
	date = {2000},
}

@book{hurford_linguistic_2011,
	title = {The linguistic theory of numerals},
	volume = {16},
	url = {http://books.google.com/books?hl=en&lr=&id=YmdCVlljkksC&oi=fnd&pg=PR1&dq=The+Linguistic+Theory+of+Numerals&ots=Jbei8GyRpR&sig=3YxIkkjFP6_YBU_jF6sPmyLqAt8},
	publisher = {Cambridge University Press},
	author = {Hurford, James R.},
	urldate = {2014-01-13},
	date = {2011},
}

@article{yarowsky_homograph_1997,
	title = {Homograph disambiguation in text-to-speech synthesis},
	url = {http://books.google.com/books?hl=en&lr=&id=rc3nG34LtvkC&oi=fnd&pg=PA157&dq=Homograph+disambiguation+in+text-to-speech+synthesis.&ots=6xrfiZ4B5b&sig=KJwHC32k45njC3WibEHH86eaDSg},
	pages = {157--172},
	journaltitle = {Progress in speech synthesis},
	author = {Yarowsky, David},
	urldate = {2014-01-13},
	date = {1997},
}

@article{cannon_abbreviations_1989,
	title = {Abbreviations and acronyms in English word-formation},
	volume = {64},
	url = {http://www.jstor.org/stable/10.2307/455038},
	pages = {99--127},
	number = {2},
	journaltitle = {American Speech},
	author = {Cannon, Garland},
	urldate = {2014-01-13},
	date = {1989},
}

@inproceedings{larkey_acrophile:_2000,
	title = {Acrophile: an automated acronym extractor and server},
	url = {http://dl.acm.org/citation.cfm?id=336664},
	shorttitle = {Acrophile},
	pages = {205--214},
	booktitle = {Proceedings of the fifth {ACM} conference on Digital libraries},
	author = {Larkey, Leah S. and Ogilvie, Paul and Price, M. Andrew and Tamilio, Brenden},
	urldate = {2014-01-13},
	date = {2000},
}

@inproceedings{sproat_lightly_2010,
	title = {Lightly supervised learning of text normalization: Russian number names},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5700892},
	shorttitle = {Lightly supervised learning of text normalization},
	pages = {436--441},
	booktitle = {Spoken Language Technology Workshop ({SLT}), 2010 {IEEE}},
	author = {Sproat, Richard},
	urldate = {2014-01-13},
	date = {2010},
}

@report{zhu_learning_2002,
	title = {Learning from labeled and unlabeled data with label propagation},
	url = {http://lvk.cs.msu.su/~bruzz/articles/classification/zhu02learning.pdf},
	institution = {Technical Report {CMU}-{CALD}-02-107, Carnegie Mellon University},
	author = {Zhu, Xiaojin and Ghahramani, Zoubin},
	urldate = {2014-01-13},
	date = {2002},
	keywords = {semisupervised},
}

@article{zhu_semi-supervised_2006,
	title = {Semi-supervised learning literature survey},
	volume = {2},
	url = {http://www.loni.ucla.edu/~ztu/courses/2013_CS_spring/reading/ssl_survey.pdf},
	pages = {3},
	journaltitle = {Computer Science, University of Wisconsin-Madison},
	author = {Zhu, Xiaojin},
	urldate = {2014-01-08},
	date = {2006},
}

@inproceedings{wang_beam-search_2013,
	title = {A Beam-Search Decoder for Normalization of Social Media Text with Application to Machine Translation},
	url = {http://www.newdesign.aclweb.org/anthology-new/N/N13/N13-1050.pdf},
	pages = {471--481},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Wang, Pidong and Ng, Hwee Tou},
	urldate = {2013-12-30},
	date = {2013},
}

@article{goldberg_efficient_nodate,
	title = {Efficient Implementation of Beam-Search Incremental Parsers},
	url = {http://acl.cs.qc.edu/~lhuang/papers/fastbeam.pdf},
	author = {Goldberg, Yoav and Zhao, Kai and Huang, Liang},
	urldate = {2013-12-29},
}

@inproceedings{goldberg_inspecting_2010,
	title = {Inspecting the structural biases of dependency parsing algorithms},
	url = {http://dl.acm.org/citation.cfm?id=1870595},
	pages = {234--242},
	booktitle = {Proceedings of the Fourteenth Conference on Computational Natural Language Learning},
	author = {Goldberg, Yoav and Elhadad, Michael},
	urldate = {2013-12-29},
	date = {2010},
}

@inproceedings{cai_language-independent_2011,
	title = {Language-Independent Parsing with Empty Elements.},
	url = {http://www.cs.bgu.ac.il/~yoavg/publications/acl2011empty.pdf},
	pages = {212--216},
	booktitle = {{ACL} (Short Papers)},
	author = {Cai, Shu and Chiang, David and Goldberg, Yoav},
	urldate = {2013-12-29},
	date = {2011},
}

@inproceedings{goldberg_em_2008,
	title = {{EM} Can Find Pretty Good {HMM} {POS}-Taggers (When Given a Good Start).},
	pages = {746--754},
	booktitle = {{ACL}},
	author = {Goldberg, Yoav and Adler, Meni and Elhadad, Michael},
	date = {2008},
}

@inproceedings{tsarfaty_statistical_2010,
	title = {Statistical parsing of morphologically rich languages ({SPMRL}): what, how and whither},
	url = {http://dl.acm.org/citation.cfm?id=1868772},
	shorttitle = {Statistical parsing of morphologically rich languages ({SPMRL})},
	pages = {1--12},
	booktitle = {Proceedings of the {NAACL} {HLT} 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages},
	author = {Tsarfaty, Reut and Seddah, Djamé and Goldberg, Yoav and Kübler, Sandra and Candito, Marie and Foster, Jennifer and Versley, Yannick and Rehbein, Ines and Tounsi, Lamia},
	urldate = {2013-12-29},
	date = {2010},
}

@inproceedings{goldberg_single_2008,
	title = {A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing.},
	url = {https://www.aclweb.org/anthology/P/P08/P08-1043.pdf},
	pages = {371--379},
	booktitle = {{ACL}},
	author = {Goldberg, Yoav and Tsarfaty, Reut},
	urldate = {2013-12-29},
	date = {2008},
}

@inproceedings{goldberg_efficient_2010,
	title = {An efficient algorithm for easy-first non-directional dependency parsing},
	url = {http://dl.acm.org/citation.cfm?id=1858114},
	pages = {742--750},
	booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	author = {Goldberg, Yoav and Elhadad, Michael},
	urldate = {2013-12-29},
	date = {2010},
}

@inproceedings{goldberg_role_2009,
	title = {On the role of lexical features in sequence labeling},
	url = {http://dl.acm.org/citation.cfm?id=1699660},
	pages = {1142--1151},
	booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3},
	author = {Goldberg, Yoav and Elhadad, Michael},
	urldate = {2013-12-29},
	date = {2009},
}

@article{honnibal_non-monotonic_2013,
	title = {A Non-Monotonic Arc-Eager Transition System for Dependency Parsing},
	url = {http://www.newdesign.aclweb.org/anthology/W/W13/W13-35.pdf#page=175},
	pages = {163},
	journaltitle = {{CoNLL}-2013},
	author = {Honnibal, Matthew and Goldberg, Yoav and Johnson, Mark},
	urldate = {2013-12-29},
	date = {2013},
}

@article{efron_bootstrap_1979,
	title = {Bootstrap methods: another look at the jackknife},
	url = {http://www.jstor.org/stable/10.2307/2958830},
	shorttitle = {Bootstrap methods},
	pages = {1--26},
	journaltitle = {The annals of Statistics},
	author = {Efron, Bradley},
	urldate = {2013-12-23},
	date = {1979},
}

@article{sontag_introduction_2011,
	title = {Introduction to dual decomposition for inference},
	volume = {1},
	url = {http://www.cse.ohio-state.edu/~kulis/teaching/5539_fa12/slides/dualdecomposlides.pdf},
	journaltitle = {Optimization for Machine Learning},
	author = {Sontag, David and Globerson, Amir and Jaakkola, Tommi},
	urldate = {2013-12-23},
	date = {2011},
}

@inproceedings{dreyer_hyter:_2012,
	title = {Hyter: Meaning-equivalent semantics for translation evaluation},
	url = {http://dl.acm.org/citation.cfm?id=2382052},
	shorttitle = {Hyter},
	pages = {162--171},
	booktitle = {Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	author = {Dreyer, Markus and Marcu, Daniel},
	urldate = {2013-12-23},
	date = {2012},
}

@inproceedings{zolghadr_online_2013,
	title = {Online Learning with Costly Features and Labels},
	url = {http://papers.nips.cc/paper/5149-online-learning-with-costly-features-and-labels},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1241--1249},
	author = {Zolghadr, Navid and Bartok, Gabor and Greiner, Russell and György, András and Szepesvari, Csaba},
	urldate = {2013-12-17},
	date = {2013},
}
@article{roweis_nonlinear_2000,
	title = {Nonlinear dimensionality reduction by locally linear embedding},
	volume = {290},
	url = {http://www.sciencemag.org/content/290/5500/2323.short},
	pages = {2323--2326},
	number = {5500},
	journaltitle = {Science},
	author = {Roweis, Sam T. and Saul, Lawrence K.},
	urldate = {2013-12-19},
	date = {2000},
}

@inproceedings{turian_preliminary_2009,
	title = {A preliminary evaluation of word representations for named-entity recognition},
	url = {http://www0.cs.ucl.ac.uk/staff/rmartin/grll09/turian1.pdf},
	booktitle = {{NIPS} Workshop on Grammar Induction, Representation of Language and Language Learning},
	author = {Turian, Joseph and Ratinov, Lev and Bengio, Yoshua and Roth, Dan},
	urldate = {2013-12-19},
	date = {2009},
}

@inproceedings{dhillon_multi-view_2011,
	title = {Multi-view learning of word embeddings via cca},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2011_0157.pdf},
	pages = {199--207},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Dhillon, Paramveer and Foster, Dean P. and Ungar, Lyle H.},
	urldate = {2013-12-19},
	date = {2011},
}

@book{knuth_mathematical_1989,
	title = {Mathematical writing},
	url = {http://books.google.com/books?hl=en&lr=&id=dDOehHMbUMcC&oi=fnd&pg=PA1&dq=knuth+mathematical+writing&ots=8aiz0aiV8t&sig=izpI0oDAdk_1SBLGvBAXHLIkgaE},
	number = {14},
	publisher = {Cambridge University Press},
	author = {Knuth, Donald E. and Larrabee, Tracy and Roberts, Paul M.},
	urldate = {2013-12-17},
	date = {1989},
}

@inproceedings{xiao_novel_2013,
	title = {A Novel Two-Step Method for Cross Language Representation Learning},
	url = {http://papers.nips.cc/paper/5164-a-novel-two-step-method-for-cross-language-representation-learning},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1259--1267},
	author = {Xiao, Min and Guo, Yuhong},
	urldate = {2013-12-17},
	date = {2013},
}

@inproceedings{taghipour_first-order_2013,
	title = {First-order Decomposition Trees},
	url = {http://papers.nips.cc/paper/5160-first-order-decomposition-trees},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1052--1060},
	author = {Taghipour, Nima and Davis, Jesse and Blockeel, Hendrik},
	urldate = {2013-12-17},
	date = {2013},
}

@inproceedings{yang_conditional_2013,
	title = {Conditional Random Fields via Univariate Exponential Families},
	url = {http://papers.nips.cc/paper/5154-conditional-random-fields-via-univariate-exponential-families},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {683--691},
	author = {Yang, Eunho and Ravikumar, Pradeep and Allen, Genevera I. and Liu, Zhandong},
	urldate = {2013-12-17},
	date = {2013},
}

@inproceedings{cesa-bianchi_online_2013,
	title = {Online Learning with Switching Costs and Other Adaptive Adversaries},
	url = {http://papers.nips.cc/paper/5151-online-learning-with-switching-costs-and-other-adaptive-adversaries},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1160--1168},
	author = {Cesa-Bianchi, Nicolò and Dekel, Ofer and Shamir, Ohad},
	urldate = {2013-12-17},
	date = {2013},
}

@inproceedings{rakhlin_optimization_2013,
	title = {Optimization, Learning, and Games with Predictable Sequences},
	url = {http://papers.nips.cc/paper/5147-optimization-learning-and-games-with-predictable-sequences},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {3066--3074},
	author = {Rakhlin, Sasha and Sridharan, Karthik},
	urldate = {2013-12-17},
	date = {2013},
}

@inproceedings{mcmahan_minimax_2013,
	title = {Minimax Optimal Algorithms for Unconstrained Linear Optimization},
	url = {http://papers.nips.cc/paper/5148-minimax-optimal-algorithms-for-unconstrained-linear-optimization},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {2724--2732},
	author = {{McMahan}, Brendan and Abernethy, Jacob},
	urldate = {2013-12-17},
	date = {2013},
}

@inproceedings{becker_non-linear_2013,
	title = {Non-Linear Domain Adaptation with Boosting},
	url = {http://papers.nips.cc/paper/5200-non-linear-domain-adaptation-with-boosting},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {485--493},
	author = {Becker, Carlos J. and Christoudias, Christos M. and Fua, Pascal},
	urldate = {2013-12-17},
	date = {2013},
}

@inproceedings{lefakis_reservoir_2013,
	title = {Reservoir Boosting : Between Online and Offline Ensemble Learning},
	url = {http://papers.nips.cc/paper/5215-reservoir-boosting-between-online-and-offline-ensemble-learning},
	shorttitle = {Reservoir Boosting},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1412--1420},
	author = {Lefakis, Leonidas and Fleuret, François},
	urldate = {2013-12-17},
	date = {2013},
}

@book{noauthor_information_nodate,
	title = {Information Theory, Inference, and Learning Algorithms},
}

@inproceedings{jehl_twitter_2012,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Twitter Translation Using Translation-based Cross-lingual Retrieval},
	url = {http://dl.acm.org/citation.cfm?id=2393015.2393074},
	series = {{WMT} '12},
	abstract = {Microblogging services such as Twitter have become popular media for real-time usercreated news reporting. Such communication often happens in parallel in different languages, e.g., microblog posts related to the same events of the Arab spring were written in Arabic and in English. The goal of this paper is to exploit this parallelism in order to eliminate the main bottleneck in automatic Twitter translation, namely the lack of bilingual sentence pairs for training {SMT} systems. We show that translation-based cross-lingual information retrieval can retrieve microblog messages across languages that are similar enough to be used to train a standard phrase-based {SMT} pipeline. Our method outperforms other approaches to domain adaptation for {SMT} such as language model adaptation, meta-parameter tuning, or self-translation.},
	pages = {410--421},
	booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
	publisher = {Association for Computational Linguistics},
	author = {Jehl, Laura and Hieber, Felix and Riezler, Stefan},
	urldate = {2013-12-15},
	date = {2012},
}

@article{shalev-shwartz_online_2011,
	title = {Online Learning and Online Convex Optimization},
	volume = {4},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-machine-learning/MAL-018},
	doi = {10.1561/2200000018},
	pages = {107--194},
	number = {2},
	journaltitle = {Foundations and Trends® in Machine Learning},
	author = {Shalev-Shwartz, Shai},
	urldate = {2013-12-10},
	date = {2011},
	keywords = {toread 1},
}

@inproceedings{shi_sparse_2013,
	title = {Sparse Additive Text Models with Low Rank Background},
	url = {http://papers.nips.cc/paper/5139-sparse-additive-text-models-with-low-rank-background},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {172--180},
	author = {Shi, Lei},
	urldate = {2013-12-10},
	date = {2013},
	keywords = {toread 2},
}

@inproceedings{perina_documents_2013,
	title = {Documents as multiple overlapping windows into grids of counts},
	url = {http://papers.nips.cc/paper/5140-documents-as-multiple-overlapping-windows-into-grids-of-counts},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {10--18},
	author = {Perina, Alessandro and Jojic, Nebojsa and Bicego, Manuele and Truski, Andrzej},
	urldate = {2013-12-10},
	date = {2013},
	keywords = {toread 2},
}

@inproceedings{natarajan_learning_2013,
	title = {Learning with Noisy Labels},
	url = {http://papers.nips.cc/paper/5073-learning-with-noisy-labels},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1196--1204},
	author = {Natarajan, Nagarajan and Dhillon, Inderjit and Ravikumar, Pradeep and Tewari, Ambuj},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 3},
}

@inproceedings{lu_deep_2013,
	title = {A Deep Architecture for Matching Short Texts},
	url = {http://papers.nips.cc/paper/5019-a-deep-architecture-for-matching-short-texts},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1367--1375},
	author = {Lu, Zhengdong and Li, Hang},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 4},
}

@inproceedings{mohan_graphical_2013,
	title = {Graphical Models for Inference with Missing Data},
	url = {http://papers.nips.cc/paper/4899-graphical-models-for-inference-with-missing-data},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1277--1285},
	author = {Mohan, Karthika and Pearl, Judea and Tian, Jin},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 4},
}

@inproceedings{domke_structured_2013,
	title = {Structured Learning via Logistic Regression},
	url = {http://papers.nips.cc/paper/4870-structured-learning-via-logistic-regression},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {647--655},
	author = {Domke, Justin},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 2},
}

@inproceedings{huang_learning_2013,
	title = {Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition},
	url = {http://papers.nips.cc/paper/5065-learning-hidden-markov-models-from-non-sequence-data-via-tensor-decomposition},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {333--341},
	author = {Huang, Tzu-Kuo and Schneider, Jeff},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 4},
}

@inproceedings{baldi_understanding_2013,
	title = {Understanding Dropout},
	url = {http://papers.nips.cc/paper/4878-understanding-dropout},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {2814--2822},
	author = {Baldi, Pierre and Sadowski, Peter J.},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 4},
}

@inproceedings{gribonval_reconciling_2013,
	title = {Reconciling priors'' \& "priors" without prejudice?},
	url = {http://papers.nips.cc/paper/4868-reconciling-priors-priors-without-prejudice},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {2193--2201},
	author = {Gribonval, Remi and Machart, Pierre},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 4},
}

@inproceedings{socher_reasoning_2013,
	title = {Reasoning With Neural Tensor Networks for Knowledge Base Completion},
	url = {http://papers.nips.cc/paper/5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {926--934},
	author = {Socher, Richard and Chen, Danqi and Manning, Christopher D. and Ng, Andrew},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 4},
}

@inproceedings{srivastava_discriminative_2013,
	title = {Discriminative Transfer Learning with Tree-based Priors},
	url = {http://papers.nips.cc/paper/5029-discriminative-transfer-learning-with-tree-based-priors},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {2094--2102},
	author = {Srivastava, Nitish and Salakhutdinov, Ruslan},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 3},
}

@inproceedings{socher_zero-shot_2013,
	title = {Zero-Shot Learning Through Cross-Modal Transfer},
	url = {http://papers.nips.cc/paper/5027-zero-shot-learning-through-cross-modal-transfer},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {935--943},
	author = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D. and Ng, Andrew},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 4},
}

@inproceedings{boytsov_learning_2013,
	title = {Learning to Prune in Metric and Non-Metric Spaces},
	url = {http://papers.nips.cc/paper/5018-learning-to-prune-in-metric-and-non-metric-spaces},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1574--1582},
	author = {Boytsov, Leonid and Naidan, Bilegsaikhan},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 2},
}

@inproceedings{yang_poisson_2013,
	title = {On Poisson Graphical Models},
	url = {http://papers.nips.cc/paper/5153-on-poisson-graphical-models},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1718--1726},
	author = {Yang, Eunho and Ravikumar, Pradeep and Allen, Genevera I. and Liu, Zhandong},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 4},
}

@inproceedings{broderick_streaming_2013,
	title = {Streaming Variational Bayes},
	url = {http://papers.nips.cc/paper/4980-streaming-variational-bayes},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1727--1735},
	author = {Broderick, Tamara and Boyd, Nicholas and Wibisono, Andre and Wilson, Ashia C. and Jordan, Michael},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 3},
}

@inproceedings{paskov_compressive_2013,
	title = {Compressive Feature Learning},
	url = {http://papers.nips.cc/paper/4932-compressive-feature-learning},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {2931--2939},
	author = {Paskov, Hristo S. and West, Robert and Mitchell, John C. and Hastie, Trevor},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 3},
}

@inproceedings{javanmard_confidence_2013,
	title = {Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models},
	url = {http://papers.nips.cc/paper/4931-confidence-intervals-and-hypothesis-testing-for-high-dimensional-statistical-models},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1187--1195},
	author = {Javanmard, Adel and Montanari, Andrea},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 3},
}
@inproceedings{smith_learning_2013,
	title = {Learning and using language via recursive pragmatic reasoning about other agents},
	url = {http://papers.nips.cc/paper/4929-learning-and-using-language-via-recursive-pragmatic-reasoning-about-other-agents},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {3039--3047},
	author = {Smith, Nathaniel J. and Goodman, Noah and Frank, Michael},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 3},
}

@inproceedings{liu_scoring_2013,
	title = {Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?},
	url = {http://papers.nips.cc/paper/4889-scoring-workers-in-crowdsourcing-how-many-control-questions-are-enough},
	shorttitle = {Scoring Workers in Crowdsourcing},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1914--1922},
	author = {Liu, Qiang and Ihler, Alex and Steyvers, Mark},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {toread 2},
}

@inproceedings{cisse_robust_2013,
	title = {Robust Bloom Filters for Large {MultiLabel} Classification Tasks},
	url = {http://papers.nips.cc/paper/5083-robust-bloom-filters-for-large-multilabel-classification-tasks},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	eventtitle = {Advances in Neural Information Processing Systems},
	pages = {1851--1859},
	author = {Cisse, Moustapha M. and Usunier, Nicolas and Artières, Thierry and Gallinari, Patrick},
	urldate = {2013-12-08},
	date = {2013},
	keywords = {bloom filters, classification, data structures, machine learning, toread 3},
}

@inproceedings{alonso_tabular_1999,
	title = {Tabular algorithms for {TAG} parsing},
	url = {http://dl.acm.org/citation.cfm?id=977056},
	pages = {150--157},
	booktitle = {Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics},
	author = {Alonso, Miguel A. and de la Clergerie, Eric and Cabrero, David and Vilares, Manuel},
	urldate = {2013-12-07},
	date = {1999},
	keywords = {771, mcsg, parsing, toread 3, tree adjoining grammar},
}

@article{del_moral_concentration_2010,
	title = {On the Concentration Properties of Interacting Particle Processes},
	volume = {3},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com.proxy1.library.jhu.edu/articles/foundations-and-trends-in-machine-learning/MAL-026},
	doi = {10.1561/2200000026},
	pages = {225--389},
	number = {3},
	journaltitle = {Foundations and Trends® in Machine Learning},
	author = {Del Moral, Pierre},
	urldate = {2013-12-07},
	date = {2010},
	keywords = {toread 4},
}

@article{alvarez_kernels_2012,
	title = {Kernels for Vector-Valued Functions: A Review},
	volume = {4},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com.proxy1.library.jhu.edu/articles/foundations-and-trends-in-machine-learning/MAL-036},
	doi = {10.1561/2200000036},
	shorttitle = {Kernels for Vector-Valued Functions},
	pages = {195--266},
	number = {3},
	journaltitle = {Foundations and Trends® in Machine Learning},
	author = {Álvarez, Mauricio A.},
	urldate = {2013-12-06},
	date = {2012},
	keywords = {kernel methods, toread 4},
}

@article{sutton_introduction_2012,
	title = {An Introduction to Conditional Random Fields},
	volume = {4},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com.proxy1.library.jhu.edu/articles/foundations-and-trends-in-machine-learning/MAL-013},
	doi = {10.1561/2200000013},
	pages = {267--373},
	number = {4},
	journaltitle = {Foundations and Trends® in Machine Learning},
	author = {Sutton, Charles},
	urldate = {2013-12-06},
	date = {2012},
	keywords = {conditional random fields, pgm, toread 2},
}

@article{kulesza_determinantal_2012,
	title = {Determinantal Point Processes for Machine Learning},
	volume = {5},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com.proxy1.library.jhu.edu/articles/foundations-and-trends-in-machine-learning/MAL-044},
	doi = {10.1561/2200000044},
	pages = {123--286},
	number = {2},
	journaltitle = {Foundations and Trends® in Machine Learning},
	author = {Kulesza, Alex},
	urldate = {2013-12-06},
	date = {2012},
	keywords = {toread 3},
}

@book{jordan_introduction_1998,
	title = {An introduction to variational methods for graphical models},
	url = {http://link.springer.com/chapter/10.1007/978-94-011-5014-9_5},
	publisher = {Springer},
	author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
	urldate = {2013-12-05},
	date = {1998},
	keywords = {toread 4},
}

@article{steyvers_probabilistic_2007,
	title = {Probabilistic topic models},
	volume = {427},
	url = {http://books.google.com/books?hl=en&lr=&id=JbzCzPvzpmQC&oi=fnd&pg=PA427&dq=probabilistic+topic+models&ots=aLN4I2PXJN&sig=Vei7ot_P--YOPK5xO-R12Sq8TYY},
	pages = {424--440},
	number = {7},
	journaltitle = {Handbook of latent semantic analysis},
	author = {Steyvers, Mark and Griffiths, Tom},
	urldate = {2013-12-04},
	date = {2007},
	keywords = {intro, topic models, toread 2},
}

@misc{riley_tag_nodate,
	title = {{TAG} Parsing with Lots of Pictures},
	author = {Riley, Darcey},
	keywords = {771, intro, mcsg, parsing, toread 2, tree adjoining grammar, tutorial},
}

@book{jordan_learning_1999,
	location = {Cambridge, Mass.},
	title = {Learning in graphical models},
	isbn = {0262600323  9780262600323},
	publisher = {{MIT} Press},
	author = {Jordan, Michael Irwin},
	date = {1999},
	keywords = {toread 3},
}

@article{kuhlmann_tree-adjoining_2012,
	title = {Tree-adjoining grammars are not closed under strong lexicalization},
	volume = {38},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00090},
	pages = {617--629},
	number = {3},
	journaltitle = {Computational Linguistics},
	author = {Kuhlmann, Marco and Satta, Giorgio},
	urldate = {2013-11-21},
	date = {2012},
	keywords = {771, grammar formalism, toread 2, tree adjoining grammar},
}

@inproceedings{rush_optimal_2013,
	title = {Optimal Beam Search for Machine Translation},
	eventtitle = {Proceedings of {EMNLP}},
	author = {Rush, Alexander M and Chang, Yin-Wen and Collins, Michael},
	date = {2013},
	keywords = {765, machine translation, toread 1},
}

@inproceedings{ling_paraphrasing_2013,
	title = {Paraphrasing 4 Microblog Normalization},
	url = {http://www.cs.cmu.edu/~lingwang/papers/emnlp2013.pdf},
	eventtitle = {Proceedings of {EMNLP}},
	author = {Ling, Wang and Trancoso, Isabel and Dyer, Chris and Black, Alan W.},
	urldate = {2013-10-16},
	date = {2013},
}

@incollection{gazdar_introduction:_1988,
	title = {Introduction: Applicability of indexed grammars to natural languages},
	url = {http://link.springer.com/chapter/10.1007/978-94-009-1337-0_3},
	booktitle = {Applicability of indexed grammars to natural languages},
	publisher = {Springer},
	author = {Gazdar, Gerald},
	urldate = {2013-11-18},
	date = {1988},
	keywords = {771, {NLP}, indexed grammar, mcsg, toread 2},
}

@incollection{joshi_tree-adjoining_1997,
	title = {Tree-Adjoining Grammars},
	rights = {©1997 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-63859-6, 978-3-642-59126-6},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-59126-6_2},
	abstract = {In this paper, we will describe a tree generating system called tree-adjoining grammar ({TAG}) and state some of the recent results about {TAGs}. The work on {TAGs} is motivated by linguistic considerations. However, a number of formal results have been established for {TAGs}, which we believe, would be of interest to researchers in formal languages and automata, including those interested in tree grammars and tree automata.},
	pages = {69--123},
	booktitle = {Handbook of Formal Languages},
	publisher = {Springer Berlin Heidelberg},
	author = {Joshi, Aravind K. and Schabes, Yves},
	editor = {Rozenberg, Prof Dr Grzegorz and Salomaa, Prof Dr Arto},
	urldate = {2013-11-18},
	date = {1997-01-01},
	langid = {english},
	keywords = {771, Combinatorics, Computer Graphics, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, grammar formalism, mcsg, reading, toread 2, tree adjoining grammar},
}

@incollection{rajaraman_chapter_2012,
	location = {New York, N.Y.; Cambridge},
	title = {Chapter 11: Dimensionality Reduction},
	isbn = {9781107015357  1107015359},
	booktitle = {Mining of massive datasets},
	publisher = {Cambridge University Press},
	author = {Rajaraman, Anand and Ullman, Jeffrey D},
	date = {2012},
	keywords = {615, big data, dimensionality reduction},
}

@book{grune_parsing_2008,
	location = {New York; London},
	title = {Parsing techniques: a practical guide},
	isbn = {9781441919014 1441919015},
	shorttitle = {Parsing techniques},
	publisher = {Springer},
	author = {Grune, Dick},
	date = {2008},
	keywords = {intro, parsing, toread 2},
}

@online{kun_probabilistic_nodate,
	title = {Probabilistic Bounds — A Primer},
	url = {http://jeremykun.com/2013/04/15/probabilistic-bounds-a-primer/},
	abstract = {Probabilistic arguments are a key tool for the analysis of algorithms in machine learning theory and probability theory. They also assume a prominent role in the analysis of randomized and streamin...},
	titleaddon = {Math ∩ Programming},
	author = {Kun, Jeremy},
	urldate = {2013-11-15},
	keywords = {intro, probability, reading, toread 3},
}

@article{knight_machine_1998,
	title = {Machine transliteration},
	volume = {24},
	url = {http://dl.acm.org/citation.cfm?id=972767},
	pages = {599--612},
	number = {4},
	journaltitle = {Computational Linguistics},
	author = {Knight, Kevin and Graehl, Jonathan},
	urldate = {2013-11-15},
	date = {1998},
	keywords = {{AT}-{AT}, alopez, toread 3},
}

@inproceedings{dyer_simple_2013,
	title = {A simple, fast, and effective reparameterization of {IBM} Model 2},
	url = {http://www.newdesign.aclweb.org/anthology/N/N13/N13-1073.pdf},
	pages = {644--648},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Dyer, Chris and Chahuneau, Victor and Smith, Noah A.},
	urldate = {2013-11-15},
	date = {2013},
	keywords = {alopez, machine translation, toread 2, word alignment},
}

@inproceedings{vogel_hmm-based_1996,
	location = {Stroudsburg, {PA}, {USA}},
	title = {{HMM}-based word alignment in statistical translation},
	url = {http://dx.doi.org/10.3115/993268.993313},
	doi = {10.3115/993268.993313},
	series = {{COLING} '96},
	abstract = {In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model ({HMM}) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment {HMM} is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.},
	pages = {836--841},
	booktitle = {Proceedings of the 16th conference on Computational linguistics - Volume 2},
	publisher = {Association for Computational Linguistics},
	author = {Vogel, Stephan and Ney, Hermann and Tillmann, Christoph},
	urldate = {2013-11-08},
	date = {1996},
	keywords = {{AT}-{AT}, {HMM}, machine learning, reading, word alignment},
}

@book{koehn_statistical_2007,
	location = {Cambridge},
	title = {Statistical machine translation},
	isbn = {9780521874151 0521874157},
	shorttitle = {Statistical machine translation},
	publisher = {Univ. Pr.},
	author = {Koehn, Philipp},
	date = {2007},
	keywords = {machine translation},
}

@article{vijay-shanker_parsing_1993,
	title = {Parsing some constrained grammar formalisms},
	volume = {19},
	url = {http://dl.acm.org/citation.cfm?id=972502},
	pages = {591--636},
	number = {4},
	journaltitle = {Computational Linguistics},
	author = {Vijay-Shanker, Krishnamurti and Weir, David J.},
	urldate = {2013-11-07},
	date = {1993},
	keywords = {771},
}

@article{bisani_joint-sequence_2008,
	title = {Joint-sequence models for grapheme-to-phoneme conversion},
	volume = {50},
	url = {http://www.sciencedirect.com/science/article/pii/S0167639308000046},
	pages = {434--451},
	number = {5},
	journaltitle = {Speech Communication},
	author = {Bisani, Maximilian and Ney, Hermann},
	urldate = {2013-11-07},
	date = {2008},
}

@inproceedings{garrette_learning_2013,
	location = {Atlanta, {GA}},
	title = {Learning a Part-of-Speech Tagger from Two Hours of Annotation},
	abstract = {Most work on weakly-supervised learning for
part-of-speech taggers has been based on un-
realistic assumptions about the amount and
quality of training data. For this paper, we
attempt to create true low-resource scenarios
by allowing a linguist just two hours to anno-
tate data and evaluating on the languages Kin-
yarwanda and Malagasy. Given these severely
limited amounts of either type supervision
(tag dictionaries) or token supervision (labeled
sentences), we are able to dramatically im-
prove the learning of a hidden Markov model
through our method of automatically general-
izing the annotations, reducing noise, and in-
ducing word-tag frequency information.},
	eventtitle = {{NAACL}},
	booktitle = {Proceedings of {NAACL}-{HLT} 2013},
	author = {Garrette, Dan and Baldridge, Jason},
	date = {2013},
	keywords = {{POS} tag, semisupervised, toread 3},
}

@article{fraser_measuring_2007,
	title = {Measuring word alignment quality for statistical machine translation},
	volume = {33},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/coli.2007.33.3.293},
	pages = {293--303},
	number = {3},
	journaltitle = {Computational Linguistics},
	author = {Fraser, Alexander and Marcu, Daniel},
	urldate = {2013-11-06},
	date = {2007},
	keywords = {{AT}-{AT}, alopez, toread 1, word alignment},
}

@inproceedings{och_improved_2000,
	title = {Improved statistical alignment models},
	url = {http://dl.acm.org/citation.cfm?id=1075274},
	pages = {440--447},
	booktitle = {Proceedings of the 38th Annual Meeting on Association for Computational Linguistics},
	author = {Och, Franz Josef and Ney, Hermann},
	urldate = {2013-11-06},
	date = {2000},
	keywords = {{AT}-{AT}, alopez, machine translation, toread 2, word alignment},
}

@article{mohri_edit-distance_2003,
	title = {Edit-distance of weighted automata: general definitions and algorithms},
	volume = {14},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0129054103002114},
	shorttitle = {Edit-distance of weighted automata},
	pages = {957--982},
	number = {6},
	journaltitle = {International Journal of Foundations of Computer Science},
	author = {Mohri, Mehryar},
	urldate = {2013-11-06},
	date = {2003},
}

@article{boyd_distributed_2011,
	title = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
	volume = {3},
	issn = {1935-8237},
	url = {http://dx.doi.org/10.1561/2200000016},
	doi = {10.1561/2200000016},
	abstract = {Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas–Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for ℓ1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed {MPI} and Hadoop {MapReduce} implementations.},
	pages = {1--122},
	number = {1},
	journaltitle = {Found. Trends Mach. Learn.},
	author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
	urldate = {2013-11-05},
	date = {2011-01},
	keywords = {toread 4},
}

@article{li_snp_2009,
	title = {{SNP} detection for massively parallel whole-genome resequencing},
	volume = {19},
	url = {http://genome.cshlp.org/content/19/6/1124.short},
	pages = {1124--1132},
	number = {6},
	journaltitle = {Genome research},
	author = {Li, Ruiqiang and Li, Yingrui and Fang, Xiaodong and Yang, Huanming and Wang, Jian and Kristiansen, Karsten and Wang, Jun},
	urldate = {2013-10-29},
	date = {2009},
	keywords = {615},
}

@article{langmead_searching_2009,
	title = {Searching for {SNPs} with cloud computing},
	volume = {10},
	rights = {2009 Langmead et al; licensee {BioMed} Central Ltd.},
	issn = {1465-6906},
	url = {http://genomebiology.com/2009/10/11/R134/abstract},
	doi = {10.1186/gb-2009-10-11-r134},
	abstract = {As {DNA} sequencing outpaces improvements in computer speed, there is a critical need to accelerate tasks like alignment and {SNP} calling. Crossbow is a cloud-computing software tool that combines the aligner Bowtie and the {SNP} caller {SOAPsnp}. Executing in parallel using Hadoop, Crossbow analyzes data comprising 38-fold coverage of the human genome in three hours using a 320-{CPU} cluster rented from a cloud computing service for about \$85. Crossbow is available from http://bowtie-bio.sourceforge.net/crossbow/.
{PMID}: 19930550},
	pages = {R134},
	number = {11},
	journaltitle = {Genome Biology},
	author = {Langmead, Ben and Schatz, Michael C. and Lin, Jimmy and Pop, Mihai and Salzberg, Steven L.},
	urldate = {2013-10-29},
	date = {2009-11-20},
	langid = {english},
	pmid = {19930550},
	keywords = {615},
}

@report{layer_lumpy:_2012,
	title = {{LUMPY}: A probabilistic framework for structural variant discovery},
	url = {http://arxiv.org/abs/1210.2342},
	shorttitle = {{LUMPY}},
	abstract = {Comprehensive discovery of structural variation ({SV}) in human genomes from {DNA} sequencing requires the integration of multiple alignment signals including read-pair, split-read and read-depth. However, owing to inherent technical challenges, most existing {SV} discovery approaches utilize only one signal and consequently suffer from reduced sensitivity, especially at low sequence coverage and for smaller {SVs}. We present a novel and extremely flexible probabilistic {SV} discovery framework that is capable of integrating any number of {SV} detection signals including those generated from read alignments or prior evidence. We demonstrate improved sensitivity over extant methods by combining paired-end and split-read alignments and emphasize the utility of our framework for comprehensive studies of structural variation in heterogeneous tumor genomes. We further discuss the broader utility of this approach for probabilistic integration of diverse genomic interval datasets.},
	number = {1210.2342},
	type = {{arXiv} e-print},
	author = {Layer, Ryan M. and Hall, Ira M. and Quinlan, Aaron R.},
	urldate = {2013-10-29},
	date = {2012-10-08},
	keywords = {615, 615 project, Quantitative Biology - Genomics},
}

@article{aho_indexed_1968,
	title = {Indexed grammars—an extension of context-free grammars},
	volume = {15},
	url = {http://dl.acm.org/citation.cfm?id=321488},
	pages = {647--671},
	number = {4},
	journaltitle = {Journal of the {ACM} ({JACM})},
	author = {Aho, Alfred V.},
	urldate = {2013-10-28},
	date = {1968},
	keywords = {771, mcsg},
}

@misc{smith_lagrange_2004,
	title = {Lagrange Multipliers Tutorial in the Context of Support Vector Machines},
	publisher = {Faculty of Engineering and Applied Science Memorial University of Newfoundland},
	author = {Smith, Baxter Tyson},
	date = {2004-06-03},
	keywords = {{SVM}, analysis, lagrange multipliers, machine learning, toread 1, tutorial},
}

@book{koller_probabilistic_2009,
	location = {Cambridge, {MA}},
	title = {Probabilistic graphical models: principles and techniques},
	isbn = {9780262013192  0262013193},
	shorttitle = {Probabilistic graphical models},
	publisher = {{MIT} Press},
	author = {Koller, Daphne and Friedman, Nir},
	date = {2009},
}

@report{mikolov_exploiting_2013,
	title = {Exploiting Similarities among Languages for Machine Translation},
	url = {http://arxiv.org/abs/1309.4168},
	abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90\% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
	number = {1309.4168},
	type = {{arXiv} e-print},
	author = {Mikolov, Tomas and Le, Quoc V. and Sutskever, Ilya},
	urldate = {2013-10-23},
	date = {2013-09-16},
	keywords = {{AT}-{AT}, Computer Science - Computation and Language, alopez, machine translation, toread 1},
}

@misc{collins_em_1997,
	title = {The {EM} algorithm},
	url = {ftp://ftp.heanet.ie/disk1/sourceforge/e/project/em/emalgorithm/The%20paper%20of%20EM%20algorithm/The%20EM%20Algorithm.pdf},
	publisher = {fulfillment of Written Preliminary Exam {II} requirement},
	author = {Collins, Michael},
	urldate = {2013-10-23},
	date = {1997},
	keywords = {{EM}, machine learning, reading, toread 3, tutorial},
}

@article{mohri_common_2006,
	title = {On a common fallacy in computational linguistics},
	volume = {19},
	url = {http://www.ling.helsinki.fi/sky/julkaisut/SKY2006_1/1.6.5.%20MOHRI%20&%20SPROAT.pdf},
	pages = {432--439},
	journaltitle = {{SKY} J Ling},
	author = {Mohri, Mehryar and Sproat, Richard},
	urldate = {2013-10-21},
	date = {2006},
	keywords = {771},
}

@incollection{shieber_evidence_1987,
	title = {Evidence against the context-freeness of natural language},
	url = {http://link.springer.com/chapter/10.1007/978-94-009-3401-6_12},
	pages = {320--334},
	booktitle = {The Formal complexity of natural language},
	publisher = {Springer},
	author = {Shieber, Stuart M.},
	urldate = {2013-10-21},
	date = {1987},
	keywords = {771, context free, linguistics},
}

@book{bangalore_supertagging:_2010,
	location = {Cambridge, Mass.},
	title = {Supertagging: using complex lexical descriptions in natural language processing},
	isbn = {9780262013871  0262013878},
	shorttitle = {Supertagging},
	abstract = {Investigations into employing statistical approaches with linguistically motivated representations and its impact on Natural Language processing tasks.},
	publisher = {{MIT} Press},
	author = {Bangalore, Srinivas and Joshi, Aravind K},
	date = {2010},
}

@misc{oetiker_not_1995,
	title = {The not so short introduction to {LATEX}2ε},
	url = {http://jamsb.austms.org.au/resources/LaTeX/lshort.pdf},
	author = {Oetiker, Tobias and Partl, Hubert and Hyna, Irene and Schlegl, Elisabeth},
	urldate = {2013-10-16},
	date = {1995},
}
@article{vidal_probabilistic_2005,
	title = {Probabilistic finite-state machines-part I},
	volume = {27},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1432736},
	pages = {1013--1025},
	number = {7},
	journaltitle = {Pattern Analysis and Machine Intelligence, {IEEE} Transactions on},
	author = {Vidal, Enrique and Thollard, Franck and De La Higuera, Colin and Casacuberta, Francisco and Carrasco, Rafael C.},
	urldate = {2013-10-16},
	date = {2005},
	keywords = {771, automata, language models, toread 4},
}

@inproceedings{eisenstein_what_2013,
	title = {What to do about bad language on the internet},
	url = {https://www.aclweb.org/anthology/N/N13/N13-1037.pdf},
	pages = {359--369},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Eisenstein, Jacob},
	urldate = {2013-10-16},
	date = {2013},
}

@article{eisenstein_phonological_2013,
	title = {Phonological factors in social media writing},
	url = {http://www.mpi-sws.org/~cristian/LASM_2013_files/LASM/LASM-2013.pdf#page=21},
	pages = {11},
	journaltitle = {{NAACL} 2013},
	author = {Eisenstein, Jacob},
	urldate = {2013-10-16},
	date = {2013},
}

@article{sproat_normalization_2001,
	title = {Normalization of non-standard words},
	volume = {15},
	url = {http://www.sciencedirect.com/science/article/pii/S088523080190169X},
	pages = {287--333},
	number = {3},
	journaltitle = {Computer Speech \& Language},
	author = {Sproat, Richard and Black, Alan W. and Chen, Stanley and Kumar, Shankar and Ostendorf, Mari and Richards, Christopher},
	urldate = {2013-10-16},
	date = {2001},
}

@inproceedings{dou_large_2012,
	title = {Large scale decipherment for out-of-domain machine translation},
	url = {http://dl.acm.org/citation.cfm?id=2390982},
	pages = {266--275},
	booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
	author = {Dou, Qing and Knight, Kevin},
	urldate = {2013-10-16},
	date = {2012},
}

@inproceedings{hassan_social_2013,
	title = {Social text normalization using contextual graph random walks},
	url = {http://www.aclweb.org/anthology/P/P13/P13-1155.pdf},
	booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August. Association for Computational Linguistics},
	author = {Hassan, Hany and Redmond, W. A. and Menezes, Arul},
	urldate = {2013-10-16},
	date = {2013},
}

@inproceedings{ravi_deciphering_2011,
	title = {Deciphering Foreign Language.},
	url = {http://www.aclweb.org/anthology/P/P11/P11-1002.pdf},
	pages = {12--21},
	booktitle = {{ACL}},
	author = {Ravi, Sujith and Knight, Kevin},
	urldate = {2013-10-16},
	date = {2011},
}

@inproceedings{xu_gathering_2013,
	title = {Gathering and Generating Paraphrases from Twitter with Application to Normalization},
	url = {http://www.aclweb.org/anthology/W/W13/W13-25.pdf#page=131},
	pages = {121--128},
	booktitle = {Proceedings of the Sixth Workshop on Building and Using Comparable Corpora},
	author = {Xu, Wei and Grishman, Alan Ritterˆ Ralph},
	urldate = {2013-10-16},
	date = {2013},
}

@inproceedings{ritter_open_2012,
	title = {Open domain event extraction from twitter},
	url = {http://dl.acm.org/citation.cfm?id=2339704},
	pages = {1104--1112},
	booktitle = {Proceedings of the 18th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Ritter, Alan and Etzioni, Oren and Clark, Sam and Mausam},
	urldate = {2013-10-16},
	date = {2012},
}

@report{gimpel_part--speech_2010,
	title = {Part-of-speech tagging for twitter: Annotation, features, and experiments},
	url = {http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA547371},
	shorttitle = {Part-of-speech tagging for twitter},
	institution = {{DTIC} Document},
	author = {Gimpel, Kevin and Schneider, Nathan and O'Connor, Brendan and Das, Dipanjan and Mills, Daniel and Eisenstein, Jacob and Heilman, Michael and Yogatama, Dani and Flanigan, Jeffrey and Smith, Noah A.},
	urldate = {2013-10-16},
	date = {2010},
	keywords = {{POS} tag, alopez, informal domains, toread 3},
}

@inproceedings{owoputi_improved_2013,
	title = {Improved part-of-speech tagging for online conversational text with word clusters},
	url = {http://www.aclweb.org/anthology/N/N13/N13-1039.pdf},
	pages = {380--390},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Owoputi, Olutobi and O’Connor, Brendan and Dyer, Chris and Gimpel, Kevin and Schneider, Nathan and Smith, Noah A.},
	urldate = {2013-10-16},
	date = {2013},
	keywords = {{POS} tag, alopez, informal domains, toread 3, twitter},
}

@article{rosasco_are_2004,
	title = {Are loss functions all the same?},
	volume = {16},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976604773135104},
	pages = {1063--1076},
	number = {5},
	journaltitle = {Neural Computation},
	author = {Rosasco, Lorenzo and De Vito, Ernesto and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
	urldate = {2013-10-16},
	date = {2004},
	keywords = {{ML} theory, machine learning, toread 2},
}

@incollection{pereira_speech_2008,
	title = {Speech recognition with weighted finite-state transducers},
	url = {http://link.springer.com/openurl?id=doi:10.1007/978-3-540-49127-9_28&from=SL},
	pages = {559--584},
	booktitle = {Springer Handbook of Speech Processing},
	publisher = {Springer},
	author = {Pereira, Fernando and Riley, Michael},
	urldate = {2013-10-16},
	date = {2008},
	keywords = {771, fsa, fst, speech processing, speech recognition},
}

@inproceedings{li_unsupervised_2008,
	title = {Unsupervised Translation Induction for Chinese Abbreviations using Monolingual Corpora.},
	url = {http://www.cs.jhu.edu/~zfli/pubs/chineseabbr_zhifei_acl_08.pdf},
	pages = {425--433},
	booktitle = {{ACL}},
	author = {Li, Zhifei and Yarowsky, David},
	urldate = {2013-10-16},
	date = {2008},
	keywords = {informal domains, sanjeev, text normalization},
}

@article{lamb_vertica_2012,
	title = {The vertica analytic database: C-store 7 years later},
	volume = {5},
	url = {http://dl.acm.org/citation.cfm?id=2367518},
	shorttitle = {The vertica analytic database},
	pages = {1790--1801},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	author = {Lamb, Andrew and Fuller, Matt and Varadarajan, Ramakrishna and Tran, Nga and Vandiver, Ben and Doshi, Lyric and Bear, Chuck},
	urldate = {2013-10-16},
	date = {2012},
	keywords = {615, databases, distributed systems, systems},
}

@article{chu_map-reduce_2007,
	title = {Map-reduce for machine learning on multicore},
	volume = {19},
	url = {http://books.google.com/books?hl=en&lr=&id=Tbn1l9P1220C&oi=fnd&pg=PA281&dq=Map-Reduce+for+Machine+Learning+on+Multicore&ots=V2s8Aioo0-&sig=qZhnVxqrrIu23itUZ7zfRx_Fbck},
	pages = {281},
	journaltitle = {Advances in neural information processing systems},
	author = {Chu, Cheng and Kim, Sang Kyun and Lin, Yi-An and Yu, {YuanYuan} and Bradski, Gary and Ng, Andrew Y. and Olukotun, Kunle},
	urldate = {2013-10-16},
	date = {2007},
	keywords = {615, concurrency, distributed systems, machine learning, mapreduce},
}

@misc{lopez_word_nodate,
	title = {Word Alignment and the Expectation-Maximization Algorithm},
	url = {http://www.cs.jhu.edu/~alopez/papers/model1-note.pdf},
	author = {Lopez, Adam},
	urldate = {2013-10-15},
	keywords = {{AT}-{AT}, intro, machine translation, tutorial, word alignment},
}

@book{murphy_machine_2012,
	location = {Cambridge, Mass.},
	title = {Machine learning a probabilistic perspective},
	isbn = {9780262305242  0262305240},
	url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=480968},
	abstract = {"This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a {MATLAB} software package--{PMTK} (probabilistic modeling toolkit)--that is freely available online"--Back cover.},
	publisher = {{MIT} Press},
	author = {Murphy, Kevin P},
	urldate = {2013-10-16},
	date = {2012},
}

@inproceedings{talbot_modelling_2006,
	title = {Modelling lexical redundancy for machine translation},
	url = {http://dl.acm.org/citation.cfm?id=1220297},
	pages = {969--976},
	booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics},
	author = {Talbot, David and Osborne, Miles},
	urldate = {2013-10-15},
	date = {2006},
	keywords = {{AT}-{AT}, alopez, machine learning, machine translation, reading},
}

@inproceedings{yang_log-linear_2013,
	title = {A Log-Linear Model for Unsupervised Text Normalization},
	url = {http://www.cc.gatech.edu/~jeisenst/papers/yang-emnlp-2013.pdf},
	booktitle = {Proc. of {EMNLP}},
	author = {Yang, Yi and Eisenstein, Jacob},
	urldate = {2013-10-15},
	date = {2013},
}

@article{cortes_context-free_2000,
	title = {Context-free recognition with weighted automata},
	volume = {3},
	url = {http://link.springer.com/article/10.1023/A:1009911903208},
	pages = {133--150},
	number = {2},
	journaltitle = {Grammars},
	author = {Cortes, Corinna and Mohri, Mehryar},
	urldate = {2013-10-15},
	date = {2000},
	keywords = {771, toread 2},
}

@inproceedings{ritter_named_2011,
	title = {Named entity recognition in tweets: an experimental study},
	url = {http://dl.acm.org/citation.cfm?id=2145595},
	shorttitle = {Named entity recognition in tweets},
	pages = {1524--1534},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	author = {Ritter, Alan and Clark, Sam and Etzioni, Oren},
	urldate = {2013-10-15},
	date = {2011},
	keywords = {informal domains, information extraction, twitter},
}

@article{allauzen_optimal_2004,
	title = {An optimal pre-determinization algorithm for weighted transducers},
	volume = {328},
	url = {http://www.sciencedirect.com/science/article/pii/S0304397504004256},
	pages = {3--18},
	number = {1},
	journaltitle = {Theoretical Computer Science},
	author = {Allauzen, Cyril and Mohri, Mehryar},
	urldate = {2013-10-15},
	date = {2004},
	keywords = {771, automata, fsa, fst},
}

@article{brown_mathematics_1993,
	title = {The mathematics of statistical machine translation: Parameter estimation},
	volume = {19},
	url = {http://dl.acm.org/citation.cfm?id=972474},
	shorttitle = {The mathematics of statistical machine translation},
	pages = {263--311},
	number = {2},
	journaltitle = {Computational linguistics},
	author = {Brown, Peter F. and Pietra, Vincent J. Della and Pietra, Stephen A. Della and Mercer, Robert L.},
	urldate = {2013-10-15},
	date = {1993},
	keywords = {{AT}-{AT}, machine translation, toread 2},
}

@inproceedings{zhang_adaptive_2013,
	title = {Adaptive Parser-Centric Text Normalization},
	url = {http://homes.cs.washington.edu/~clzhang/paper/acl2013.pdf},
	pages = {1159--1168},
	booktitle = {Proceedings of {ACL}},
	author = {Zhang, Congle and Baldwin, Tyler and Ho, Howard and Kimelfeld, Benny and Li, Yunyao},
	urldate = {2013-10-15},
	date = {2013},
	keywords = {normalization, parsing, social media},
}

@inproceedings{collobert_unified_2008,
	title = {A unified architecture for natural language processing: Deep neural networks with multitask learning},
	url = {http://dl.acm.org/citation.cfm?id=1390177},
	shorttitle = {A unified architecture for natural language processing},
	pages = {160--167},
	booktitle = {Proceedings of the 25th international conference on Machine learning},
	author = {Collobert, Ronan and Weston, Jason},
	urldate = {2013-10-15},
	date = {2008},
	keywords = {765, deep learning, toread 4},
}

@inproceedings{snyder_statistical_2010,
	title = {A statistical model for lost language decipherment},
	url = {http://dl.acm.org/citation.cfm?id=1858788},
	pages = {1048--1057},
	booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
	author = {Snyder, Benjamin and Barzilay, Regina and Knight, Kevin},
	urldate = {2013-10-15},
	date = {2010},
	keywords = {{OOV}, alopez, decipherment, lost languages, toread 4},
}

@article{stolcke_efficient_1995,
	title = {An efficient probabilistic context-free parsing algorithm that computes prefix probabilities},
	volume = {21},
	issn = {0891-2017},
	url = {http://dl.acm.org/citation.cfm?id=211190.211197},
	abstract = {We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for {SCFGs} in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.},
	pages = {165--201},
	number = {2},
	journaltitle = {Comput. Linguist.},
	author = {Stolcke, Andreas},
	urldate = {2013-10-13},
	date = {1995-06},
	keywords = {771, context free, parsing, statistical parsing},
}

@article{booth_applying_1973,
	title = {Applying Probability Measures to Abstract Languages},
	volume = {C-22},
	issn = {0018-9340},
	doi = {10.1109/T-C.1973.223746},
	abstract = {The problem of assigning a probability to each word of a language is considered. Two methods are discussed. One method assigns a probability to a word on the basis of particular measurable features of the language. The second method is applied to languages L(G) generated by a grammar G. A probability is associated with each production of G. These in turn define the word probabilities of each word in the language. The conditions for this assignment to be a probabilistic measure are derived.},
	pages = {442--450},
	number = {5},
	journaltitle = {{IEEE} Transactions on Computers},
	author = {Booth, T.L. and Thompson, R.A.},
	date = {1973},
	keywords = {771, Abstract languages, probabilistic languages, stochastic automata, word functions., Automata, Computer science, Dictionaries, Information systems, Particle measurements, Production, Stochastic processes, context free, probabilistic languages},
}

@misc{smith_tutorial_nodate,
	title = {A tutorial on Principal Components Analysis},
	author = {Smith, Lindsay},
	keywords = {dimensionality reduction, intro, linear algebra, principle component analysis, toread 4, tutorial},
}

@inproceedings{darwish_language_2012,
	location = {New York, {NY}, {USA}},
	title = {Language processing for arabic microblog retrieval},
	isbn = {978-1-4503-1156-4},
	url = {http://doi.acm.org/10.1145/2396761.2398658},
	doi = {10.1145/2396761.2398658},
	series = {{CIKM} '12},
	abstract = {The use of social media has profoundly affected social and political dynamics in the Arab world. In this paper, we explore the Arabic microblogs retrieval. We illustrate some of the challenges associated with Arabic microblog retrieval, which mainly stem from the use of different Arabic dialects that vary in lexical selection, morphology, and phonetics and lack orthographic and spelling conventions. We present some of the required processing for effective retrieval such as improved letter normalization, elongated word handling, stopword removal, and stemming},
	pages = {2427--2430},
	booktitle = {Proceedings of the 21st {ACM} international conference on Information and knowledge management},
	publisher = {{ACM}},
	author = {Darwish, Kareem and Magdy, Walid and Mourad, Ahmed},
	urldate = {2013-09-19},
	date = {2012},
	keywords = {alopez, arabic, arabic retrieval, arabic twitter, dialect, dialect arabic normalization, microblog search, text normalization},
}